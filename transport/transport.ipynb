{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anderson\\Desktop\\gnn-transport\\transport\\transport.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Desktop/gnn-transport/transport/transport.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m combinations, groupby\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Desktop/gnn-transport/transport/transport.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Desktop/gnn-transport/transport/transport.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpecanpy\u001b[39;00m \u001b[39mimport\u001b[39;00m pecanpy\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Desktop/gnn-transport/transport/transport.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Desktop/gnn-transport/transport/transport.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\pecanpy\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"PecanPy: parallelized, efficient, and accelerated node2vec.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m graph\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pecanpy\n\u001b[0;32m      5\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpecanpy\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\pecanpy\\pecanpy.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m njit\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m prange\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba_progress\u001b[39;00m \u001b[39mimport\u001b[39;00m ProgressBar\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\__init__.py:88\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[39m# Re-export decorators\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m (cfunc, generated_jit, jit, njit, stencil,\n\u001b[0;32m     89\u001b[0m                                    jit_module)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[0;32m     93\u001b[0m                             get_num_threads, set_num_threads,\n\u001b[0;32m     94\u001b[0m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[0;32m     95\u001b[0m                             get_thread_id)\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\core\\decorators.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m DeprecationError, NumbaDeprecationWarning\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstencils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstencil\u001b[39;00m \u001b[39mimport\u001b[39;00m stencil\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m config, extending, sigutils, registry\n\u001b[0;32m     15\u001b[0m _logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\stencils\\stencil.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllvmlite\u001b[39;00m \u001b[39mimport\u001b[39;00m ir \u001b[39mas\u001b[39;00m lir\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m types, typing, utils, ir, config, ir_utils, registry\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtemplates\u001b[39;00m \u001b[39mimport\u001b[39;00m (CallableTemplate, signature,\n\u001b[0;32m     13\u001b[0m                                          infer_global, AbstractTemplate)\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimputils\u001b[39;00m \u001b[39mimport\u001b[39;00m lower_builtin\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\core\\registry.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_property\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdescriptors\u001b[39;00m \u001b[39mimport\u001b[39;00m TargetDescriptor\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, typing, dispatcher, cpu\n\u001b[0;32m      7\u001b[0m \u001b[39m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Default CPU target descriptors\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_NestedContext\u001b[39;00m(\u001b[39mobject\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\core\\dispatcher.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypeof\u001b[39;00m \u001b[39mimport\u001b[39;00m Purpose, typeof\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbytecode\u001b[39;00m \u001b[39mimport\u001b[39;00m get_code_object\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m \u001b[39mimport\u001b[39;00m NullCache, FunctionCache\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m entrypoints\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretarget\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseRetarget\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\core\\caching.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39muuid\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mappdirs\u001b[39;00m \u001b[39mimport\u001b[39;00m AppDirs\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumba\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m NumbaWarning\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\numba\\misc\\appdirs.py:507\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m system \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    506\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mwin32com\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshell\u001b[39;00m\n\u001b[0;32m    508\u001b[0m         _get_win_folder \u001b[39m=\u001b[39m _get_win_folder_with_pywin32\n\u001b[0;32m    509\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\win32com\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpythoncom\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwin32api\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# flag if we are in a \"frozen\" build.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m _frozen \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(sys, \u001b[39m\"\u001b[39m\u001b[39mfrozen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations, groupby\n",
    "from gensim.models import Word2Vec\n",
    "from pecanpy import pecanpy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnp_random_connected_graph(n: int, p: float) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Generates a random undirected graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted\n",
    "    \"\"\"\n",
    "    edges = combinations(range(n), 2)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    if p <= 0:\n",
    "        return G\n",
    "    if p >= 1:\n",
    "        return nx.complete_graph(n, create_using=G)\n",
    "    \n",
    "    for _, node_edges in groupby(edges, key=lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < p:\n",
    "                G.add_edge(*e)\n",
    "\n",
    "    for (u, v) in G.edges():\n",
    "        G.edges[u,v]['weight'] = random.random()*100\n",
    "    return G\n",
    "\n",
    "def distance_sum(g):\n",
    "    lengths = dict(nx.floyd_warshall(g, weight='weight'))\n",
    "    \n",
    "    total_sum=0\n",
    "    \n",
    "    for i in range(len(g.nodes)):\n",
    "        node_lengths=lengths[i]\n",
    "        node_sum=sum(np.array(list(node_lengths.values())))\n",
    "        # print(node_sum)\n",
    "        total_sum+=node_sum\n",
    "    return total_sum\n",
    "\n",
    "def get_node_embedding(graph,dimensions=128,walk_length=30, num_walks=200, workers=4,pfilename=\"test.edg\"):\n",
    "    print(\"Edges: \"+str(len(graph.edges())))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nodelist = list(graph.nodes)\n",
    "    \n",
    "    \n",
    "    \"already seen the q=5, lets try now q=2\"\n",
    "    \n",
    "    ####Pecanpy\n",
    "    nx.write_weighted_edgelist(graph, pfilename,delimiter='\\t')\n",
    "    \n",
    "    # g=pecanpy.PreComp(p=1,q=5,workers=8,verbose=True)\n",
    "    # g=pecanpy.SparseOTF(p=1,q=1,workers=8,verbose=True)\n",
    "    \n",
    "    g = pecanpy.SparseOTF(p=1, q=2, workers=8, verbose=True, extend=True, gamma=0)\n",
    "    print(\"Extend=True\")\n",
    "    g.read_edg(pfilename, weighted=True ,directed=True)\n",
    "    \n",
    "    starttime2 = time.time()\n",
    "    walks = g.simulate_walks(num_walks=10, walk_length=100)# use random walks to train embeddings\n",
    "    model = Word2Vec(walks, vector_size=dimensions, window=5, min_count=0, sg=1, workers=8, epochs=10)\n",
    "    endtime2 = time.time()\n",
    "    duration = endtime2-starttime2\n",
    "    \n",
    "    print(f'Pecanpy {duration}')\n",
    "    \n",
    "    feature_matrix = (model.wv.vectors)\n",
    "    \n",
    "    feature_matrix_sparse = sparse.csr_matrix(feature_matrix)\n",
    "    \n",
    "    return feature_matrix_sparse\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    \n",
    "    sparse_mx = sparse_mx.tocoo().astype(float)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    # FOR UNWEIGHTED, data only contains ones!!!!\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def get_adjacency_feature_centrality(list_graph, num_nodes,pfilename=\"test.edg\"):\n",
    "    \n",
    "    \"\"\"Returns adjacency matrices, their node sequences and centrality matrix.\"\"\"\n",
    "    \n",
    "    num_graph = len(list_graph)\n",
    "    # list_adj = list()\n",
    "    # list_adj_t = list()\n",
    "    cent_mat = np.zeros((1,num_graph),dtype=float)\n",
    "    list_nodelist = list()\n",
    "    list_feature_mat = list()\n",
    "    list_edge_index=list()\n",
    "    list_edge_weight=list()\n",
    "    list_distance_sum=list()\n",
    "\n",
    "    for ind,g in enumerate(list_graph):\n",
    "        print(\"Generating Graph: \"+str(ind))\n",
    "        print(\"Max_Node: \"+str(num_nodes))\n",
    "        node_seq = list(g.nodes())\n",
    "        \n",
    "        starttime2 = time.time()\n",
    "        # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "        total_sum_new=distance_sum(g)\n",
    "        # eff=total_sum_orig/total_sum_new\n",
    "        \n",
    "        endtime2 = time.time()\n",
    "        duration = endtime2-starttime2\n",
    "        \n",
    "        print(\"EFF:\"+str(duration))\n",
    "        cent_mat[0, ind] = total_sum_new\n",
    "          \n",
    "        # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "        # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "        # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "        # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "        \n",
    "\n",
    "        \n",
    "        ndim = 256\n",
    "        feature_mat = np.zeros((num_nodes,ndim))\n",
    "        feature_mat_excerpt = get_node_embedding(g, dimensions = ndim, walk_length = 50, num_walks = 50, workers = 12,pfilename=pfilename).todense()\n",
    "        feature_mat[0:feature_mat_excerpt.shape[0]] = feature_mat_excerpt\n",
    "        feature_mat = sparse.csr_matrix(feature_mat)\n",
    "\n",
    "        # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    " \n",
    "        feature_mat = sparse_mx_to_torch_sparse_tensor(feature_mat)\n",
    "        \n",
    "        # list_adj.append(adj_mat)\n",
    "        list_edge_index.append(torch.tensor(list(g.edges)).t().contiguous())\n",
    "        list_edge_weight.append(torch.tensor([g[u][v]['weight'] for u, v in g.edges]))\n",
    "\n",
    "        \n",
    "        list_nodelist.append(list(range(num_nodes)))\n",
    "        list_feature_mat.append(feature_mat)\n",
    "      \n",
    "         \n",
    "    # return list_adj,list_adj_t,list_nodelist,cent_mat,list_feature_mat\n",
    "    return list_edge_index,list_edge_weight , cent_mat, list_feature_mat\n",
    "\n",
    "\n",
    "def get_adjacency_centrality(list_graph, num_nodes):\n",
    "    \n",
    "    \"\"\"Returns adjacency matrices, their node sequences and centrality matrix.\"\"\"\n",
    "    \n",
    "    num_graph = len(list_graph)\n",
    "    # list_adj = list()\n",
    "    # list_adj_t = list()\n",
    "    cent_mat = np.zeros((1,num_graph),dtype=float)\n",
    "    list_nodelist = list()\n",
    "    # list_feature_mat = list()\n",
    "    list_edge_index=list()\n",
    "    list_edge_weight=list()\n",
    "\n",
    "    for ind,g in enumerate(list_graph):\n",
    "        print(\"Generating Graph: \"+str(ind))\n",
    "        print(\"Max_Edge: \"+str(num_nodes))\n",
    "        node_seq = list(g.nodes())\n",
    "        \n",
    "        starttime2 = time.time()\n",
    "        # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "        total_sum_new=distance_sum(g)\n",
    "        # eff=total_sum_orig/total_sum_new\n",
    "        endtime2 = time.time()\n",
    "        duration = endtime2-starttime2\n",
    "        \n",
    "        print(\"EFF:\"+str(duration))\n",
    "        cent_mat[0, ind] = total_sum_new\n",
    "          \n",
    "        # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "        # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "        # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "        # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "        \n",
    "        # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "        # list_adj.append(adj_mat)\n",
    "        \n",
    "        list_edge_index.append(torch.tensor(list(g.edges)).t().contiguous())\n",
    "        list_edge_weight.append(torch.tensor([g[u][v]['weight'] for u, v in g.edges]))\n",
    "\n",
    "        list_nodelist.append(list(range(num_nodes)))\n",
    "\n",
    "    return list_edge_index,list_edge_weight, cent_mat#, list_feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse this to draw unmodified + modified\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify_graph(base_graph: nx.Graph, k: int) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Remove k edges from a graph while maintaining connectedness\n",
    "    \"\"\"\n",
    "    modified_graph = base_graph.copy()\n",
    "\n",
    "    while True:\n",
    "        test_graph = modified_graph.copy()\n",
    "        edges_to_delete = random.sample(list(modified_graph.edges()), k)\n",
    "        edges_to_delete.sort(reverse=True)\n",
    "\n",
    "        for u, v in edges_to_delete:\n",
    "            test_graph.remove_edge(u, v)\n",
    "        \n",
    "        if nx.is_connected(test_graph):\n",
    "            for u, v in edges_to_delete:\n",
    "                modified_graph.edges[u, v]['weight'] = 1000000000\n",
    "            break\n",
    "    \n",
    "    return modified_graph\n",
    "\n",
    "\n",
    "# Create N_GRAPHS training graphs\n",
    "N_GRAPHS = 100\n",
    "MAX_NODES = 500\n",
    "PROB_EDGE = 1.2/(100-1)\n",
    "base_graphs: 'list[nx.Graph]' = [gnp_random_connected_graph(MAX_NODES, PROB_EDGE) for _ in range(N_GRAPHS)]\n",
    "\n",
    "# Create 1 modified graph for each\n",
    "modified_graphs = [modify_graph(base_graph, k=1) for base_graph in base_graphs]\n",
    "\n",
    "'''\n",
    "Use this to draw unmodified + modified\n",
    "'''\n",
    "\n",
    "# G = base_graphs[0]\n",
    "# labels = nx.get_edge_attributes(G, 'weight')\n",
    "# pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "# nx.draw(G, pos, with_labels=True)\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "# G = modified_graphs[0]\n",
    "# labels = nx.get_edge_attributes(G, 'weight')\n",
    "# # pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "# nx.draw(G, pos, with_labels=True)\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "class PairData(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'edge_index_base':\n",
    "            return self.num_nodes\n",
    "        if key == 'edge_index_modified':\n",
    "            return self.num_nodes\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "    \n",
    "torch_base: 'list[Data]' = list(map(from_networkx, base_graphs))\n",
    "torch_modified: 'list[Data]' = list(map(from_networkx, modified_graphs))\n",
    "\n",
    "torch_base_edge_index, torch_base_weights = list(zip(*[(g.edge_index, g['weight']) for g in torch_base]))\n",
    "torch_modified_edge_index, torch_modified_weights = list(zip(*[(g.edge_index, g['weight']) for g in torch_modified]))\n",
    "\n",
    "pair_data = list(zip(torch_base_edge_index, torch_base_weights, torch_modified_edge_index, torch_modified_weights))\n",
    "\n",
    "data_list = [PairData(edge_index_base=edge_index_base, \n",
    "                      weight_base=weight_base, \n",
    "                      edge_index_modified=edge_index_modified,\n",
    "                      weight_modified=weight_modified,\n",
    "                      num_nodes=MAX_NODES)\n",
    "                for edge_index_base, weight_base, edge_index_modified, weight_modified in pair_data\n",
    "            ]\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "split_index = int(TRAIN_TEST_SPLIT * N_GRAPHS)\n",
    "train_dataset = data_list[:split_index]\n",
    "test_dataset = data_list[split_index:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "=====\n",
      "Number of graphs in batch: 16\n",
      "PairDataBatch(edge_index_base=[2, 64050], weight_base=[64050], edge_index_modified=[2, 64050], weight_modified=[64050], num_nodes=8000, batch=[8000], ptr=[17])\n",
      "\n",
      "Step 2\n",
      "=====\n",
      "Number of graphs in batch: 16\n",
      "PairDataBatch(edge_index_base=[2, 64656], weight_base=[64656], edge_index_modified=[2, 64656], weight_modified=[64656], num_nodes=8000, batch=[8000], ptr=[17])\n",
      "\n",
      "Step 3\n",
      "=====\n",
      "Number of graphs in batch: 16\n",
      "PairDataBatch(edge_index_base=[2, 63980], weight_base=[63980], edge_index_modified=[2, 63980], weight_modified=[63980], num_nodes=8000, batch=[8000], ptr=[17])\n",
      "\n",
      "Step 4\n",
      "=====\n",
      "Number of graphs in batch: 16\n",
      "PairDataBatch(edge_index_base=[2, 63942], weight_base=[63942], edge_index_modified=[2, 63942], weight_modified=[63942], num_nodes=8000, batch=[8000], ptr=[17])\n",
      "\n",
      "Step 5\n",
      "=====\n",
      "Number of graphs in batch: 16\n",
      "PairDataBatch(edge_index_base=[2, 63916], weight_base=[63916], edge_index_modified=[2, 63916], weight_modified=[63916], num_nodes=8000, batch=[8000], ptr=[17])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}')\n",
    "    print('=====')\n",
    "    print(f'Number of graphs in batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class Graph_Diff_Reg(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,dropout):\n",
    "        super(Graph_Diff_Reg, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(int(hidden_dim), hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            # # nn.Linear(int(hidden_dim/4), int(hidden_dim/8)),\n",
    "            # # nn.ReLU(),\n",
    "            # # nn.Tanh(),\n",
    "            # nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(int(hidden_dim/4),1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,edge_index1,edge_weight1,edge_index2,edge_weight2,fm,batch_tensor):#,adj,adj_orig):\n",
    "        # x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN layer\n",
    "        x11 = self.gcn1(fm, edge_index1,edge_weight1)\n",
    "        # print(sum())\n",
    "        # x11 = F.relu(x11)\n",
    "\n",
    "        # First GCN layer\n",
    "        # with torch.no_grad():\n",
    "        x12 = self.gcn1(fm, edge_index2,edge_weight2)\n",
    "        # x12 = F.relu(x12)\n",
    "        \n",
    "        \n",
    "        ###Secong GCN layer\n",
    "        x21 = self.gcn2(x11, edge_index1,edge_weight1)\n",
    "        # x21 = F.relu(x21)\n",
    "\n",
    "        # First GCN layer\n",
    "        # with torch.no_grad():\n",
    "        x22 = self.gcn2(x12 , edge_index2,edge_weight2)\n",
    "        # x22 = F.relu(x22)\n",
    "        \n",
    "        \n",
    "        # x1=x11+x21+x31\n",
    "        # x2=x12+x22+x32\n",
    "        \n",
    "        x1=x12-x11\n",
    "        x2=x22-x21\n",
    "        # x3=x32-x31\n",
    "        # x4=x42-x41\n",
    "        \n",
    "        x=x1*x2#*x3#*x4\n",
    "        \n",
    "        \n",
    "        # s = self.diff_pool(x, edge_index1, edge_weight1)\n",
    "\n",
    "        # # Updated node representations after DiffPool\n",
    "        # x = s.x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # x=x1-x2\n",
    "        \n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        # Graph pooling\n",
    "        \n",
    "        # from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "        # x = torch.cat([gmp(x, batch_tensor), \n",
    "        #             gap(x, batch_tensor)], dim=1)\n",
    "        \n",
    "        x = global_mean_pool(x,batch_tensor)\n",
    "        \n",
    "        # MLP layers\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "num_epoch = 50\n",
    "\n",
    "hidden = 256\n",
    "n_features = 256\n",
    "dropout_val = 0.4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = Graph_Diff_Reg(input_dim=n_features, hidden_dim=hidden, output_dim=1, dropout=dropout_val)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00005/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: Graph_Diff_Reg, optimizer: optim.Adam):\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        out = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=500\n",
    "prob_edge = 1.2/(100-1)\n",
    "\n",
    "BASE_GRAPH = gnp_random_connected_graph(n_nodes, prob_edge)\n",
    "MAX_MODEL_NUM = len(BASE_GRAPH.nodes)\n",
    "\n",
    "X_train_base = [BASE_GRAPH.to_directed()]\n",
    "X_test_base =  [BASE_GRAPH.to_directed()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations, groupby\n",
    "from gensim.models import Word2Vec\n",
    "from pecanpy import pecanpy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnp_random_connected_graph(n: int, p: float) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Generates a random undirected graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted\n",
    "    \"\"\"\n",
    "    edges = combinations(range(n), 2)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    if p <= 0:\n",
    "        return G\n",
    "    if p >= 1:\n",
    "        return nx.complete_graph(n, create_using=G)\n",
    "    \n",
    "    for _, node_edges in groupby(edges, key=lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < p:\n",
    "                G.add_edge(*e)\n",
    "\n",
    "    for (u, v) in G.edges():\n",
    "        G.edges[u,v]['weight'] = random.random()*100\n",
    "    return G\n",
    "\n",
    "def distance_sum(g):\n",
    "    lengths = dict(nx.floyd_warshall(g, weight='weight'))\n",
    "    \n",
    "    total_sum=0\n",
    "    \n",
    "    for i in range(len(g.nodes)):\n",
    "        node_lengths=lengths[i]\n",
    "        node_sum=sum(np.array(list(node_lengths.values())))\n",
    "        # print(node_sum)\n",
    "        total_sum+=node_sum\n",
    "    return total_sum\n",
    "\n",
    "def get_node_embedding(graph,dimensions=128,walk_length=30, num_walks=200, workers=4,pfilename=\"test.edg\"):\n",
    "    print(\"Edges: \"+str(len(graph.edges())))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nodelist = list(graph.nodes)\n",
    "    \n",
    "    \n",
    "    \"already seen the q=5, lets try now q=2\"\n",
    "    \n",
    "    ####Pecanpy\n",
    "    nx.write_weighted_edgelist(graph, pfilename,delimiter='\\t')\n",
    "    \n",
    "    # g=pecanpy.PreComp(p=1,q=5,workers=8,verbose=True)\n",
    "    # g=pecanpy.SparseOTF(p=1,q=1,workers=8,verbose=True)\n",
    "    \n",
    "    g = pecanpy.SparseOTF(p=1, q=2, workers=8, verbose=True, extend=True, gamma=0)\n",
    "    print(\"Extend=True\")\n",
    "    g.read_edg(pfilename, weighted=True ,directed=True)\n",
    "    \n",
    "    starttime2 = time.time()\n",
    "    walks = g.simulate_walks(num_walks=10, walk_length=100)# use random walks to train embeddings\n",
    "    model = Word2Vec(walks, vector_size=dimensions, window=5, min_count=0, sg=1, workers=8, epochs=10)\n",
    "    endtime2 = time.time()\n",
    "    duration = endtime2-starttime2\n",
    "    \n",
    "    print(f'Pecanpy {duration}')\n",
    "    \n",
    "    feature_matrix = (model.wv.vectors)\n",
    "    \n",
    "    feature_matrix_sparse = sparse.csr_matrix(feature_matrix)\n",
    "    \n",
    "    return feature_matrix_sparse\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    \n",
    "    sparse_mx = sparse_mx.tocoo().astype(float)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    # FOR UNWEIGHTED, data only contains ones!!!!\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def adjacency_feature_centrality(graph: )\n",
    "\n",
    "def get_adjacency_feature_centrality(list_graph, num_nodes,pfilename=\"test.edg\"):\n",
    "    \n",
    "    \"\"\"Returns adjacency matrices, their node sequences and centrality matrix.\"\"\"\n",
    "    \n",
    "    num_graph = len(list_graph)\n",
    "    # list_adj = list()\n",
    "    # list_adj_t = list()\n",
    "    cent_mat = np.zeros((1,num_graph),dtype=float)\n",
    "    list_nodelist = list()\n",
    "    list_feature_mat = list()\n",
    "    list_edge_index=list()\n",
    "    list_edge_weight=list()\n",
    "    list_distance_sum=list()\n",
    "\n",
    "    for ind,g in enumerate(list_graph):\n",
    "        print(\"Generating Graph: \"+str(ind))\n",
    "        print(\"Max_Node: \"+str(num_nodes))\n",
    "        node_seq = list(g.nodes())\n",
    "        \n",
    "        starttime2 = time.time()\n",
    "        # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "        total_sum_new=distance_sum(g)\n",
    "        # eff=total_sum_orig/total_sum_new\n",
    "        \n",
    "        endtime2 = time.time()\n",
    "        duration = endtime2-starttime2\n",
    "        \n",
    "        print(\"EFF:\"+str(duration))\n",
    "        cent_mat[0, ind] = total_sum_new\n",
    "          \n",
    "        # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "        # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "        # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "        # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "        \n",
    "\n",
    "        \n",
    "        ndim = 256\n",
    "        feature_mat = np.zeros((num_nodes,ndim))\n",
    "        feature_mat_excerpt = get_node_embedding(g, dimensions = ndim, walk_length = 50, num_walks = 50, workers = 12,pfilename=pfilename).todense()\n",
    "        feature_mat[0:feature_mat_excerpt.shape[0]] = feature_mat_excerpt\n",
    "        feature_mat = sparse.csr_matrix(feature_mat)\n",
    "\n",
    "        # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    " \n",
    "        feature_mat = sparse_mx_to_torch_sparse_tensor(feature_mat)\n",
    "        \n",
    "        # list_adj.append(adj_mat)\n",
    "        list_edge_index.append(torch.tensor(list(g.edges)).t().contiguous())\n",
    "        list_edge_weight.append(torch.tensor([g[u][v]['weight'] for u, v in g.edges]))\n",
    "\n",
    "        \n",
    "        list_nodelist.append(list(range(num_nodes)))\n",
    "        list_feature_mat.append(feature_mat)\n",
    "      \n",
    "         \n",
    "    # return list_adj,list_adj_t,list_nodelist,cent_mat,list_feature_mat\n",
    "    return list_edge_index,list_edge_weight , cent_mat, list_feature_mat\n",
    "\n",
    "\n",
    "def get_adjacency_centrality(list_graph, num_nodes):\n",
    "    \n",
    "    \"\"\"Returns adjacency matrices, their node sequences and centrality matrix.\"\"\"\n",
    "    \n",
    "    num_graph = len(list_graph)\n",
    "    # list_adj = list()\n",
    "    # list_adj_t = list()\n",
    "    cent_mat = np.zeros((1,num_graph),dtype=float)\n",
    "    list_nodelist = list()\n",
    "    # list_feature_mat = list()\n",
    "    list_edge_index=list()\n",
    "    list_edge_weight=list()\n",
    "\n",
    "    for ind,g in enumerate(list_graph):\n",
    "        print(\"Generating Graph: \"+str(ind))\n",
    "        print(\"Max_Edge: \"+str(num_nodes))\n",
    "        node_seq = list(g.nodes())\n",
    "        \n",
    "        starttime2 = time.time()\n",
    "        # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "        total_sum_new=distance_sum(g)\n",
    "        # eff=total_sum_orig/total_sum_new\n",
    "        endtime2 = time.time()\n",
    "        duration = endtime2-starttime2\n",
    "        \n",
    "        print(\"EFF:\"+str(duration))\n",
    "        cent_mat[0, ind] = total_sum_new\n",
    "          \n",
    "        # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "        # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "        # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "        # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "        \n",
    "        # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "        # list_adj.append(adj_mat)\n",
    "        \n",
    "        list_edge_index.append(torch.tensor(list(g.edges)).t().contiguous())\n",
    "        list_edge_weight.append(torch.tensor([g[u][v]['weight'] for u, v in g.edges]))\n",
    "\n",
    "        list_nodelist.append(list(range(num_nodes)))\n",
    "\n",
    "    return list_edge_index,list_edge_weight, cent_mat#, list_feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse this to draw unmodified + modified\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify_graph(base_graph: nx.Graph, k: int) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Remove k edges from a graph while maintaining connectedness\n",
    "    \"\"\"\n",
    "    modified_graph = base_graph.copy()\n",
    "\n",
    "    while True:\n",
    "        test_graph = modified_graph.copy()\n",
    "        edges_to_delete = random.sample(list(modified_graph.edges()), k)\n",
    "        edges_to_delete.sort(reverse=True)\n",
    "\n",
    "        for u, v in edges_to_delete:\n",
    "            test_graph.remove_edge(u, v)\n",
    "        \n",
    "        if nx.is_connected(test_graph):\n",
    "            for u, v in edges_to_delete:\n",
    "                modified_graph.edges[u, v]['weight'] = 1000000000\n",
    "            break\n",
    "    \n",
    "    return modified_graph\n",
    "\n",
    "\n",
    "# Create N_GRAPHS training graphs\n",
    "N_GRAPHS = 10\n",
    "MAX_NODES = 100\n",
    "PROB_EDGE = 1.2/(100-1)\n",
    "base_graphs: 'list[nx.Graph]' = [gnp_random_connected_graph(MAX_NODES, PROB_EDGE) for _ in range(N_GRAPHS)]\n",
    "\n",
    "# Create 1 modified graph for each\n",
    "modified_graphs = [modify_graph(base_graph, k=1) for base_graph in base_graphs]\n",
    "\n",
    "'''\n",
    "Use this to draw unmodified + modified\n",
    "'''\n",
    "\n",
    "# G = base_graphs[0]\n",
    "# labels = nx.get_edge_attributes(G, 'weight')\n",
    "# pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "# nx.draw(G, pos, with_labels=True)\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "# G = modified_graphs[0]\n",
    "# labels = nx.get_edge_attributes(G, 'weight')\n",
    "# # pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "# nx.draw(G, pos, with_labels=True)\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "class PairData(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'edge_index_base':\n",
    "            return self.num_nodes\n",
    "        if key == 'edge_index_modified':\n",
    "            return self.num_nodes\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "    \n",
    "torch_base: 'list[Data]' = list(map(from_networkx, base_graphs))\n",
    "torch_modified: 'list[Data]' = list(map(from_networkx, modified_graphs))\n",
    "\n",
    "torch_base_edge_index, torch_base_weights = list(zip(*[(g.edge_index, g['weight']) for g in torch_base]))\n",
    "torch_modified_edge_index, torch_modified_weights = list(zip(*[(g.edge_index, g['weight']) for g in torch_modified]))\n",
    "\n",
    "pair_data = list(zip(torch_base_edge_index, torch_base_weights, torch_modified_edge_index, torch_modified_weights))\n",
    "\n",
    "data_list = [PairData(edge_index_base=edge_index_base, \n",
    "                      weight_base=weight_base, \n",
    "                      edge_index_modified=edge_index_modified,\n",
    "                      weight_modified=weight_modified,\n",
    "                      num_nodes=MAX_NODES)\n",
    "                for edge_index_base, weight_base, edge_index_modified, weight_modified in pair_data\n",
    "            ]\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "BATCH_SIZE = 4\n",
    "split_index = int(TRAIN_TEST_SPLIT * N_GRAPHS)\n",
    "train_dataset = data_list[:split_index]\n",
    "test_dataset = data_list[split_index:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class Graph_Diff_Reg(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,dropout):\n",
    "        super(Graph_Diff_Reg, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(int(hidden_dim), hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            # # nn.Linear(int(hidden_dim/4), int(hidden_dim/8)),\n",
    "            # # nn.ReLU(),\n",
    "            # # nn.Tanh(),\n",
    "            # nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(int(hidden_dim/4),1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,edge_index1,edge_weight1,edge_index2,edge_weight2,fm,batch_tensor):#,adj,adj_orig):\n",
    "        # x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN layer\n",
    "        x11 = self.gcn1(fm, edge_index1,edge_weight1)\n",
    "        # print(sum())\n",
    "        # x11 = F.relu(x11)\n",
    "\n",
    "        # First GCN layer\n",
    "        # with torch.no_grad():\n",
    "        x12 = self.gcn1(fm, edge_index2,edge_weight2)\n",
    "        # x12 = F.relu(x12)\n",
    "        \n",
    "        \n",
    "        ###Secong GCN layer\n",
    "        x21 = self.gcn2(x11, edge_index1,edge_weight1)\n",
    "        # x21 = F.relu(x21)\n",
    "\n",
    "        # First GCN layer\n",
    "        # with torch.no_grad():\n",
    "        x22 = self.gcn2(x12 , edge_index2,edge_weight2)\n",
    "        # x22 = F.relu(x22)\n",
    "        \n",
    "        \n",
    "        # x1=x11+x21+x31\n",
    "        # x2=x12+x22+x32\n",
    "        \n",
    "        x1=x12-x11\n",
    "        x2=x22-x21\n",
    "        # x3=x32-x31\n",
    "        # x4=x42-x41\n",
    "        \n",
    "        x=x1*x2#*x3#*x4\n",
    "        \n",
    "        \n",
    "        # s = self.diff_pool(x, edge_index1, edge_weight1)\n",
    "\n",
    "        # # Updated node representations after DiffPool\n",
    "        # x = s.x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # x=x1-x2\n",
    "        \n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        # Graph pooling\n",
    "        \n",
    "        # from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "        # x = torch.cat([gmp(x, batch_tensor), \n",
    "        #             gap(x, batch_tensor)], dim=1)\n",
    "        \n",
    "        x = global_mean_pool(x,batch_tensor)\n",
    "        \n",
    "        # MLP layers\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "num_epoch = 50\n",
    "\n",
    "hidden = 256\n",
    "n_features = 256\n",
    "dropout_val = 0.4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = Graph_Diff_Reg(input_dim=n_features, hidden_dim=hidden, output_dim=1, dropout=dropout_val)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00005/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: Graph_Diff_Reg, optimizer: optim.Adam):\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        out = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=500\n",
    "prob_edge = 1.2/(100-1)\n",
    "\n",
    "BASE_GRAPH = gnp_random_connected_graph(n_nodes, prob_edge)\n",
    "MAX_MODEL_NUM = len(BASE_GRAPH.nodes)\n",
    "\n",
    "X_train_base = [BASE_GRAPH.to_directed()]\n",
    "X_test_base =  [BASE_GRAPH.to_directed()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

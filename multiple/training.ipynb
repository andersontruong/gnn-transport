{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following device is used: cuda\n"
     ]
    }
   ],
   "source": [
    "from model_bet import Graph_Diff_Reg\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random, glob, time, os, pickle\n",
    "import numpy as np\n",
    "from graphcuda import generate_random_adjacency_matrix, faq_align\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('The following device is used: '+str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 256\n",
    "num_epoch = 200\n",
    "\n",
    "n_features= 256#list_fm_train_shuffled[0].shape[1]\n",
    "dropout_val = 0.2\n",
    "learning_rate = 1e-6\n",
    "\n",
    "model = Graph_Diff_Reg(input_dim=n_features,hidden_dim=hidden,output_dim=1,dropout=dropout_val)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-7)\n",
    "\n",
    "training_loss_epochs = []\n",
    "training_mse_epochs = []\n",
    "\n",
    "testing_loss_epochs= []\n",
    "testing_mse_epochs= []\n",
    "\n",
    "training_score_epochs = []\n",
    "testing_score_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damage_graph(base_graph):\n",
    "    G0=base_graph.copy()\n",
    "    while True:\n",
    "        tempg=base_graph.copy()\n",
    "        ndel=list(np.random.randint(0,len(base_graph.edges())-1,random.randint(0,20)))\n",
    "        ndel.sort(reverse=True)\n",
    "        # ndel=i\n",
    "        try:\n",
    "            for j in range(len(ndel)):\n",
    "                u,v=list(base_graph.edges())[ndel[j]][0:2]\n",
    "                \n",
    "                tempg.remove_edge(u, v)\n",
    "            \n",
    "            if nx.is_connected(tempg):\n",
    "                for j in range(len(ndel)):\n",
    "                    u,v=list(base_graph.edges())[ndel[j]][0:2]\n",
    "                    # G0.remove_edge(u, v)\n",
    "                    G0.edges[u,v]['weight'] = 1000000000\n",
    "                break\n",
    "            else:\n",
    "                print(\"try again\")\n",
    "                continue\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except  nx.NetworkXError:\n",
    "            continue\n",
    "    # list_node_weights_train.append(np.random.random(max_node).reshape(1,-1))\n",
    "    return G0.to_directed()\n",
    "\n",
    "def get_fm_centrality(graph: nx.Graph):\n",
    "    pfilename=\"test.edg\"\n",
    "\n",
    "    # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "    total_sum_new=distance_sum(graph)\n",
    "    # eff=total_sum_orig/total_sum_new\n",
    "    \n",
    "    centrality = total_sum_new\n",
    "        \n",
    "    # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "    # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "    # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "    # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "    \n",
    "\n",
    "    \n",
    "    ndim = 256\n",
    "    feature_mat = np.zeros((graph.number_of_nodes(),ndim))\n",
    "    feature_mat_excerpt = get_node_embedding(graph, dimensions = ndim, walk_length = 50, num_walks = 50, workers = 12,pfilename=pfilename).todense()\n",
    "    feature_mat[0:feature_mat_excerpt.shape[0]] = feature_mat_excerpt\n",
    "    feature_mat = sparse.csr_matrix(feature_mat)\n",
    "\n",
    "    # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "\n",
    "    feature_mat = sparse_mx_to_torch_sparse_tensor(feature_mat)\n",
    "\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    edge_weight = torch.tensor([graph[u][v]['weight'] for u, v in graph.edges])\n",
    "\n",
    "    return edge_index, edge_weight, centrality, feature_mat\n",
    "\n",
    "def get_centrality(graph):\n",
    "    # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "    total_sum_new=distance_sum(graph)\n",
    "    # eff=total_sum_orig/total_sum_new\n",
    "    centrality = total_sum_new\n",
    "        \n",
    "    # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "    # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "    # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "    # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "    \n",
    "    # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "    # list_adj.append(adj_mat)\n",
    "    \n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    edge_weight = torch.tensor([graph[u][v]['weight'] for u, v in graph.edges])\n",
    "\n",
    "    return edge_index, edge_weight, centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNSample:\n",
    "    def __init__(self, reference_graph, n_nodes, prob_edge):\n",
    "        self.base_graph = generate_random_adjacency_matrix(n_nodes, prob_edge)\n",
    "        self.base_graph = faq_align(reference_graph, self.base_graph, seed=123, max_iter=30)\n",
    "\n",
    "        self.mod_graph = damage_graph(self.base_graph)\n",
    "\n",
    "        self.base_graph = nx.from_numpy_array(self.base_graph)\n",
    "        self.mod_graph = nx.from_numpy_array(self.mod_graph).to_directed()\n",
    "\n",
    "        self.base_edge_index, self.base_edge_weight, self.base_centrality, self.base_fm = get_fm_centrality(self.base_graph)\n",
    "        self.mod_edge_index, self.mod_edge_weight, self.mod_centrality, self.mod_fm = get_fm_centrality(self.mod_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(filefolder):\n",
    "    model.train() \n",
    "    \n",
    "    loss_train = 0\n",
    "    num_files_train = len(filefolder)\n",
    "    \n",
    "    starttime3 = time.time()\n",
    "    y_trues=[]\n",
    "    y_preds=[]\n",
    "    \n",
    "    random.shuffle(filefolder)\n",
    "    n=0\n",
    "    \n",
    "    for i in range(num_files_train):\n",
    "        print(\"Batch: \"+str(i))\n",
    "\n",
    "        filepath=filefolder[i]\n",
    "        with open(filepath,\"rb\") as f:\n",
    "            # list_adj_deg_train_shuffled, list_adj_wt_train_shuffled,list_num_edge_train, bc_mat_train_shuffled, list_fm_train_shuffled, list_n_seq_train, list_n_unshuffle_train,list_adj_deg_test_shuffled, list_adj_wt_test_shuffled,list_num_edge_test, bc_mat_test_shuffled, list_fm_test_shuffled, list_n_seq_test, list_n_unshuffle_test=pickle.load(f)\n",
    "            graphs=pickle.load(f)\n",
    "        \n",
    "        # for j in range(len(list_train_graph)):\n",
    "        #     print(len(list_train_graph[j].edges()))\n",
    "        \n",
    "        random.shuffle(graphs)\n",
    "        \n",
    "        for sample in graphs:\n",
    "            \n",
    "            # g=list_train_graph\n",
    "            # adj2=torch.tensor(nx.adjacency_matrix(list_train_graph[j]).toarray()).numpy()\n",
    "            \n",
    "            # for \n",
    "            \n",
    "            \n",
    "            edge_index = sample.mod_edge_index\n",
    "            edge_weigth = sample.mod_edge_weight\n",
    "            \n",
    "            # adj=torch.tensor(edge_list_to_adjacency_matrix(edge_index.numpy().transpose(), edge_weigth.numpy(), 500))\n",
    "            # adj=adj.to(device)\n",
    "            \n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_weigth = edge_weigth.to(device)\n",
    "            fm0 = sample.base_fm\n",
    "            # fm=torch.tensor(np.eye(500,dtype=np.float32))\n",
    "            \n",
    "            batch_vector=np.zeros((fm0.shape[0]),dtype=np.int64)\n",
    "            batch_vector=torch.tensor(batch_vector)\n",
    "            batch_vector=batch_vector.to(device)\n",
    "            \n",
    "            fm0=fm0.to(device).to_dense()\n",
    "\n",
    "            fm1 = sample.mod_fm\n",
    "            # fm=torch.tensor(np.eye(500,dtype=np.float32))\n",
    "            \n",
    "            batch_vector=np.zeros((fm1.shape[0]),dtype=np.int64)\n",
    "            batch_vector=torch.tensor(batch_vector)\n",
    "            batch_vector=batch_vector.to(device)\n",
    "            \n",
    "            fm1=fm1.to(device).to_dense()\n",
    "            # adj_orig=torch.tensor(nx.adjacency_matrix(list_train_graph_orig[j]).toarray())\n",
    "            edge_index_orig = sample.base_edge_index\n",
    "            edge_weigth_orig = sample.base_edge_weight\n",
    "            \n",
    "            # adj_orig=torch.tensor(edge_list_to_adjacency_matrix(edge_index_orig.numpy().transpose(), edge_weigth_orig.numpy(), 500))\n",
    "            # adj_orig=adj_orig.to(device)\n",
    "            \n",
    "            # adjn=adj.cpu().numpy()\n",
    "            # adjon=adj_orig.cpu().numpy()\n",
    "            # adjr=adjon-adjn\n",
    "            # adjr[adjr!=0]=1\n",
    "            # adjr=torch.tensor(adjr.astype(np.float32)).to(device)\n",
    "            \n",
    "            # edge_index_n=edge_index.cpu().numpy()\n",
    "            # edge_index_orig_n=edge_index_orig.cpu().numpy()\n",
    "            # edge_weight_n=edge_weigth.cpu().numpy()\n",
    "            # edge_weight_n[edge_weight_n<10000]=1\n",
    "            # edge_weight_n[edge_weight_n!=1]=0\n",
    "            # edge_mask=torch.tensor(edge_weight_n.astype(np.int64)).to(device)\n",
    "            \n",
    "            edge_index_orig=edge_index_orig.to(device)\n",
    "            \n",
    "            edge_weigth_orig=edge_weigth_orig.to(device)\n",
    "            \n",
    "            y_true=torch.tensor(float(sample.ratio_normalized)).reshape([1,1])\n",
    "            y_true=y_true.to(device)\n",
    "            \n",
    "            # adj = list_adj_deg_train_shuffled[j]\n",
    "            # num_nodes = list_num_edge_train[j]\n",
    "            # adj_t = list_adj_wt_train_shuffled[j]\n",
    "            # fm = list_fm_train_shuffled[j]\n",
    "            # adj = adj.to(device)\n",
    "            # adj_t = adj_t.to(device)\n",
    "            # fm=fm.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # y_out = model(adj,adj_t,fm)\n",
    "            y_out = model(edge_index,edge_weigth.float(),edge_index_orig,edge_weigth_orig.float(),fm0.float(), fm1.float(),batch_vector)#,adjr.float(),edge_mask)#,adj,adj_orig)\n",
    "                        \n",
    "            # true_arr = torch.from_numpy(bc_mat_train_shuffled[:,j]).float()\n",
    "            # true_val = true_arr.to(device)\n",
    "            \n",
    "            # true_val_final = true_val[list_n_unshuffle_train[j]]\n",
    "            # y_out_final = y_out[list_n_unshuffle_train[j]]\n",
    "            \n",
    "            # true_val_ultimate = true_val_final[:num_nodes]\n",
    "            # y_out_ultimate = y_out_final[:num_nodes]\n",
    "            \n",
    "            mse = loss_fn(y_out, y_true)\n",
    "            \n",
    "            mse.backward()\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            y_trues.append(float(y_true))\n",
    "            y_preds.append(float(y_out))\n",
    "            \n",
    "            \n",
    "            # loss_rank = new_loss_cal(y_out_ultimate, true_val_ultimate, num_nodes, device, model_size)\n",
    "            loss_train = loss_train + float(mse)\n",
    "            n+=1\n",
    "            \n",
    "    mse_final=loss_train/n\n",
    "    duration3 = time.time()-starttime3\n",
    "    print(\"training time this epoch:\"+str(duration3))\n",
    "    print(\"training loss:\"+str(loss_train))\n",
    "    print(\"training rmse:\"+str(np.sqrt(float(mse_final))))\n",
    "    \n",
    "    score=r2_score(y_trues, y_preds)\n",
    "    print(\"Training_R_Squared:\"+str(score))\n",
    "    \n",
    "    return loss_train,mse_final,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Total Number of epoches: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57399231f2a747c6a17a549bea941a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2100203037261963\n",
      "training loss:104.16292679118487\n",
      "training rmse:1.0206024044219417\n",
      "Training_R_Squared:-102.89151582464207\n",
      "Epoch number: 2/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2862491607666016\n",
      "training loss:103.32172765536234\n",
      "training rmse:1.0164729590862824\n",
      "Training_R_Squared:-102.05250908239155\n",
      "Epoch number: 3/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.421039581298828\n",
      "training loss:101.5781016186811\n",
      "training rmse:1.0078596212701505\n",
      "Training_R_Squared:-100.3134261212375\n",
      "Epoch number: 4/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.83894681930542\n",
      "training loss:100.41281683396664\n",
      "training rmse:1.002061958333748\n",
      "Training_R_Squared:-99.15117736546598\n",
      "Epoch number: 5/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6491472721099854\n",
      "training loss:98.82617424521595\n",
      "training rmse:0.9941135460560627\n",
      "Training_R_Squared:-97.56867015618185\n",
      "Epoch number: 6/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.127509593963623\n",
      "training loss:97.75327560509322\n",
      "training rmse:0.9887025619724732\n",
      "Training_R_Squared:-96.4985665700252\n",
      "Epoch number: 7/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7559401988983154\n",
      "training loss:97.34679551191266\n",
      "training rmse:0.9866447968337575\n",
      "Training_R_Squared:-96.09314534076594\n",
      "Epoch number: 8/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8598718643188477\n",
      "training loss:95.52949486250509\n",
      "training rmse:0.9773919114792443\n",
      "Training_R_Squared:-94.28058054553671\n",
      "Epoch number: 9/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.469752550125122\n",
      "training loss:94.52029442782077\n",
      "training rmse:0.972215482430828\n",
      "Training_R_Squared:-93.27400987473607\n",
      "Epoch number: 10/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.127044200897217\n",
      "training loss:92.80867588159163\n",
      "training rmse:0.9633725960478201\n",
      "Training_R_Squared:-91.56685116839937\n",
      "Epoch number: 11/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9292736053466797\n",
      "training loss:92.85186530987266\n",
      "training rmse:0.9635967274221756\n",
      "Training_R_Squared:-91.60992700717811\n",
      "Epoch number: 12/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9193038940429688\n",
      "training loss:89.52514933890052\n",
      "training rmse:0.9461773054713399\n",
      "Training_R_Squared:-88.2918793911235\n",
      "Epoch number: 13/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7266721725463867\n",
      "training loss:88.51413495349698\n",
      "training rmse:0.940819509542064\n",
      "Training_R_Squared:-87.28349973486931\n",
      "Epoch number: 14/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9558990001678467\n",
      "training loss:88.6944630118087\n",
      "training rmse:0.9417773782153014\n",
      "Training_R_Squared:-87.46335754380495\n",
      "Epoch number: 15/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7442660331726074\n",
      "training loss:87.56925864459481\n",
      "training rmse:0.935784476493358\n",
      "Training_R_Squared:-86.34108619670225\n",
      "Epoch number: 16/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8752617835998535\n",
      "training loss:87.90989575250569\n",
      "training rmse:0.937602771713617\n",
      "Training_R_Squared:-86.68083487807417\n",
      "Epoch number: 17/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.72333025932312\n",
      "training loss:84.35112537466921\n",
      "training rmse:0.9184286873495906\n",
      "Training_R_Squared:-83.13133724517716\n",
      "Epoch number: 18/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.842751979827881\n",
      "training loss:83.91246756038163\n",
      "training rmse:0.9160374859162786\n",
      "Training_R_Squared:-82.693822332902\n",
      "Epoch number: 19/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.905693769454956\n",
      "training loss:82.5119792278856\n",
      "training rmse:0.9083610473148086\n",
      "Training_R_Squared:-81.29698302252285\n",
      "Epoch number: 20/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8133842945098877\n",
      "training loss:81.17250877059996\n",
      "training rmse:0.9009578723258928\n",
      "Training_R_Squared:-79.96100291813677\n",
      "Epoch number: 21/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9388957023620605\n",
      "training loss:80.47855015099049\n",
      "training rmse:0.8970983789473175\n",
      "Training_R_Squared:-79.26885318479987\n",
      "Epoch number: 22/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8200252056121826\n",
      "training loss:79.07064649031963\n",
      "training rmse:0.8892167704801773\n",
      "Training_R_Squared:-77.86461747653601\n",
      "Epoch number: 23/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7208664417266846\n",
      "training loss:78.34140172833577\n",
      "training rmse:0.8851067829834758\n",
      "Training_R_Squared:-77.13727287226436\n",
      "Epoch number: 24/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8860597610473633\n",
      "training loss:77.3218180956319\n",
      "training rmse:0.8793282555202687\n",
      "Training_R_Squared:-76.12034529099915\n",
      "Epoch number: 25/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6996219158172607\n",
      "training loss:76.88782681524754\n",
      "training rmse:0.8768570397462037\n",
      "Training_R_Squared:-75.68748552279344\n",
      "Epoch number: 26/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.895393133163452\n",
      "training loss:73.07364695519209\n",
      "training rmse:0.8548312520912655\n",
      "Training_R_Squared:-71.88324382335001\n",
      "Epoch number: 27/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8237690925598145\n",
      "training loss:73.54960221424699\n",
      "training rmse:0.8576106471718211\n",
      "Training_R_Squared:-72.35795889490599\n",
      "Epoch number: 28/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0151772499084473\n",
      "training loss:71.39470079168677\n",
      "training rmse:0.8449538495781103\n",
      "Training_R_Squared:-70.20867262916728\n",
      "Epoch number: 29/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.054738998413086\n",
      "training loss:69.00118466094136\n",
      "training rmse:0.8306695170821027\n",
      "Training_R_Squared:-67.82139322603054\n",
      "Epoch number: 30/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.697810173034668\n",
      "training loss:68.08293810859323\n",
      "training rmse:0.8251238580273462\n",
      "Training_R_Squared:-66.90553862472265\n",
      "Epoch number: 31/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.887953281402588\n",
      "training loss:67.7507791146636\n",
      "training rmse:0.8231086144286404\n",
      "Training_R_Squared:-66.57424529025327\n",
      "Epoch number: 32/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3039355278015137\n",
      "training loss:65.86199313029647\n",
      "training rmse:0.8115540224180795\n",
      "Training_R_Squared:-64.69038069449374\n",
      "Epoch number: 33/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.3055975437164307\n",
      "training loss:65.44117753580213\n",
      "training rmse:0.8089572147882861\n",
      "Training_R_Squared:-64.2706623575041\n",
      "Epoch number: 34/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.769850730895996\n",
      "training loss:63.03728460520506\n",
      "training rmse:0.7939602295153395\n",
      "Training_R_Squared:-61.87303271000774\n",
      "Epoch number: 35/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.944974184036255\n",
      "training loss:61.065527722239494\n",
      "training rmse:0.7814443532474945\n",
      "Training_R_Squared:-59.90641337292647\n",
      "Epoch number: 36/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.828094244003296\n",
      "training loss:61.432589542120695\n",
      "training rmse:0.783789445847038\n",
      "Training_R_Squared:-60.27251880113804\n",
      "Epoch number: 37/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.03249192237854\n",
      "training loss:56.48166236281395\n",
      "training rmse:0.7515428288714752\n",
      "Training_R_Squared:-55.33449148295842\n",
      "Epoch number: 38/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.760528802871704\n",
      "training loss:56.51064873486757\n",
      "training rmse:0.751735649912039\n",
      "Training_R_Squared:-55.363402264846485\n",
      "Epoch number: 39/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9510393142700195\n",
      "training loss:55.417474798858166\n",
      "training rmse:0.7444291423557932\n",
      "Training_R_Squared:-54.273077078554046\n",
      "Epoch number: 40/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.847785711288452\n",
      "training loss:53.8031313046813\n",
      "training rmse:0.7335061779200044\n",
      "Training_R_Squared:-52.66293974461273\n",
      "Epoch number: 41/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3717942237854004\n",
      "training loss:51.908158987760544\n",
      "training rmse:0.7204731708242892\n",
      "Training_R_Squared:-50.772905600006574\n",
      "Epoch number: 42/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.198239326477051\n",
      "training loss:49.241822212934494\n",
      "training rmse:0.7017251756416788\n",
      "Training_R_Squared:-48.11351604796788\n",
      "Epoch number: 43/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9699742794036865\n",
      "training loss:48.837735660374165\n",
      "training rmse:0.6988400078728618\n",
      "Training_R_Squared:-47.71048249288266\n",
      "Epoch number: 44/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.823800563812256\n",
      "training loss:47.70184137672186\n",
      "training rmse:0.6906651965802378\n",
      "Training_R_Squared:-46.57754816659706\n",
      "Epoch number: 45/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.976468801498413\n",
      "training loss:44.19151172786951\n",
      "training rmse:0.6647669646415164\n",
      "Training_R_Squared:-43.07636496656073\n",
      "Epoch number: 46/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.749931573867798\n",
      "training loss:43.303120121359825\n",
      "training rmse:0.6580510627706624\n",
      "Training_R_Squared:-42.190288400799226\n",
      "Epoch number: 47/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9144935607910156\n",
      "training loss:42.00045661628246\n",
      "training rmse:0.6480775927023126\n",
      "Training_R_Squared:-40.8910190637841\n",
      "Epoch number: 48/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9057672023773193\n",
      "training loss:38.646313831210136\n",
      "training rmse:0.6216615946896683\n",
      "Training_R_Squared:-37.54561579848699\n",
      "Epoch number: 49/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7765557765960693\n",
      "training loss:38.73135159164667\n",
      "training rmse:0.6223451742533774\n",
      "Training_R_Squared:-37.63043146205814\n",
      "Epoch number: 50/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.879749298095703\n",
      "training loss:35.34557615965605\n",
      "training rmse:0.5945214559598001\n",
      "Training_R_Squared:-34.25347863840023\n",
      "Epoch number: 51/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9320080280303955\n",
      "training loss:35.09687101840973\n",
      "training rmse:0.5924261221317788\n",
      "Training_R_Squared:-34.00542147622174\n",
      "Epoch number: 52/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.76282000541687\n",
      "training loss:31.974924113135785\n",
      "training rmse:0.5654637398908597\n",
      "Training_R_Squared:-30.89160961354211\n",
      "Epoch number: 53/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.824225664138794\n",
      "training loss:29.73336648568511\n",
      "training rmse:0.5452831052369503\n",
      "Training_R_Squared:-28.655892018139237\n",
      "Epoch number: 54/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.912856101989746\n",
      "training loss:28.0361467609182\n",
      "training rmse:0.5294917068370212\n",
      "Training_R_Squared:-26.963094986509994\n",
      "Epoch number: 55/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8206264972686768\n",
      "training loss:26.953283924609423\n",
      "training rmse:0.519165522012098\n",
      "Training_R_Squared:-25.883053796264758\n",
      "Epoch number: 56/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.925774574279785\n",
      "training loss:25.694843787699938\n",
      "training rmse:0.5069008166071538\n",
      "Training_R_Squared:-24.627892770659763\n",
      "Epoch number: 57/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6721692085266113\n",
      "training loss:22.425387630030855\n",
      "training rmse:0.47355451249070424\n",
      "Training_R_Squared:-21.366955366464143\n",
      "Epoch number: 58/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9396557807922363\n",
      "training loss:22.249413007870317\n",
      "training rmse:0.4716928344576618\n",
      "Training_R_Squared:-21.19143919339869\n",
      "Epoch number: 59/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7785394191741943\n",
      "training loss:19.91135614796076\n",
      "training rmse:0.4462214265133484\n",
      "Training_R_Squared:-18.859474152037905\n",
      "Epoch number: 60/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8826968669891357\n",
      "training loss:19.010442283703014\n",
      "training rmse:0.4360096591097841\n",
      "Training_R_Squared:-17.96090815106095\n",
      "Epoch number: 61/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8924784660339355\n",
      "training loss:18.545945885471156\n",
      "training rmse:0.43065004220911385\n",
      "Training_R_Squared:-17.497622147525554\n",
      "Epoch number: 62/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.543633460998535\n",
      "training loss:18.009524955879897\n",
      "training rmse:0.4243763065473837\n",
      "Training_R_Squared:-16.96259892960531\n",
      "Epoch number: 63/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.280174493789673\n",
      "training loss:15.053995596532332\n",
      "training rmse:0.3879947885801088\n",
      "Training_R_Squared:-14.014770629304696\n",
      "Epoch number: 64/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.740016222000122\n",
      "training loss:14.295440542773576\n",
      "training rmse:0.378093117403287\n",
      "Training_R_Squared:-13.258191915770968\n",
      "Epoch number: 65/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.046124219894409\n",
      "training loss:14.221085637580472\n",
      "training rmse:0.3771085472059798\n",
      "Training_R_Squared:-13.184030872844149\n",
      "Epoch number: 66/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.733044147491455\n",
      "training loss:11.88112760038348\n",
      "training rmse:0.3446901159067878\n",
      "Training_R_Squared:-10.85016967447411\n",
      "Epoch number: 67/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9024429321289062\n",
      "training loss:12.17051318497397\n",
      "training rmse:0.34886262604317436\n",
      "Training_R_Squared:-11.138801278097816\n",
      "Epoch number: 68/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8557002544403076\n",
      "training loss:11.163704673301254\n",
      "training rmse:0.33412130541618046\n",
      "Training_R_Squared:-10.13461614064584\n",
      "Epoch number: 69/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9924330711364746\n",
      "training loss:9.450368386111222\n",
      "training rmse:0.30741451472094194\n",
      "Training_R_Squared:-8.42574423960928\n",
      "Epoch number: 70/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.752642869949341\n",
      "training loss:9.988929180605737\n",
      "training rmse:0.31605267251845437\n",
      "Training_R_Squared:-8.962901711554068\n",
      "Epoch number: 71/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.868086814880371\n",
      "training loss:8.993710467217397\n",
      "training rmse:0.2998951561332293\n",
      "Training_R_Squared:-7.970276067672394\n",
      "Epoch number: 72/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.943146228790283\n",
      "training loss:7.724064220287801\n",
      "training rmse:0.27792200741013295\n",
      "Training_R_Squared:-6.703938247369014\n",
      "Epoch number: 73/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.792344093322754\n",
      "training loss:7.9227620570673025\n",
      "training rmse:0.28147401402380473\n",
      "Training_R_Squared:-6.9021183595434445\n",
      "Epoch number: 74/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9396438598632812\n",
      "training loss:7.847360912013755\n",
      "training rmse:0.2801314140187379\n",
      "Training_R_Squared:-6.82691346418038\n",
      "Epoch number: 75/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7605373859405518\n",
      "training loss:7.38890047734003\n",
      "training rmse:0.2718253203316429\n",
      "Training_R_Squared:-6.369647707936981\n",
      "Epoch number: 76/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.404742956161499\n",
      "training loss:6.384924906044034\n",
      "training rmse:0.2526840894485451\n",
      "Training_R_Squared:-5.368288168694769\n",
      "Epoch number: 77/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.312138557434082\n",
      "training loss:7.46688025418058\n",
      "training rmse:0.27325592864896053\n",
      "Training_R_Squared:-6.447424310032333\n",
      "Epoch number: 78/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.795980930328369\n",
      "training loss:7.916427552851928\n",
      "training rmse:0.2813614677394886\n",
      "Training_R_Squared:-6.89580032186263\n",
      "Epoch number: 79/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9094600677490234\n",
      "training loss:6.471446825539715\n",
      "training rmse:0.25439038554040744\n",
      "Training_R_Squared:-5.454584620497905\n",
      "Epoch number: 80/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9772183895111084\n",
      "training loss:5.634558128261233\n",
      "training rmse:0.2373722420221293\n",
      "Training_R_Squared:-4.619876498229595\n",
      "Epoch number: 81/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.742539882659912\n",
      "training loss:6.244169289250749\n",
      "training rmse:0.24988335857457072\n",
      "Training_R_Squared:-5.227899228480439\n",
      "Epoch number: 82/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.668275833129883\n",
      "training loss:5.50613201564579\n",
      "training rmse:0.2346514865848028\n",
      "Training_R_Squared:-4.491785139573217\n",
      "Epoch number: 83/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9209799766540527\n",
      "training loss:6.2204111761745935\n",
      "training rmse:0.24940752146185555\n",
      "Training_R_Squared:-5.204203022667235\n",
      "Epoch number: 84/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.729058265686035\n",
      "training loss:6.554506978456629\n",
      "training rmse:0.25601771381013133\n",
      "Training_R_Squared:-5.5374283006165514\n",
      "Epoch number: 85/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.858304262161255\n",
      "training loss:5.903961221974271\n",
      "training rmse:0.24298068281191143\n",
      "Training_R_Squared:-4.888577661722421\n",
      "Epoch number: 86/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9260146617889404\n",
      "training loss:5.593103671589233\n",
      "training rmse:0.23649743490340933\n",
      "Training_R_Squared:-4.578530102669148\n",
      "Epoch number: 87/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.80816650390625\n",
      "training loss:6.13169551907464\n",
      "training rmse:0.24762260638065015\n",
      "Training_R_Squared:-5.1157185596773065\n",
      "Epoch number: 88/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8376412391662598\n",
      "training loss:6.5568583293345455\n",
      "training rmse:0.2560636313367157\n",
      "Training_R_Squared:-5.539773564758741\n",
      "Epoch number: 89/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849884033203125\n",
      "training loss:5.4194656549916544\n",
      "training rmse:0.23279745821188974\n",
      "Training_R_Squared:-4.405344557409876\n",
      "Epoch number: 90/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.865614175796509\n",
      "training loss:5.534610565040623\n",
      "training rmse:0.2352575304860744\n",
      "Training_R_Squared:-4.520189443965479\n",
      "Epoch number: 91/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7743799686431885\n",
      "training loss:5.672587153280574\n",
      "training rmse:0.238171936912823\n",
      "Training_R_Squared:-4.657806499922758\n",
      "Epoch number: 92/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.797173023223877\n",
      "training loss:5.264017537729615\n",
      "training rmse:0.22943446859026248\n",
      "Training_R_Squared:-4.250301481940782\n",
      "Epoch number: 93/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.805089235305786\n",
      "training loss:5.409631562388995\n",
      "training rmse:0.2325861466723458\n",
      "Training_R_Squared:-4.3955361261880554\n",
      "Epoch number: 94/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.060750722885132\n",
      "training loss:4.958807059564606\n",
      "training rmse:0.22268379059923976\n",
      "Training_R_Squared:-3.9458862091479228\n",
      "Epoch number: 95/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.387924909591675\n",
      "training loss:5.894837694749185\n",
      "training rmse:0.2427928684032788\n",
      "Training_R_Squared:-4.879477889399401\n",
      "Epoch number: 96/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1647632122039795\n",
      "training loss:5.595258419125457\n",
      "training rmse:0.23654298592698658\n",
      "Training_R_Squared:-4.580679246000475\n",
      "Epoch number: 97/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.895782470703125\n",
      "training loss:5.199996830698183\n",
      "training rmse:0.22803501552827765\n",
      "Training_R_Squared:-4.186447535774148\n",
      "Epoch number: 98/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8517415523529053\n",
      "training loss:5.692332355771214\n",
      "training rmse:0.23858609254881588\n",
      "Training_R_Squared:-4.677500230209034\n",
      "Epoch number: 99/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8361799716949463\n",
      "training loss:5.875227626832384\n",
      "training rmse:0.24238868840835753\n",
      "Training_R_Squared:-4.85991901605014\n",
      "Epoch number: 100/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.90561580657959\n",
      "training loss:5.466743991335136\n",
      "training rmse:0.23381069247010788\n",
      "Training_R_Squared:-4.4524997667837685\n",
      "Epoch number: 101/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9206714630126953\n",
      "training loss:5.064813071243634\n",
      "training rmse:0.22505139571314892\n",
      "Training_R_Squared:-4.051616014116363\n",
      "Epoch number: 102/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8551688194274902\n",
      "training loss:4.631354578164519\n",
      "training rmse:0.21520582190462503\n",
      "Training_R_Squared:-3.6192868934643174\n",
      "Epoch number: 103/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8874247074127197\n",
      "training loss:5.606028476451229\n",
      "training rmse:0.23677053187529964\n",
      "Training_R_Squared:-4.59142122479418\n",
      "Epoch number: 104/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8856194019317627\n",
      "training loss:5.442633538299106\n",
      "training rmse:0.2332945249743145\n",
      "Training_R_Squared:-4.428452036715235\n",
      "Epoch number: 105/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7670538425445557\n",
      "training loss:5.125615861559254\n",
      "training rmse:0.22639823015119295\n",
      "Training_R_Squared:-4.112260421500864\n",
      "Epoch number: 106/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9096460342407227\n",
      "training loss:4.38096788224766\n",
      "training rmse:0.2093076176885987\n",
      "Training_R_Squared:-3.3695526564812024\n",
      "Epoch number: 107/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.83994197845459\n",
      "training loss:5.155625064824562\n",
      "training rmse:0.22706001552066718\n",
      "Training_R_Squared:-4.142191440628883\n",
      "Epoch number: 108/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.883622646331787\n",
      "training loss:6.156711533441012\n",
      "training rmse:0.2481272160292178\n",
      "Training_R_Squared:-5.140669390846995\n",
      "Epoch number: 109/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.82684588432312\n",
      "training loss:4.261057224988875\n",
      "training rmse:0.20642328417571684\n",
      "Training_R_Squared:-3.2499545208350913\n",
      "Epoch number: 110/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8105545043945312\n",
      "training loss:5.203360030019894\n",
      "training rmse:0.22810874665430728\n",
      "Training_R_Squared:-4.189801961242068\n",
      "Epoch number: 111/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.073025941848755\n",
      "training loss:4.925275306326512\n",
      "training rmse:0.22192961285791746\n",
      "Training_R_Squared:-3.912441876109501\n",
      "Epoch number: 112/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9776413440704346\n",
      "training loss:5.3655827740479936\n",
      "training rmse:0.2316372762326477\n",
      "Training_R_Squared:-4.351602037955706\n",
      "Epoch number: 113/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8443753719329834\n",
      "training loss:4.49693385043156\n",
      "training rmse:0.21205975220280626\n",
      "Training_R_Squared:-3.4852164687819176\n",
      "Epoch number: 114/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3250210285186768\n",
      "training loss:5.171379472510324\n",
      "training rmse:0.22740667256064243\n",
      "Training_R_Squared:-4.15790472569648\n",
      "Epoch number: 115/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.09863018989563\n",
      "training loss:4.9573015523500885\n",
      "training rmse:0.22264998433303534\n",
      "Training_R_Squared:-3.944384685153979\n",
      "Epoch number: 116/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.869379758834839\n",
      "training loss:5.500371863518012\n",
      "training rmse:0.23452871601400993\n",
      "Training_R_Squared:-4.4860398563532025\n",
      "Epoch number: 117/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.71525239944458\n",
      "training loss:5.396577930892818\n",
      "training rmse:0.23230535789974405\n",
      "Training_R_Squared:-4.382516437752455\n",
      "Epoch number: 118/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9121463298797607\n",
      "training loss:4.406158333392\n",
      "training rmse:0.20990851181864922\n",
      "Training_R_Squared:-3.3946774816103273\n",
      "Epoch number: 119/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8574769496917725\n",
      "training loss:4.249957770287438\n",
      "training rmse:0.2061542570573656\n",
      "Training_R_Squared:-3.238883973512583\n",
      "Epoch number: 120/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8599212169647217\n",
      "training loss:4.391962507874609\n",
      "training rmse:0.20957009585994396\n",
      "Training_R_Squared:-3.38051868340907\n",
      "Epoch number: 121/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8656859397888184\n",
      "training loss:4.8244497821297045\n",
      "training rmse:0.21964630163355142\n",
      "Training_R_Squared:-3.811878979392322\n",
      "Epoch number: 122/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.855806827545166\n",
      "training loss:4.57757513461047\n",
      "training rmse:0.21395268483032573\n",
      "Training_R_Squared:-3.5656476573441918\n",
      "Epoch number: 123/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.857170820236206\n",
      "training loss:3.9916280635656562\n",
      "training rmse:0.19979059195982318\n",
      "Training_R_Squared:-2.9812273740226343\n",
      "Epoch number: 124/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.85217547416687\n",
      "training loss:4.570371041243561\n",
      "training rmse:0.2137842613768273\n",
      "Training_R_Squared:-3.5584622487473725\n",
      "Epoch number: 125/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.900994300842285\n",
      "training loss:5.362635431249203\n",
      "training rmse:0.23157364770735903\n",
      "Training_R_Squared:-4.3486624464506916\n",
      "Epoch number: 126/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9838008880615234\n",
      "training loss:4.367479146043479\n",
      "training rmse:0.20898514650671898\n",
      "Training_R_Squared:-3.3560991184979745\n",
      "Epoch number: 127/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.944430112838745\n",
      "training loss:4.2873533934980514\n",
      "training rmse:0.2070592522322548\n",
      "Training_R_Squared:-3.276182144346498\n",
      "Epoch number: 128/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6695306301116943\n",
      "training loss:4.849434756885557\n",
      "training rmse:0.22021432189768125\n",
      "Training_R_Squared:-3.836798964296011\n",
      "Epoch number: 129/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7981367111206055\n",
      "training loss:4.792122542640072\n",
      "training rmse:0.2189091716360937\n",
      "Training_R_Squared:-3.7796360152529482\n",
      "Epoch number: 130/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9087507724761963\n",
      "training loss:4.761564284096494\n",
      "training rmse:0.2182100887698938\n",
      "Training_R_Squared:-3.749157436161644\n",
      "Epoch number: 131/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7983834743499756\n",
      "training loss:4.813046607716387\n",
      "training rmse:0.2193865676771572\n",
      "Training_R_Squared:-3.8005055979645483\n",
      "Epoch number: 132/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8343658447265625\n",
      "training loss:3.9513329869514564\n",
      "training rmse:0.19877960124095873\n",
      "Training_R_Squared:-2.941037266791007\n",
      "Epoch number: 133/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0590484142303467\n",
      "training loss:3.70604998115083\n",
      "training rmse:0.19251103815498036\n",
      "Training_R_Squared:-2.6963933651621605\n",
      "Epoch number: 134/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8935158252716064\n",
      "training loss:4.64025056504579\n",
      "training rmse:0.21541240830197758\n",
      "Training_R_Squared:-3.6281598013873007\n",
      "Epoch number: 135/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.826495409011841\n",
      "training loss:4.053638708044218\n",
      "training rmse:0.20133650210640439\n",
      "Training_R_Squared:-3.043076416397575\n",
      "Epoch number: 136/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9218878746032715\n",
      "training loss:3.6917782487653312\n",
      "training rmse:0.19214000751445107\n",
      "Training_R_Squared:-2.6821588654100816\n",
      "Epoch number: 137/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3122811317443848\n",
      "training loss:3.9833406156876663\n",
      "training rmse:0.19958308083822301\n",
      "Training_R_Squared:-2.972961543196569\n",
      "Epoch number: 138/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.229656219482422\n",
      "training loss:4.124106163304532\n",
      "training rmse:0.20307895418542346\n",
      "Training_R_Squared:-3.113360277070348\n",
      "Epoch number: 139/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.831777811050415\n",
      "training loss:4.156242323173501\n",
      "training rmse:0.20386864210009104\n",
      "Training_R_Squared:-3.1454126894392846\n",
      "Epoch number: 140/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.887179374694824\n",
      "training loss:3.2193359490261173\n",
      "training rmse:0.17942508043821848\n",
      "Training_R_Squared:-2.210947545957791\n",
      "Epoch number: 141/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.908567428588867\n",
      "training loss:4.005371826201554\n",
      "training rmse:0.20013425059698187\n",
      "Training_R_Squared:-2.994935316404045\n",
      "Epoch number: 142/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8534810543060303\n",
      "training loss:3.939346243079626\n",
      "training rmse:0.1984778638306959\n",
      "Training_R_Squared:-2.9290817659883257\n",
      "Epoch number: 143/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6802706718444824\n",
      "training loss:4.22529562306741\n",
      "training rmse:0.20555523887917354\n",
      "Training_R_Squared:-3.214286053710861\n",
      "Epoch number: 144/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6203246116638184\n",
      "training loss:4.492089891327183\n",
      "training rmse:0.2119455093019709\n",
      "Training_R_Squared:-3.480385175399042\n",
      "Epoch number: 145/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8458266258239746\n",
      "training loss:3.59976932030213\n",
      "training rmse:0.1897305805689249\n",
      "Training_R_Squared:-2.5903896146588563\n",
      "Epoch number: 146/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3390238285064697\n",
      "training loss:3.8467474260843915\n",
      "training rmse:0.1961312679325862\n",
      "Training_R_Squared:-2.8367242272421724\n",
      "Epoch number: 147/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2374541759490967\n",
      "training loss:4.204914087484212\n",
      "training rmse:0.2050588717291747\n",
      "Training_R_Squared:-3.1939576538363257\n",
      "Epoch number: 148/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.87853741645813\n",
      "training loss:3.4187804135704027\n",
      "training rmse:0.18489944330825883\n",
      "Training_R_Squared:-2.40987228964564\n",
      "Epoch number: 149/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.884463310241699\n",
      "training loss:3.6266985360663284\n",
      "training rmse:0.19043892816507682\n",
      "Training_R_Squared:-2.617248673145667\n",
      "Epoch number: 150/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.828599691390991\n",
      "training loss:3.4290727374368544\n",
      "training rmse:0.18517755634624986\n",
      "Training_R_Squared:-2.42013783494361\n",
      "Epoch number: 151/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.904506206512451\n",
      "training loss:3.8134268415997212\n",
      "training rmse:0.19527997443669745\n",
      "Training_R_Squared:-2.8034904584475338\n",
      "Epoch number: 152/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8790674209594727\n",
      "training loss:3.535322279043612\n",
      "training rmse:0.1880245270980255\n",
      "Training_R_Squared:-2.5261105359462497\n",
      "Epoch number: 153/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8547770977020264\n",
      "training loss:3.874788647047353\n",
      "training rmse:0.19684482840672632\n",
      "Training_R_Squared:-2.864692372590123\n",
      "Epoch number: 154/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9057071208953857\n",
      "training loss:3.051658712422295\n",
      "training rmse:0.1746899743094118\n",
      "Training_R_Squared:-2.0437072104750102\n",
      "Epoch number: 155/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.867626667022705\n",
      "training loss:3.8396220719623813\n",
      "training rmse:0.19594953615567406\n",
      "Training_R_Squared:-2.829617445887487\n",
      "Epoch number: 156/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9128293991088867\n",
      "training loss:4.016630932407679\n",
      "training rmse:0.20041534203767133\n",
      "Training_R_Squared:-3.0061650853867414\n",
      "Epoch number: 157/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.790286064147949\n",
      "training loss:3.1378308670948627\n",
      "training rmse:0.17713923526691827\n",
      "Training_R_Squared:-2.129654828806149\n",
      "Epoch number: 158/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8930976390838623\n",
      "training loss:3.8391587992778113\n",
      "training rmse:0.1959377145747549\n",
      "Training_R_Squared:-2.8291553691895164\n",
      "Epoch number: 159/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8993146419525146\n",
      "training loss:3.0312974905029932\n",
      "training rmse:0.17410621730722292\n",
      "Training_R_Squared:-2.0233990515646116\n",
      "Epoch number: 160/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.819662570953369\n",
      "training loss:3.2898244648240507\n",
      "training rmse:0.18137873262386775\n",
      "Training_R_Squared:-2.281252412811235\n",
      "Epoch number: 161/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8686025142669678\n",
      "training loss:2.9787621516170475\n",
      "training rmse:0.17259090797655152\n",
      "Training_R_Squared:-1.9710005973223317\n",
      "Epoch number: 162/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8626174926757812\n",
      "training loss:3.3674526570770382\n",
      "training rmse:0.18350620308526464\n",
      "Training_R_Squared:-2.358678302011963\n",
      "Epoch number: 163/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8722152709960938\n",
      "training loss:3.5266697029119314\n",
      "training rmse:0.18779429445305124\n",
      "Training_R_Squared:-2.517480485111152\n",
      "Epoch number: 164/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.816601514816284\n",
      "training loss:2.7755265162436444\n",
      "training rmse:0.16659911513101275\n",
      "Training_R_Squared:-1.7682945128301037\n",
      "Epoch number: 165/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.846174955368042\n",
      "training loss:3.4414234493615368\n",
      "training rmse:0.18551073956408928\n",
      "Training_R_Squared:-2.4324563628037694\n",
      "Epoch number: 166/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.099362850189209\n",
      "training loss:2.9353037648788813\n",
      "training rmse:0.1713272822663945\n",
      "Training_R_Squared:-1.9276554360047666\n",
      "Epoch number: 167/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.35894775390625\n",
      "training loss:3.163868516903676\n",
      "training rmse:0.1778726656038998\n",
      "Training_R_Squared:-2.155624652433216\n",
      "Epoch number: 168/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.225425958633423\n",
      "training loss:3.3436125113391917\n",
      "training rmse:0.18285547602790547\n",
      "Training_R_Squared:-2.3349003058905757\n",
      "Epoch number: 169/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9301562309265137\n",
      "training loss:3.0787488966353465\n",
      "training rmse:0.17546364001226425\n",
      "Training_R_Squared:-2.0707267810509142\n",
      "Epoch number: 170/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9819133281707764\n",
      "training loss:2.5077166602987973\n",
      "training rmse:0.1583577172195532\n",
      "Training_R_Squared:-1.501182476472855\n",
      "Epoch number: 171/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8474674224853516\n",
      "training loss:3.127556832078426\n",
      "training rmse:0.17684899864230008\n",
      "Training_R_Squared:-2.11940755987056\n",
      "Epoch number: 172/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.874397039413452\n",
      "training loss:2.2522689030433867\n",
      "training rmse:0.15007561104467929\n",
      "Training_R_Squared:-1.2464003089915527\n",
      "Epoch number: 173/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.881847620010376\n",
      "training loss:3.3229718951545237\n",
      "training rmse:0.182290205308857\n",
      "Training_R_Squared:-2.314313452545959\n",
      "Epoch number: 174/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.883918285369873\n",
      "training loss:3.2531426624384494\n",
      "training rmse:0.18036470448617292\n",
      "Training_R_Squared:-2.244666164225307\n",
      "Epoch number: 175/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8302862644195557\n",
      "training loss:2.3977820364510762\n",
      "training rmse:0.15484773283619868\n",
      "Training_R_Squared:-1.391534304625798\n",
      "Epoch number: 176/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8799831867218018\n",
      "training loss:3.099778726248587\n",
      "training rmse:0.17606188475216852\n",
      "Training_R_Squared:-2.0917018253280864\n",
      "Epoch number: 177/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9074625968933105\n",
      "training loss:2.89804492373878\n",
      "training rmse:0.17023645096567244\n",
      "Training_R_Squared:-1.8904936817875853\n",
      "Epoch number: 178/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8822481632232666\n",
      "training loss:2.9464880929212995\n",
      "training rmse:0.17165337436011271\n",
      "Training_R_Squared:-1.9388106606641466\n",
      "Epoch number: 179/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9091780185699463\n",
      "training loss:3.1340951866528712\n",
      "training rmse:0.17703375911539787\n",
      "Training_R_Squared:-2.125928877339097\n",
      "Epoch number: 180/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.885404348373413\n",
      "training loss:2.6153609254078276\n",
      "training rmse:0.1617207755796338\n",
      "Training_R_Squared:-1.6085462641708457\n",
      "Epoch number: 181/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.418156147003174\n",
      "training loss:2.686459726349824\n",
      "training rmse:0.1639042319877624\n",
      "Training_R_Squared:-1.6794598024810115\n",
      "Epoch number: 182/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2774548530578613\n",
      "training loss:3.455037095453008\n",
      "training rmse:0.18587730080493983\n",
      "Training_R_Squared:-2.446034558719747\n",
      "Epoch number: 183/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.918213129043579\n",
      "training loss:2.9493770953013154\n",
      "training rmse:0.17173750595898718\n",
      "Training_R_Squared:-1.9416921252458401\n",
      "Epoch number: 184/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.769908905029297\n",
      "training loss:2.6629999232196724\n",
      "training rmse:0.16318700693436572\n",
      "Training_R_Squared:-1.6560611316184506\n",
      "Epoch number: 185/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8613905906677246\n",
      "training loss:2.8561406210337736\n",
      "training rmse:0.1690012018014598\n",
      "Training_R_Squared:-1.848698575533661\n",
      "Epoch number: 186/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.891686201095581\n",
      "training loss:2.685896547921402\n",
      "training rmse:0.16388705098089362\n",
      "Training_R_Squared:-1.6788980769805364\n",
      "Epoch number: 187/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.928938865661621\n",
      "training loss:2.767645423742806\n",
      "training rmse:0.16636241834449286\n",
      "Training_R_Squared:-1.7604339397213544\n",
      "Epoch number: 188/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8229591846466064\n",
      "training loss:2.706266259986478\n",
      "training rmse:0.16450733296684614\n",
      "Training_R_Squared:-1.6992147252664855\n",
      "Epoch number: 189/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.84771728515625\n",
      "training loss:2.8494204492890276\n",
      "training rmse:0.16880226447796923\n",
      "Training_R_Squared:-1.8419959010422726\n",
      "Epoch number: 190/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.915221929550171\n",
      "training loss:3.69984688761285\n",
      "training rmse:0.1923498606085497\n",
      "Training_R_Squared:-2.690206414224285\n",
      "Epoch number: 191/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.865151882171631\n",
      "training loss:2.6862051272546523\n",
      "training rmse:0.16389646510082675\n",
      "Training_R_Squared:-1.67920588531606\n",
      "Epoch number: 192/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8574059009552\n",
      "training loss:2.5958696516013333\n",
      "training rmse:0.1611170273931757\n",
      "Training_R_Squared:-1.5891057743582353\n",
      "Epoch number: 193/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.866422414779663\n",
      "training loss:2.4499503256702155\n",
      "training rmse:0.156523171628683\n",
      "Training_R_Squared:-1.4435666746729572\n",
      "Epoch number: 194/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8830349445343018\n",
      "training loss:2.6561158875394426\n",
      "training rmse:0.1629759456956591\n",
      "Training_R_Squared:-1.6491950398242992\n",
      "Epoch number: 195/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.853003978729248\n",
      "training loss:2.763663134894742\n",
      "training rmse:0.1662426881067177\n",
      "Training_R_Squared:-1.7564620525909485\n",
      "Epoch number: 196/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.565636157989502\n",
      "training loss:2.220241175087949\n",
      "training rmse:0.14900473734374856\n",
      "Training_R_Squared:-1.2144560397865587\n",
      "Epoch number: 197/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.593785047531128\n",
      "training loss:2.347445700397202\n",
      "training rmse:0.1532137624496312\n",
      "Training_R_Squared:-1.3413291194251888\n",
      "Epoch number: 198/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.827148914337158\n",
      "training loss:2.462920173911698\n",
      "training rmse:0.1569369355477447\n",
      "Training_R_Squared:-1.4565027241467257\n",
      "Epoch number: 199/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.062870502471924\n",
      "training loss:2.4898515682277775\n",
      "training rmse:0.15779263506982122\n",
      "Training_R_Squared:-1.483363924745877\n",
      "Epoch number: 200/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.328685998916626\n",
      "training loss:2.651298009950551\n",
      "training rmse:0.16282806913890957\n",
      "Training_R_Squared:-1.6443897043583533\n",
      "Epoch number: 201/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.30724835395813\n",
      "training loss:2.813071155401758\n",
      "training rmse:0.16772212601209652\n",
      "Training_R_Squared:-1.8057413311338468\n",
      "Epoch number: 202/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.735621929168701\n",
      "training loss:2.8953261601786835\n",
      "training rmse:0.17015657966057862\n",
      "Training_R_Squared:-1.8877819981775814\n",
      "Epoch number: 203/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8962700366973877\n",
      "training loss:2.1820345462189152\n",
      "training rmse:0.14771711296322154\n",
      "Training_R_Squared:-1.1763489750339073\n",
      "Epoch number: 204/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7878644466400146\n",
      "training loss:2.8632208660524725\n",
      "training rmse:0.16921054535851104\n",
      "Training_R_Squared:-1.855760365143925\n",
      "Epoch number: 205/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9116036891937256\n",
      "training loss:2.5727010742798484\n",
      "training rmse:0.16039641748741923\n",
      "Training_R_Squared:-1.5659975745904324\n",
      "Epoch number: 206/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.89093017578125\n",
      "training loss:2.6894943110214626\n",
      "training rmse:0.1639967777433893\n",
      "Training_R_Squared:-1.6824865008012662\n",
      "Epoch number: 207/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.861069679260254\n",
      "training loss:2.769750673428007\n",
      "training rmse:0.16642567931145744\n",
      "Training_R_Squared:-1.7625337151947087\n",
      "Epoch number: 208/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.858832597732544\n",
      "training loss:3.1621839354656913\n",
      "training rmse:0.17782530572069014\n",
      "Training_R_Squared:-2.153944449893227\n",
      "Epoch number: 209/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8958613872528076\n",
      "training loss:2.475077041946861\n",
      "training rmse:0.15732377576027282\n",
      "Training_R_Squared:-1.4686278954319913\n",
      "Epoch number: 210/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8122196197509766\n",
      "training loss:2.862388469198777\n",
      "training rmse:0.1691859470877761\n",
      "Training_R_Squared:-1.8549301323962006\n",
      "Epoch number: 211/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.843987226486206\n",
      "training loss:2.6781266382307933\n",
      "training rmse:0.16364982854347246\n",
      "Training_R_Squared:-1.6711484462027202\n",
      "Epoch number: 212/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8922903537750244\n",
      "training loss:2.325695506550801\n",
      "training rmse:0.15250231167266942\n",
      "Training_R_Squared:-1.3196355981277734\n",
      "Epoch number: 213/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.868309736251831\n",
      "training loss:2.253532833170766\n",
      "training rmse:0.15011771491635376\n",
      "Training_R_Squared:-1.247660953728515\n",
      "Epoch number: 214/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8190178871154785\n",
      "training loss:2.636507927117691\n",
      "training rmse:0.1623732714185956\n",
      "Training_R_Squared:-1.6296381859280484\n",
      "Epoch number: 215/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.831451654434204\n",
      "training loss:2.2305016611456097\n",
      "training rmse:0.14934864114365454\n",
      "Training_R_Squared:-1.2246898003269702\n",
      "Epoch number: 216/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9991512298583984\n",
      "training loss:2.3806309982282983\n",
      "training rmse:0.1542929356201475\n",
      "Training_R_Squared:-1.3744279293986752\n",
      "Epoch number: 217/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.941847085952759\n",
      "training loss:2.139927006203834\n",
      "training rmse:0.14628489348541202\n",
      "Training_R_Squared:-1.134351145190402\n",
      "Epoch number: 218/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849112033843994\n",
      "training loss:2.309461804968578\n",
      "training rmse:0.15196913518766164\n",
      "Training_R_Squared:-1.303444227489118\n",
      "Epoch number: 219/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.344115734100342\n",
      "training loss:2.0088838490773924\n",
      "training rmse:0.14173509971342288\n",
      "Training_R_Squared:-1.00364944948694\n",
      "Epoch number: 220/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.223540782928467\n",
      "training loss:2.4784296747411645\n",
      "training rmse:0.15743029170846265\n",
      "Training_R_Squared:-1.4719717964092456\n",
      "Epoch number: 221/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.830880880355835\n",
      "training loss:2.043794004388758\n",
      "training rmse:0.14296132359448685\n",
      "Training_R_Squared:-1.0384686282958304\n",
      "Epoch number: 222/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.878040075302124\n",
      "training loss:2.3818779917423853\n",
      "training rmse:0.1543333402652319\n",
      "Training_R_Squared:-1.375671695404625\n",
      "Epoch number: 223/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9071760177612305\n",
      "training loss:2.129362782012322\n",
      "training rmse:0.14592336283173857\n",
      "Training_R_Squared:-1.1238144403459933\n",
      "Epoch number: 224/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8909027576446533\n",
      "training loss:2.5699112352831435\n",
      "training rmse:0.16030942689945415\n",
      "Training_R_Squared:-1.5632149891971907\n",
      "Epoch number: 225/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.791118621826172\n",
      "training loss:2.6056056386042883\n",
      "training rmse:0.16141888484945893\n",
      "Training_R_Squared:-1.5988163974492058\n",
      "Epoch number: 226/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.90205979347229\n",
      "training loss:2.324709423679451\n",
      "training rmse:0.15246997814912452\n",
      "Training_R_Squared:-1.3186520744584622\n",
      "Epoch number: 227/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.718583106994629\n",
      "training loss:2.308570556923371\n",
      "training rmse:0.15193980903382007\n",
      "Training_R_Squared:-1.3025552621575684\n",
      "Epoch number: 228/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849694013595581\n",
      "training loss:2.6367230275253632\n",
      "training rmse:0.16237989492315122\n",
      "Training_R_Squared:-1.6298526802875402\n",
      "Epoch number: 229/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.847402811050415\n",
      "training loss:2.1050821752477304\n",
      "training rmse:0.14508901320388565\n",
      "Training_R_Squared:-1.0995970910595947\n",
      "Epoch number: 230/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849773645401001\n",
      "training loss:2.3385936820486677\n",
      "training rmse:0.15292461155905115\n",
      "Training_R_Squared:-1.3325001758220254\n",
      "Epoch number: 231/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.870126724243164\n",
      "training loss:2.199140158645605\n",
      "training rmse:0.1482949816630895\n",
      "Training_R_Squared:-1.193410024371102\n",
      "Epoch number: 232/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.807739019393921\n",
      "training loss:2.017730311096784\n",
      "training rmse:0.14204683421663378\n",
      "Training_R_Squared:-1.0124728547350137\n",
      "Epoch number: 233/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9475715160369873\n",
      "training loss:1.887165628823709\n",
      "training rmse:0.13737414708829712\n",
      "Training_R_Squared:-0.8822483717754661\n",
      "Epoch number: 234/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.735008478164673\n",
      "training loss:2.6997706511951947\n",
      "training rmse:0.1643097882414555\n",
      "Training_R_Squared:-1.6927360313004978\n",
      "Epoch number: 235/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7862586975097656\n",
      "training loss:2.248602490864883\n",
      "training rmse:0.14995340912646443\n",
      "Training_R_Squared:-1.242743440906966\n",
      "Epoch number: 236/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.016287088394165\n",
      "training loss:2.648293926759834\n",
      "training rmse:0.16273579590120404\n",
      "Training_R_Squared:-1.6413934509149382\n",
      "Epoch number: 237/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8969616889953613\n",
      "training loss:2.519548226912775\n",
      "training rmse:0.158730848511333\n",
      "Training_R_Squared:-1.512983210837294\n",
      "Epoch number: 238/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8660569190979004\n",
      "training loss:2.1753552442133923\n",
      "training rmse:0.14749085545258026\n",
      "Training_R_Squared:-1.1696870692155135\n",
      "Epoch number: 239/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9176576137542725\n",
      "training loss:2.618494224312599\n",
      "training rmse:0.16181762031103408\n",
      "Training_R_Squared:-1.6116713925779913\n",
      "Epoch number: 240/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9386467933654785\n",
      "training loss:2.413604021620632\n",
      "training rmse:0.15535778131849823\n",
      "Training_R_Squared:-1.4073150685959206\n",
      "Epoch number: 241/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7689547538757324\n",
      "training loss:1.648871905010452\n",
      "training rmse:0.1284084072407431\n",
      "Training_R_Squared:-0.6445755431234594\n",
      "Epoch number: 242/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8744699954986572\n",
      "training loss:2.974585574922628\n",
      "training rmse:0.17246986910537818\n",
      "Training_R_Squared:-1.9668349031901537\n",
      "Epoch number: 243/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8285117149353027\n",
      "training loss:2.091986249164279\n",
      "training rmse:0.14463700249812558\n",
      "Training_R_Squared:-1.0865352998342392\n",
      "Epoch number: 244/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.894110918045044\n",
      "training loss:1.9947456224199414\n",
      "training rmse:0.141235463762468\n",
      "Training_R_Squared:-0.9895480502382796\n",
      "Epoch number: 245/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.808699607849121\n",
      "training loss:1.9946475493989055\n",
      "training rmse:0.14123199175112222\n",
      "Training_R_Squared:-0.9894502399776883\n",
      "Epoch number: 246/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8137147426605225\n",
      "training loss:2.304580431249292\n",
      "training rmse:0.15180844611711472\n",
      "Training_R_Squared:-1.2985755371566068\n",
      "Epoch number: 247/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8298087120056152\n",
      "training loss:2.28692971433793\n",
      "training rmse:0.151225980384917\n",
      "Training_R_Squared:-1.2809708200353072\n",
      "Epoch number: 248/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8578240871429443\n",
      "training loss:2.145766148398252\n",
      "training rmse:0.14648433869865585\n",
      "Training_R_Squared:-1.140175084295763\n",
      "Epoch number: 249/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.88191819190979\n",
      "training loss:2.3622730008501094\n",
      "training rmse:0.15369687702910914\n",
      "Training_R_Squared:-1.3561177800102295\n",
      "Epoch number: 250/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8355231285095215\n",
      "training loss:1.7419488486243608\n",
      "training rmse:0.1319829098263999\n",
      "Training_R_Squared:-0.7374099720663756\n",
      "Epoch number: 251/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.80102801322937\n",
      "training loss:2.2150254468906496\n",
      "training rmse:0.14882961556392765\n",
      "Training_R_Squared:-1.2092539145876526\n",
      "Epoch number: 252/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0361125469207764\n",
      "training loss:2.444856268592048\n",
      "training rmse:0.15636036161994663\n",
      "Training_R_Squared:-1.4384858603041586\n",
      "Epoch number: 253/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8539040088653564\n",
      "training loss:2.0960517558710308\n",
      "training rmse:0.14477747600614643\n",
      "Training_R_Squared:-1.0905902133225158\n",
      "Epoch number: 254/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9098892211914062\n",
      "training loss:2.0948723455313143\n",
      "training rmse:0.14473673844367624\n",
      "Training_R_Squared:-1.0894138785183132\n",
      "Epoch number: 255/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.38738751411438\n",
      "training loss:2.117008527573489\n",
      "training rmse:0.14549943393613218\n",
      "Training_R_Squared:-1.1114923927475804\n",
      "Epoch number: 256/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1970176696777344\n",
      "training loss:2.0059140985276827\n",
      "training rmse:0.14163029684808554\n",
      "Training_R_Squared:-1.0006874265820556\n",
      "Epoch number: 257/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.888314723968506\n",
      "training loss:1.7547321284050668\n",
      "training rmse:0.13246630244726643\n",
      "Training_R_Squared:-0.7501599434957598\n",
      "Epoch number: 258/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9698610305786133\n",
      "training loss:1.7967099655554648\n",
      "training rmse:0.1340414102266708\n",
      "Training_R_Squared:-0.7920284017481649\n",
      "Epoch number: 259/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.779376268386841\n",
      "training loss:2.413319245351886\n",
      "training rmse:0.1553486158725557\n",
      "Training_R_Squared:-1.407031042870699\n",
      "Epoch number: 260/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9145431518554688\n",
      "training loss:1.9255609738956423\n",
      "training rmse:0.13876458387843932\n",
      "Training_R_Squared:-0.920543672270592\n",
      "Epoch number: 261/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.880937099456787\n",
      "training loss:1.8624057696097225\n",
      "training rmse:0.136469988261512\n",
      "Training_R_Squared:-0.8575530139148819\n",
      "Epoch number: 262/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.872713327407837\n",
      "training loss:1.621399587008348\n",
      "training rmse:0.12733418971385288\n",
      "Training_R_Squared:-0.6171748095880163\n",
      "Epoch number: 263/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8448023796081543\n",
      "training loss:1.734538201303181\n",
      "training rmse:0.13170186791777788\n",
      "Training_R_Squared:-0.7300186249319001\n",
      "Epoch number: 264/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8712246417999268\n",
      "training loss:1.3137759806877511\n",
      "training rmse:0.11462006720848453\n",
      "Training_R_Squared:-0.3103527719959229\n",
      "Epoch number: 265/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9155547618865967\n",
      "training loss:2.4407194283452327\n",
      "training rmse:0.15622802016108484\n",
      "Training_R_Squared:-1.434359818187315\n",
      "Epoch number: 266/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8334131240844727\n",
      "training loss:1.8847231359281977\n",
      "training rmse:0.1372852190123976\n",
      "Training_R_Squared:-0.8798122410681013\n",
      "Epoch number: 267/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8973817825317383\n",
      "training loss:2.292197757441617\n",
      "training rmse:0.15140005803967238\n",
      "Training_R_Squared:-1.2862251351221499\n",
      "Epoch number: 268/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.783677339553833\n",
      "training loss:2.002932547218734\n",
      "training rmse:0.14152499946012131\n",
      "Training_R_Squared:-0.9977136382714724\n",
      "Epoch number: 269/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8415005207061768\n",
      "training loss:1.8991584033469735\n",
      "training rmse:0.13780995622040426\n",
      "Training_R_Squared:-0.8942098800030294\n",
      "Epoch number: 270/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.825150966644287\n",
      "training loss:2.5738126399875227\n",
      "training rmse:0.16043106432320153\n",
      "Training_R_Squared:-1.5671062396431994\n",
      "Epoch number: 271/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.879777669906616\n",
      "training loss:2.0099725493601\n",
      "training rmse:0.14177350067484756\n",
      "Training_R_Squared:-1.0047352879255214\n",
      "Epoch number: 272/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8660786151885986\n",
      "training loss:2.3828873007905713\n",
      "training rmse:0.1543660357977289\n",
      "Training_R_Squared:-1.3766783747747944\n",
      "Epoch number: 273/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8384087085723877\n",
      "training loss:2.0508247682772662\n",
      "training rmse:0.14320700989397364\n",
      "Training_R_Squared:-1.0454810954057385\n",
      "Epoch number: 274/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0280582904815674\n",
      "training loss:2.0886046619624636\n",
      "training rmse:0.14452005611549087\n",
      "Training_R_Squared:-1.0831625220197627\n",
      "Epoch number: 275/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9436161518096924\n",
      "training loss:2.5720410609947066\n",
      "training rmse:0.16037584172794564\n",
      "Training_R_Squared:-1.5653392474198542\n",
      "Epoch number: 276/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.77972412109375\n",
      "training loss:1.9915896880347113\n",
      "training rmse:0.14112369354699839\n",
      "Training_R_Squared:-0.9864003308551648\n",
      "Epoch number: 277/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9065074920654297\n",
      "training loss:1.723777819198176\n",
      "training rmse:0.13129271949343482\n",
      "Training_R_Squared:-0.719286295747487\n",
      "Epoch number: 278/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.325144052505493\n",
      "training loss:2.3959406860426498\n",
      "training rmse:0.15478826460822698\n",
      "Training_R_Squared:-1.3896977467869527\n",
      "Epoch number: 279/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.205857753753662\n",
      "training loss:1.7811819494190786\n",
      "training rmse:0.13346092871769918\n",
      "Training_R_Squared:-0.7765408324098286\n",
      "Epoch number: 280/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8931636810302734\n",
      "training loss:1.720374859302865\n",
      "training rmse:0.13116306108439468\n",
      "Training_R_Squared:-0.7158921859183822\n",
      "Epoch number: 281/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9028007984161377\n",
      "training loss:2.0449524843697304\n",
      "training rmse:0.14300183510604786\n",
      "Training_R_Squared:-1.0396240799641725\n",
      "Epoch number: 282/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8947505950927734\n",
      "training loss:1.884909333698488\n",
      "training rmse:0.13729200026580166\n",
      "Training_R_Squared:-0.8799979520309338\n",
      "Epoch number: 283/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8964858055114746\n",
      "training loss:2.194934005375785\n",
      "training rmse:0.14815309667286017\n",
      "Training_R_Squared:-1.1892148274594438\n",
      "Epoch number: 284/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8550007343292236\n",
      "training loss:1.481453064586578\n",
      "training rmse:0.12171495654136258\n",
      "Training_R_Squared:-0.477592946808888\n",
      "Epoch number: 285/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.886626720428467\n",
      "training loss:2.0186054423284077\n",
      "training rmse:0.14207763519739508\n",
      "Training_R_Squared:-1.0133457047212704\n",
      "Epoch number: 286/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9179184436798096\n",
      "training loss:2.038533187736789\n",
      "training rmse:0.14277721063729984\n",
      "Training_R_Squared:-1.0332215178627688\n",
      "Epoch number: 287/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3500797748565674\n",
      "training loss:1.8327825289661632\n",
      "training rmse:0.13538029875008267\n",
      "Training_R_Squared:-0.8280069678794155\n",
      "Epoch number: 288/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1866681575775146\n",
      "training loss:1.5556906590368271\n",
      "training rmse:0.12472732896349649\n",
      "Training_R_Squared:-0.551637103125777\n",
      "Epoch number: 289/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.910435438156128\n",
      "training loss:2.051336435189569\n",
      "training rmse:0.14322487337015066\n",
      "Training_R_Squared:-1.0459914191554733\n",
      "Epoch number: 290/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.83646559715271\n",
      "training loss:1.620154902134928\n",
      "training rmse:0.12728530559867968\n",
      "Training_R_Squared:-0.6159333681718588\n",
      "Epoch number: 291/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8578543663024902\n",
      "training loss:1.9864659349724434\n",
      "training rmse:0.14094204252005302\n",
      "Training_R_Squared:-0.9812899399694939\n",
      "Epoch number: 292/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.731137752532959\n",
      "training loss:2.400622639169569\n",
      "training rmse:0.15493942813788777\n",
      "Training_R_Squared:-1.39436751091919\n",
      "Epoch number: 293/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7934603691101074\n",
      "training loss:1.8508324749327585\n",
      "training rmse:0.13604530403261844\n",
      "Training_R_Squared:-0.8460098824736868\n",
      "Epoch number: 294/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9433817863464355\n",
      "training loss:1.638029644444842\n",
      "training rmse:0.1279855321684776\n",
      "Training_R_Squared:-0.6337615442027804\n",
      "Epoch number: 295/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.871520757675171\n",
      "training loss:2.237262970236088\n",
      "training rmse:0.14957482977547018\n",
      "Training_R_Squared:-1.2314334874465196\n",
      "Epoch number: 296/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.924175977706909\n",
      "training loss:1.8443727691364984\n",
      "training rmse:0.13580768642225294\n",
      "Training_R_Squared:-0.8395670174867345\n",
      "Epoch number: 297/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7496185302734375\n",
      "training loss:1.9232404239703556\n",
      "training rmse:0.13868094403955994\n",
      "Training_R_Squared:-0.9182291690445605\n",
      "Epoch number: 298/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.917039155960083\n",
      "training loss:2.060621821537584\n",
      "training rmse:0.1435486614893216\n",
      "Training_R_Squared:-1.0552525885914852\n",
      "Epoch number: 299/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.969630479812622\n",
      "training loss:1.3570223251974767\n",
      "training rmse:0.11649130118586008\n",
      "Training_R_Squared:-0.3534864291143778\n",
      "Epoch number: 300/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8351500034332275\n",
      "training loss:1.8697590227423149\n",
      "training rmse:0.13673913202672872\n",
      "Training_R_Squared:-0.8648871276734202\n",
      "Epoch number: 301/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8345634937286377\n",
      "training loss:1.8283164451197678\n",
      "training rmse:0.13521525228759393\n",
      "Training_R_Squared:-0.823552526920041\n",
      "Epoch number: 302/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.895453453063965\n",
      "training loss:1.4040930366136308\n",
      "training rmse:0.11849443179380333\n",
      "Training_R_Squared:-0.40043448440559826\n",
      "Epoch number: 303/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8849551677703857\n",
      "training loss:1.5190445867992821\n",
      "training rmse:0.12324952684693284\n",
      "Training_R_Squared:-0.5150865204121169\n",
      "Epoch number: 304/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.861658811569214\n",
      "training loss:1.6263511207923926\n",
      "training rmse:0.12752847214612087\n",
      "Training_R_Squared:-0.6221134433990763\n",
      "Epoch number: 305/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7594680786132812\n",
      "training loss:1.7668836035086315\n",
      "training rmse:0.1329241740056575\n",
      "Training_R_Squared:-0.7622797477434742\n",
      "Epoch number: 306/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.799755811691284\n",
      "training loss:1.6236810404099629\n",
      "training rmse:0.12742374348644617\n",
      "Training_R_Squared:-0.6194503282170858\n",
      "Epoch number: 307/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0837292671203613\n",
      "training loss:1.4871252448446626\n",
      "training rmse:0.1219477447452253\n",
      "Training_R_Squared:-0.4832503518385385\n",
      "Epoch number: 308/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3406500816345215\n",
      "training loss:1.4908738016001735\n",
      "training rmse:0.12210134321948196\n",
      "Training_R_Squared:-0.4869891290387349\n",
      "Epoch number: 309/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1325621604919434\n",
      "training loss:1.6306181367554018\n",
      "training rmse:0.12769565915705208\n",
      "Training_R_Squared:-0.6263693543508733\n",
      "Epoch number: 310/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.619135618209839\n",
      "training loss:1.4878328554550535\n",
      "training rmse:0.12197675415648071\n",
      "Training_R_Squared:-0.4839560995655725\n",
      "Epoch number: 311/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8182761669158936\n",
      "training loss:1.551360109111556\n",
      "training rmse:0.1245536072986871\n",
      "Training_R_Squared:-0.5473178286666618\n",
      "Epoch number: 312/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9465675354003906\n",
      "training loss:1.7713821200646862\n",
      "training rmse:0.13309328007321355\n",
      "Training_R_Squared:-0.766766543323911\n",
      "Epoch number: 313/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8132152557373047\n",
      "training loss:1.371377810042759\n",
      "training rmse:0.11710584144451373\n",
      "Training_R_Squared:-0.3678045021951657\n",
      "Epoch number: 314/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.854039192199707\n",
      "training loss:1.9625832012025057\n",
      "training rmse:0.14009222680800337\n",
      "Training_R_Squared:-0.9574694348515116\n",
      "Epoch number: 315/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.829834222793579\n",
      "training loss:1.5932405614096865\n",
      "training rmse:0.12622363334216324\n",
      "Training_R_Squared:-0.5890891669608969\n",
      "Epoch number: 316/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.938413381576538\n",
      "training loss:1.6794833417254154\n",
      "training rmse:0.12959488190995103\n",
      "Training_R_Squared:-0.6751072272888452\n",
      "Epoch number: 317/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8676934242248535\n",
      "training loss:1.8211314616476102\n",
      "training rmse:0.134949303875478\n",
      "Training_R_Squared:-0.8163862546916851\n",
      "Epoch number: 318/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.928633451461792\n",
      "training loss:1.4707952595326788\n",
      "training rmse:0.12127634804580317\n",
      "Training_R_Squared:-0.46696290124908835\n",
      "Epoch number: 319/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.87625789642334\n",
      "training loss:1.8264477406310107\n",
      "training rmse:0.13514613352334615\n",
      "Training_R_Squared:-0.821688697273359\n",
      "Epoch number: 320/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8700764179229736\n",
      "training loss:1.7528680477937542\n",
      "training rmse:0.13239592319228544\n",
      "Training_R_Squared:-0.7483007130413899\n",
      "Epoch number: 321/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.361712694168091\n",
      "training loss:1.5397633768844798\n",
      "training rmse:0.12408720227664413\n",
      "Training_R_Squared:-0.5357513219668675\n",
      "Epoch number: 322/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2442831993103027\n",
      "training loss:1.5382052532943362\n",
      "training rmse:0.12402440297354131\n",
      "Training_R_Squared:-0.5341972541400195\n",
      "Epoch number: 323/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9334964752197266\n",
      "training loss:1.7838896496286907\n",
      "training rmse:0.1335623318764947\n",
      "Training_R_Squared:-0.7792414904447489\n",
      "Epoch number: 324/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8774802684783936\n",
      "training loss:2.043558752800948\n",
      "training rmse:0.14295309555238556\n",
      "Training_R_Squared:-1.0382339764307766\n",
      "Epoch number: 325/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8504889011383057\n",
      "training loss:1.574837518541699\n",
      "training rmse:0.12549253039690048\n",
      "Training_R_Squared:-0.5707340720745111\n",
      "Epoch number: 326/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9062278270721436\n",
      "training loss:1.764084939790905\n",
      "training rmse:0.13281885934576101\n",
      "Training_R_Squared:-0.7594883814198081\n",
      "Epoch number: 327/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8639354705810547\n",
      "training loss:1.5657305775674217\n",
      "training rmse:0.12512915637721775\n",
      "Training_R_Squared:-0.56165086387799\n",
      "Epoch number: 328/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8691513538360596\n",
      "training loss:1.2494960247420295\n",
      "training rmse:0.1117808581440503\n",
      "Training_R_Squared:-0.24624030047092949\n",
      "Epoch number: 329/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9092962741851807\n",
      "training loss:1.5440041781465936\n",
      "training rmse:0.12425796466008099\n",
      "Training_R_Squared:-0.5399810765876472\n",
      "Epoch number: 330/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9035909175872803\n",
      "training loss:1.930169401099377\n",
      "training rmse:0.13893053663969548\n",
      "Training_R_Squared:-0.925140083723111\n",
      "Epoch number: 331/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8751578330993652\n",
      "training loss:1.5928253629713822\n",
      "training rmse:0.1262071853331411\n",
      "Training_R_Squared:-0.5886750566996606\n",
      "Epoch number: 332/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7790591716766357\n",
      "training loss:1.4742041143588835\n",
      "training rmse:0.12141680750039854\n",
      "Training_R_Squared:-0.4703628826492412\n",
      "Epoch number: 333/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8955471515655518\n",
      "training loss:1.764895999673172\n",
      "training rmse:0.13284938839427046\n",
      "Training_R_Squared:-0.7602973344315869\n",
      "Epoch number: 334/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7655529975891113\n",
      "training loss:2.1489314580167047\n",
      "training rmse:0.14659234147856104\n",
      "Training_R_Squared:-1.1433321271179495\n",
      "Epoch number: 335/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.836853504180908\n",
      "training loss:1.7308733326521804\n",
      "training rmse:0.13156265931685102\n",
      "Training_R_Squared:-0.7263633223080084\n",
      "Epoch number: 336/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.811274766921997\n",
      "training loss:1.8870449202066908\n",
      "training rmse:0.13736975359250997\n",
      "Training_R_Squared:-0.8821279875041956\n",
      "Epoch number: 337/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0985684394836426\n",
      "training loss:1.6596229404026417\n",
      "training rmse:0.12882635368598466\n",
      "Training_R_Squared:-0.6552985835177181\n",
      "Epoch number: 338/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.335327386856079\n",
      "training loss:1.7215356271422024\n",
      "training rmse:0.13120730266041605\n",
      "Training_R_Squared:-0.7170499478610168\n",
      "Epoch number: 339/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2573328018188477\n",
      "training loss:1.470295862009209\n",
      "training rmse:0.12125575705958085\n",
      "Training_R_Squared:-0.46646480835312\n",
      "Epoch number: 340/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8680007457733154\n",
      "training loss:1.3722672369704583\n",
      "training rmse:0.11714381063336032\n",
      "Training_R_Squared:-0.36869161365373015\n",
      "Epoch number: 341/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9144270420074463\n",
      "training loss:1.9538278222735244\n",
      "training rmse:0.13977939126614924\n",
      "Training_R_Squared:-0.9487368571473498\n",
      "Epoch number: 342/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8769338130950928\n",
      "training loss:1.6063802103485614\n",
      "training rmse:0.12674305544480777\n",
      "Training_R_Squared:-0.6021945844857663\n",
      "Epoch number: 343/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.869399309158325\n",
      "training loss:1.6116110606745906\n",
      "training rmse:0.12694924421494563\n",
      "Training_R_Squared:-0.6074117922350337\n",
      "Epoch number: 344/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.929443597793579\n",
      "training loss:1.410487990465299\n",
      "training rmse:0.118763967198191\n",
      "Training_R_Squared:-0.406812783952166\n",
      "Epoch number: 345/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9732718467712402\n",
      "training loss:1.7284070444341069\n",
      "training rmse:0.13146889534920825\n",
      "Training_R_Squared:-0.7239034394276016\n",
      "Epoch number: 346/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7763400077819824\n",
      "training loss:1.5353232112793194\n",
      "training rmse:0.12390815999276719\n",
      "Training_R_Squared:-0.5313227311753452\n",
      "Epoch number: 347/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8257412910461426\n",
      "training loss:1.450173351333433\n",
      "training rmse:0.12042314359513427\n",
      "Training_R_Squared:-0.44639473565687715\n",
      "Epoch number: 348/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.861158847808838\n",
      "training loss:1.2891603556900009\n",
      "training rmse:0.1135411976196306\n",
      "Training_R_Squared:-0.28580128729446885\n",
      "Epoch number: 349/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8454456329345703\n",
      "training loss:1.8110048723601722\n",
      "training rmse:0.13457358107593675\n",
      "Training_R_Squared:-0.8062860634725446\n",
      "Epoch number: 350/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8299789428710938\n",
      "training loss:1.422538655997414\n",
      "training rmse:0.11927022495146951\n",
      "Training_R_Squared:-0.41883204568900245\n",
      "Epoch number: 351/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.892505168914795\n",
      "training loss:2.022962859230745\n",
      "training rmse:0.14223089886627113\n",
      "Training_R_Squared:-1.0176917573157866\n",
      "Epoch number: 352/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.683851480484009\n",
      "training loss:1.797111630753136\n",
      "training rmse:0.1340563922665807\n",
      "Training_R_Squared:-0.7924290263309341\n",
      "Epoch number: 353/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8270068168640137\n",
      "training loss:1.5518649785128673\n",
      "training rmse:0.12457387280296248\n",
      "Training_R_Squared:-0.5478214042897958\n",
      "Epoch number: 354/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0096757411956787\n",
      "training loss:1.2856130681608136\n",
      "training rmse:0.11338487854034211\n",
      "Training_R_Squared:-0.28226323925624786\n",
      "Epoch number: 355/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9603331089019775\n",
      "training loss:1.6290941052614016\n",
      "training rmse:0.12763597084135028\n",
      "Training_R_Squared:-0.6248492943130566\n",
      "Epoch number: 356/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.908583402633667\n",
      "training loss:1.4885136741831957\n",
      "training rmse:0.12200465868905153\n",
      "Training_R_Squared:-0.48463516199972645\n",
      "Epoch number: 357/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.368823289871216\n",
      "training loss:1.5633973413127933\n",
      "training rmse:0.12503588850057384\n",
      "Training_R_Squared:-0.5593237052530016\n",
      "Epoch number: 358/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.288095712661743\n",
      "training loss:1.8979940960198292\n",
      "training rmse:0.13776770652151502\n",
      "Training_R_Squared:-0.8930486258805257\n",
      "Epoch number: 359/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7688679695129395\n",
      "training loss:1.4175397766899493\n",
      "training rmse:0.1190604794501496\n",
      "Training_R_Squared:-0.41384618658244743\n",
      "Epoch number: 360/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9971814155578613\n",
      "training loss:1.6780577820377545\n",
      "training rmse:0.1295398696169544\n",
      "Training_R_Squared:-0.6736853755342729\n",
      "Epoch number: 361/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.823763847351074\n",
      "training loss:1.8161264487765187\n",
      "training rmse:0.13476373580368417\n",
      "Training_R_Squared:-0.8113942981548727\n",
      "Epoch number: 362/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9112300872802734\n",
      "training loss:1.5419422196766845\n",
      "training rmse:0.1241749660630791\n",
      "Training_R_Squared:-0.5379244729847108\n",
      "Epoch number: 363/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.886244058609009\n",
      "training loss:1.6459629282966262\n",
      "training rmse:0.12829508674523066\n",
      "Training_R_Squared:-0.641674147819381\n",
      "Epoch number: 364/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8622214794158936\n",
      "training loss:2.047524109877486\n",
      "training rmse:0.14309172267736125\n",
      "Training_R_Squared:-1.042189021098495\n",
      "Epoch number: 365/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9310154914855957\n",
      "training loss:1.7176858126958905\n",
      "training rmse:0.1310605132256047\n",
      "Training_R_Squared:-0.7132101635668289\n",
      "Epoch number: 366/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8941452503204346\n",
      "training loss:1.5523466500458198\n",
      "training rmse:0.12459320407011852\n",
      "Training_R_Squared:-0.5483018141824425\n",
      "Epoch number: 367/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7637181282043457\n",
      "training loss:1.301512931064991\n",
      "training rmse:0.11408386963392288\n",
      "Training_R_Squared:-0.29812166449661204\n",
      "Epoch number: 368/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.88680100440979\n",
      "training loss:1.5750683784061437\n",
      "training rmse:0.12550172821145306\n",
      "Training_R_Squared:-0.5709643360226109\n",
      "Epoch number: 369/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8578994274139404\n",
      "training loss:1.5542297623228478\n",
      "training rmse:0.12466875159087973\n",
      "Training_R_Squared:-0.5501800118563867\n",
      "Epoch number: 370/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8109548091888428\n",
      "training loss:1.5740109052821936\n",
      "training rmse:0.12545959131458198\n",
      "Training_R_Squared:-0.5699096092185729\n",
      "Epoch number: 371/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.856407880783081\n",
      "training loss:1.4267077248198348\n",
      "training rmse:0.11944487116740654\n",
      "Training_R_Squared:-0.4229902477523797\n",
      "Epoch number: 372/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.828237295150757\n",
      "training loss:1.2185823145173345\n",
      "training rmse:0.11038941591100726\n",
      "Training_R_Squared:-0.21540713199037054\n",
      "Epoch number: 373/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.900477886199951\n",
      "training loss:1.329309692642255\n",
      "training rmse:0.11529569344265443\n",
      "Training_R_Squared:-0.32584599166540795\n",
      "Epoch number: 374/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.802074670791626\n",
      "training loss:1.8321571319679606\n",
      "training rmse:0.1353571989946586\n",
      "Training_R_Squared:-0.827383205591603\n",
      "Epoch number: 375/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7858479022979736\n",
      "training loss:1.619728743078781\n",
      "training rmse:0.12726856418922863\n",
      "Training_R_Squared:-0.6155083156210539\n",
      "Epoch number: 376/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0486083030700684\n",
      "training loss:1.7972451091984567\n",
      "training rmse:0.13406137061802914\n",
      "Training_R_Squared:-0.7925621453916212\n",
      "Epoch number: 377/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8893845081329346\n",
      "training loss:1.7485424247275887\n",
      "training rmse:0.1322324629101186\n",
      "Training_R_Squared:-0.7439863722510507\n",
      "Epoch number: 378/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.818410634994507\n",
      "training loss:1.6621514419493906\n",
      "training rmse:0.12892445237228625\n",
      "Training_R_Squared:-0.657820499589133\n",
      "Epoch number: 379/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.841661214828491\n",
      "training loss:1.6885041945934063\n",
      "training rmse:0.12994245628713527\n",
      "Training_R_Squared:-0.6841045690296705\n",
      "Epoch number: 380/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.443748950958252\n",
      "training loss:1.5880426417334093\n",
      "training rmse:0.12601756392397884\n",
      "Training_R_Squared:-0.5839047959158403\n",
      "Epoch number: 381/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.173428773880005\n",
      "training loss:1.581553635049488\n",
      "training rmse:0.12575983599899804\n",
      "Training_R_Squared:-0.5774326863427277\n",
      "Epoch number: 382/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.807934045791626\n",
      "training loss:1.5794295619148215\n",
      "training rmse:0.1256753580426498\n",
      "Training_R_Squared:-0.5753141463485913\n",
      "Epoch number: 383/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9311184883117676\n",
      "training loss:1.6651198592536502\n",
      "training rmse:0.12903952337379623\n",
      "Training_R_Squared:-0.6607811805058845\n",
      "Epoch number: 384/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.767630100250244\n",
      "training loss:1.3422553267366197\n",
      "training rmse:0.11585574335079896\n",
      "Training_R_Squared:-0.338757914439648\n",
      "Epoch number: 385/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8342339992523193\n",
      "training loss:2.013572414011321\n",
      "training rmse:0.14190040218446603\n",
      "Training_R_Squared:-1.0083257784784005\n",
      "Epoch number: 386/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.960951328277588\n",
      "training loss:1.7320704788816386\n",
      "training rmse:0.13160814864139828\n",
      "Training_R_Squared:-0.7275573363647383\n",
      "Epoch number: 387/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7977750301361084\n",
      "training loss:1.348774385866136\n",
      "training rmse:0.1161367463753887\n",
      "Training_R_Squared:-0.3452599800155636\n",
      "Epoch number: 388/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8654682636260986\n",
      "training loss:1.570548131018768\n",
      "training rmse:0.12532151176149958\n",
      "Training_R_Squared:-0.5664558483204785\n",
      "Epoch number: 389/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4016287326812744\n",
      "training loss:1.5854716266330797\n",
      "training rmse:0.1259155124134068\n",
      "Training_R_Squared:-0.5813404633775578\n",
      "Epoch number: 390/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2662832736968994\n",
      "training loss:1.3767679744705674\n",
      "training rmse:0.11733575646283478\n",
      "Training_R_Squared:-0.37318062334874846\n",
      "Epoch number: 391/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.901418924331665\n",
      "training loss:1.5078780188654015\n",
      "training rmse:0.12279568473140258\n",
      "Training_R_Squared:-0.503949044221111\n",
      "Epoch number: 392/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.830059766769409\n",
      "training loss:1.6217748467090587\n",
      "training rmse:0.12734892409082452\n",
      "Training_R_Squared:-0.6175491208564217\n",
      "Epoch number: 393/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8469231128692627\n",
      "training loss:1.3461881630328207\n",
      "training rmse:0.11602534908513833\n",
      "Training_R_Squared:-0.3426804964172423\n",
      "Epoch number: 394/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8933491706848145\n",
      "training loss:1.6447474895135201\n",
      "training rmse:0.1282477091223668\n",
      "Training_R_Squared:-0.6404618750403717\n",
      "Epoch number: 395/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.822469711303711\n",
      "training loss:1.7735826308287557\n",
      "training rmse:0.13317592240449308\n",
      "Training_R_Squared:-0.7689613424585924\n",
      "Epoch number: 396/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9237749576568604\n",
      "training loss:1.3704549108920219\n",
      "training rmse:0.11706643032449661\n",
      "Training_R_Squared:-0.3668840071676571\n",
      "Epoch number: 397/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8428876399993896\n",
      "training loss:1.5063170086598348\n",
      "training rmse:0.12273210699160325\n",
      "Training_R_Squared:-0.5023921026290388\n",
      "Epoch number: 398/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8904452323913574\n",
      "training loss:1.6103803288251903\n",
      "training rmse:0.12690076157475141\n",
      "Training_R_Squared:-0.6061842542626406\n",
      "Epoch number: 399/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.911609172821045\n",
      "training loss:1.2583874885567639\n",
      "training rmse:0.11217787163949777\n",
      "Training_R_Squared:-0.25510859246530293\n",
      "Epoch number: 400/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.782860040664673\n",
      "training loss:1.1907289727806898\n",
      "training rmse:0.10912052844358341\n",
      "Training_R_Squared:-0.18762636921774511\n",
      "Epoch number: 401/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.883882999420166\n",
      "training loss:1.6878884546076733\n",
      "training rmse:0.12991876133213684\n",
      "Training_R_Squared:-0.6834904406917053\n",
      "Epoch number: 402/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.832564353942871\n",
      "training loss:1.2563356369705652\n",
      "training rmse:0.11208637905519854\n",
      "Training_R_Squared:-0.2530620800447305\n",
      "Epoch number: 403/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8668930530548096\n",
      "training loss:1.658662126461877\n",
      "training rmse:0.12878905723942066\n",
      "Training_R_Squared:-0.6543402648870555\n",
      "Epoch number: 404/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.760662078857422\n",
      "training loss:1.3970093320795058\n",
      "training rmse:0.1181951493116154\n",
      "Training_R_Squared:-0.39336924004750595\n",
      "Epoch number: 405/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.899761915206909\n",
      "training loss:1.4355486373593784\n",
      "training rmse:0.11981438299967907\n",
      "Training_R_Squared:-0.4318081349587124\n",
      "Epoch number: 406/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.813203811645508\n",
      "training loss:1.6195747088697772\n",
      "training rmse:0.1272625125034775\n",
      "Training_R_Squared:-0.6153546895409805\n",
      "Epoch number: 407/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8317108154296875\n",
      "training loss:1.8356902421538734\n",
      "training rmse:0.13548764674884103\n",
      "Training_R_Squared:-0.8309071012240865\n",
      "Epoch number: 408/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.865743637084961\n",
      "training loss:1.8785611351769411\n",
      "training rmse:0.13706061196335514\n",
      "Training_R_Squared:-0.8736662999219154\n",
      "Epoch number: 409/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.076380729675293\n",
      "training loss:1.317166051226934\n",
      "training rmse:0.11476785487351997\n",
      "Training_R_Squared:-0.31373399143432246\n",
      "Epoch number: 410/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.309577465057373\n",
      "training loss:1.7759754568601238\n",
      "training rmse:0.13326572916020546\n",
      "Training_R_Squared:-0.7713479318093088\n",
      "Epoch number: 411/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.261282205581665\n",
      "training loss:1.5341376296778435\n",
      "training rmse:0.12386030961037695\n",
      "Training_R_Squared:-0.5301402346459816\n",
      "Epoch number: 412/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8823578357696533\n",
      "training loss:1.971711631926727\n",
      "training rmse:0.1404176496002809\n",
      "Training_R_Squared:-0.9665740909160907\n",
      "Epoch number: 413/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849375009536743\n",
      "training loss:1.6586016572487665\n",
      "training rmse:0.12878670961123148\n",
      "Training_R_Squared:-0.6542799471131442\n",
      "Epoch number: 414/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.831061363220215\n",
      "training loss:1.2679644907942453\n",
      "training rmse:0.1126039293628\n",
      "Training_R_Squared:-0.2646606467922963\n",
      "Epoch number: 415/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.905712127685547\n",
      "training loss:1.9578728390836204\n",
      "training rmse:0.13992400934377275\n",
      "Training_R_Squared:-0.9527713357139269\n",
      "Epoch number: 416/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.852489948272705\n",
      "training loss:1.5797948530503163\n",
      "training rmse:0.12568989032735753\n",
      "Training_R_Squared:-0.5756784939191357\n",
      "Epoch number: 417/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.853987693786621\n",
      "training loss:1.1021548180742258\n",
      "training rmse:0.10498356147865368\n",
      "Training_R_Squared:-0.09928301187801503\n",
      "Epoch number: 418/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.928680658340454\n",
      "training loss:1.5268863263829644\n",
      "training rmse:0.12356724187190408\n",
      "Training_R_Squared:-0.5229078171743373\n",
      "Epoch number: 419/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.795187473297119\n",
      "training loss:1.4146957168565706\n",
      "training rmse:0.11894098187153873\n",
      "Training_R_Squared:-0.4110095485041543\n",
      "Epoch number: 420/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9214141368865967\n",
      "training loss:1.2567724112977885\n",
      "training rmse:0.11210586118922544\n",
      "Training_R_Squared:-0.2534977376699348\n",
      "Epoch number: 421/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.872643232345581\n",
      "training loss:1.3250223100816925\n",
      "training rmse:0.11510961341615619\n",
      "Training_R_Squared:-0.32156978790727386\n",
      "Epoch number: 422/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.919879674911499\n",
      "training loss:1.6772523829875041\n",
      "training rmse:0.12950877896835813\n",
      "Training_R_Squared:-0.672882074677174\n",
      "Epoch number: 423/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.949720621109009\n",
      "training loss:1.4862258917146391\n",
      "training rmse:0.12191086463948318\n",
      "Training_R_Squared:-0.4823533347250424\n",
      "Epoch number: 424/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3385095596313477\n",
      "training loss:1.3993627430863995\n",
      "training rmse:0.11829466357728904\n",
      "Training_R_Squared:-0.3957165087002572\n",
      "Epoch number: 425/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2113139629364014\n",
      "training loss:1.3525198950458162\n",
      "training rmse:0.11629788884781254\n",
      "Training_R_Squared:-0.3489957224072626\n",
      "Epoch number: 426/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.916276454925537\n",
      "training loss:1.1915696549215227\n",
      "training rmse:0.1091590424528139\n",
      "Training_R_Squared:-0.1884648573515435\n",
      "Epoch number: 427/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8314175605773926\n",
      "training loss:1.44475997801419\n",
      "training rmse:0.12019816878863795\n",
      "Training_R_Squared:-0.44099546776336385\n",
      "Epoch number: 428/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.856614112854004\n",
      "training loss:1.77755418351677\n",
      "training rmse:0.13332494828488667\n",
      "Training_R_Squared:-0.7729225314520352\n",
      "Epoch number: 429/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9106438159942627\n",
      "training loss:1.513789846309237\n",
      "training rmse:0.12303616729682525\n",
      "Training_R_Squared:-0.5098454615689567\n",
      "Epoch number: 430/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7831320762634277\n",
      "training loss:1.3257156276649198\n",
      "training rmse:0.11513972501551842\n",
      "Training_R_Squared:-0.3222613001499348\n",
      "Epoch number: 431/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.80924391746521\n",
      "training loss:1.4051004747847173\n",
      "training rmse:0.11853693410851815\n",
      "Training_R_Squared:-0.4014392986595561\n",
      "Epoch number: 432/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9016122817993164\n",
      "training loss:1.4066039540416\n",
      "training rmse:0.11860033533011616\n",
      "Training_R_Squared:-0.40293886469000184\n",
      "Epoch number: 433/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.858478546142578\n",
      "training loss:1.5704809494612988\n",
      "training rmse:0.12531883136469552\n",
      "Training_R_Squared:-0.5663888509898616\n",
      "Epoch number: 434/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8577635288238525\n",
      "training loss:1.6025275895818538\n",
      "training rmse:0.12659097872999694\n",
      "Training_R_Squared:-0.5983519889939324\n",
      "Epoch number: 435/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8551087379455566\n",
      "training loss:1.7279616203006753\n",
      "training rmse:0.13145195397180962\n",
      "Training_R_Squared:-0.7234591981429026\n",
      "Epoch number: 436/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.906665325164795\n",
      "training loss:1.5123416619908312\n",
      "training rmse:0.12297730123851439\n",
      "Training_R_Squared:-0.5084010538095225\n",
      "Epoch number: 437/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.909792423248291\n",
      "training loss:1.7277617718908687\n",
      "training rmse:0.1314443521757732\n",
      "Training_R_Squared:-0.7232598689096459\n",
      "Epoch number: 438/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.804396867752075\n",
      "training loss:1.3057139930162975\n",
      "training rmse:0.1142678429400108\n",
      "Training_R_Squared:-0.3023117881761994\n",
      "Epoch number: 439/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.824981689453125\n",
      "training loss:1.126791956485249\n",
      "training rmse:0.10615045720510341\n",
      "Training_R_Squared:-0.12385594376923725\n",
      "Epoch number: 440/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8821537494659424\n",
      "training loss:1.5920993588428907\n",
      "training rmse:0.12617841966211538\n",
      "Training_R_Squared:-0.5879509225407766\n",
      "Epoch number: 441/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.869004964828491\n",
      "training loss:1.5191410831135101\n",
      "training rmse:0.12325344145757189\n",
      "Training_R_Squared:-0.515182763385923\n",
      "Epoch number: 442/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0800390243530273\n",
      "training loss:1.6218045847465419\n",
      "training rmse:0.1273500916664979\n",
      "Training_R_Squared:-0.6175787616760384\n",
      "Epoch number: 443/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.333702564239502\n",
      "training loss:1.723997610924485\n",
      "training rmse:0.13130108952040287\n",
      "Training_R_Squared:-0.719505505996918\n",
      "Epoch number: 444/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.228135824203491\n",
      "training loss:1.764815768930987\n",
      "training rmse:0.13284636874717304\n",
      "Training_R_Squared:-0.7602173083217119\n",
      "Epoch number: 445/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.880521059036255\n",
      "training loss:1.30504535540803\n",
      "training rmse:0.11423858172299015\n",
      "Training_R_Squared:-0.3016448785089265\n",
      "Epoch number: 446/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9144628047943115\n",
      "training loss:1.279318298284636\n",
      "training rmse:0.11310695373338617\n",
      "Training_R_Squared:-0.2759848727169063\n",
      "Epoch number: 447/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.863260269165039\n",
      "training loss:1.4071092894541977\n",
      "training rmse:0.11862163754788574\n",
      "Training_R_Squared:-0.40344287763264575\n",
      "Epoch number: 448/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8809075355529785\n",
      "training loss:1.3400298693218247\n",
      "training rmse:0.11575965917891365\n",
      "Training_R_Squared:-0.3365382484414259\n",
      "Epoch number: 449/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8799843788146973\n",
      "training loss:1.6468080330269004\n",
      "training rmse:0.12832801849272435\n",
      "Training_R_Squared:-0.6425170598850352\n",
      "Epoch number: 450/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.841374397277832\n",
      "training loss:1.4276383743126644\n",
      "training rmse:0.11948382209791686\n",
      "Training_R_Squared:-0.42391845705894093\n",
      "Epoch number: 451/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9649689197540283\n",
      "training loss:1.3463425879908755\n",
      "training rmse:0.11603200368824437\n",
      "Training_R_Squared:-0.34283451457157876\n",
      "Epoch number: 452/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7177157402038574\n",
      "training loss:1.6224003792534063\n",
      "training rmse:0.12737348151218159\n",
      "Training_R_Squared:-0.6181730063218167\n",
      "Epoch number: 453/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8578438758850098\n",
      "training loss:1.5689081161258116\n",
      "training rmse:0.12525606237327644\n",
      "Training_R_Squared:-0.5648201131400377\n",
      "Epoch number: 454/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8438470363616943\n",
      "training loss:1.3707091399119236\n",
      "training rmse:0.11707728814385493\n",
      "Training_R_Squared:-0.3671375703088118\n",
      "Epoch number: 455/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.872382879257202\n",
      "training loss:1.484166712505612\n",
      "training rmse:0.12182638107181924\n",
      "Training_R_Squared:-0.4802995209072809\n",
      "Epoch number: 456/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.873431921005249\n",
      "training loss:1.383200666778663\n",
      "training rmse:0.11760955177104719\n",
      "Training_R_Squared:-0.3795965584544132\n",
      "Epoch number: 457/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.816206455230713\n",
      "training loss:1.7791562017182514\n",
      "training rmse:0.1333850142151753\n",
      "Training_R_Squared:-0.7745203658207829\n",
      "Epoch number: 458/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.826173782348633\n",
      "training loss:1.5064253594631651\n",
      "training rmse:0.12273652103034227\n",
      "Training_R_Squared:-0.5025001752053133\n",
      "Epoch number: 459/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.085923671722412\n",
      "training loss:1.3839040903057622\n",
      "training rmse:0.11763945300390351\n",
      "Training_R_Squared:-0.3802981430427035\n",
      "Epoch number: 460/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8541934490203857\n",
      "training loss:1.7885246220284898\n",
      "training rmse:0.13373573277282663\n",
      "Training_R_Squared:-0.7838643783904853\n",
      "Epoch number: 461/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8878180980682373\n",
      "training loss:1.4734384816783859\n",
      "training rmse:0.12138527429957828\n",
      "Training_R_Squared:-0.4695992369713926\n",
      "Epoch number: 462/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3749475479125977\n",
      "training loss:1.4036664728864707\n",
      "training rmse:0.11847643111127507\n",
      "Training_R_Squared:-0.400009038597696\n",
      "Epoch number: 463/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2537808418273926\n",
      "training loss:1.3023508185815444\n",
      "training rmse:0.11412058616137337\n",
      "Training_R_Squared:-0.2989573739656366\n",
      "Epoch number: 464/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8671514987945557\n",
      "training loss:1.2405070807610628\n",
      "training rmse:0.11137805352766149\n",
      "Training_R_Squared:-0.2372747785138214\n",
      "Epoch number: 465/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.888338565826416\n",
      "training loss:1.3061602368309906\n",
      "training rmse:0.1142873674922557\n",
      "Training_R_Squared:-0.3027568709034769\n",
      "Epoch number: 466/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.914151430130005\n",
      "training loss:1.7106155143537762\n",
      "training rmse:0.13079050096829573\n",
      "Training_R_Squared:-0.7061582832075313\n",
      "Epoch number: 467/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9064581394195557\n",
      "training loss:1.4102915440903416\n",
      "training rmse:0.11875569645664757\n",
      "Training_R_Squared:-0.4066168417896907\n",
      "Epoch number: 468/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.856811046600342\n",
      "training loss:1.9032417562711998\n",
      "training rmse:0.13795802826480233\n",
      "Training_R_Squared:-0.8982826047080334\n",
      "Epoch number: 469/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.937904119491577\n",
      "training loss:1.438594551930919\n",
      "training rmse:0.11994142536800698\n",
      "Training_R_Squared:-0.4348460999873405\n",
      "Epoch number: 470/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8886866569519043\n",
      "training loss:1.7015422036911332\n",
      "training rmse:0.1304431755091516\n",
      "Training_R_Squared:-0.6971086056908875\n",
      "Epoch number: 471/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9539172649383545\n",
      "training loss:1.1188796545536661\n",
      "training rmse:0.10577710785201429\n",
      "Training_R_Squared:-0.1159642703359296\n",
      "Epoch number: 472/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7294416427612305\n",
      "training loss:1.4193232575307775\n",
      "training rmse:0.11913535401092228\n",
      "Training_R_Squared:-0.4156250248953186\n",
      "Epoch number: 473/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.88645076751709\n",
      "training loss:1.55133658325542\n",
      "training rmse:0.1245526628882506\n",
      "Training_R_Squared:-0.5472943784551423\n",
      "Epoch number: 474/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8815479278564453\n",
      "training loss:1.4359966149477685\n",
      "training rmse:0.11983307619133243\n",
      "Training_R_Squared:-0.4322549423486153\n",
      "Epoch number: 475/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.840221405029297\n",
      "training loss:1.5767328138826997\n",
      "training rmse:0.125568021959522\n",
      "Training_R_Squared:-0.5726244195253587\n",
      "Epoch number: 476/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9025015830993652\n",
      "training loss:1.673879658663509\n",
      "training rmse:0.12937850125362826\n",
      "Training_R_Squared:-0.66951815678854\n",
      "Epoch number: 477/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8451766967773438\n",
      "training loss:1.6650984992593294\n",
      "training rmse:0.12903869571796397\n",
      "Training_R_Squared:-0.6607598582879453\n",
      "Epoch number: 478/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8711955547332764\n",
      "training loss:1.5199804441075457\n",
      "training rmse:0.12328748696066223\n",
      "Training_R_Squared:-0.5160199358036208\n",
      "Epoch number: 479/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8067259788513184\n",
      "training loss:0.9902734553620576\n",
      "training rmse:0.09951248441085458\n",
      "Training_R_Squared:0.012306841608694885\n",
      "Epoch number: 480/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8445494174957275\n",
      "training loss:1.7334607490947747\n",
      "training rmse:0.13166095659286298\n",
      "Training_R_Squared:-0.7289439822689032\n",
      "Epoch number: 481/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.064542770385742\n",
      "training loss:1.8341885536108293\n",
      "training rmse:0.13543221749682863\n",
      "Training_R_Squared:-0.829409328989756\n",
      "Epoch number: 482/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.873946189880371\n",
      "training loss:1.1284216755078411\n",
      "training rmse:0.10622719404690313\n",
      "Training_R_Squared:-0.12548142556577235\n",
      "Epoch number: 483/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.901298999786377\n",
      "training loss:1.5422319569837555\n",
      "training rmse:0.12418663200939767\n",
      "Training_R_Squared:-0.538213476959966\n",
      "Epoch number: 484/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8386001586914062\n",
      "training loss:1.2820831145709235\n",
      "training rmse:0.11322910909174033\n",
      "Training_R_Squared:-0.2787424765166875\n",
      "Epoch number: 485/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4366061687469482\n",
      "training loss:1.5166770873105406\n",
      "training rmse:0.12315344442241721\n",
      "Training_R_Squared:-0.512725193480295\n",
      "Epoch number: 486/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2762720584869385\n",
      "training loss:1.2948473220958476\n",
      "training rmse:0.11379135828769457\n",
      "Training_R_Squared:-0.2914734326887187\n",
      "Epoch number: 487/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.905139207839966\n",
      "training loss:1.4682988667359496\n",
      "training rmse:0.12117338266863517\n",
      "Training_R_Squared:-0.4644730239115451\n",
      "Epoch number: 488/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8773250579833984\n",
      "training loss:1.347006304156821\n",
      "training rmse:0.1160606007289649\n",
      "Training_R_Squared:-0.3434965024718659\n",
      "Epoch number: 489/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.871405601501465\n",
      "training loss:1.1393493948094147\n",
      "training rmse:0.10674031079256864\n",
      "Training_R_Squared:-0.1363806682235431\n",
      "Epoch number: 490/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9482531547546387\n",
      "training loss:1.498134912036278\n",
      "training rmse:0.12239832155860136\n",
      "Training_R_Squared:-0.4942313189015166\n",
      "Epoch number: 491/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7615838050842285\n",
      "training loss:1.4088794386477161\n",
      "training rmse:0.11869622734727991\n",
      "Training_R_Squared:-0.4052084145225563\n",
      "Epoch number: 492/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8708462715148926\n",
      "training loss:1.2177743110619303\n",
      "training rmse:0.11035281197422793\n",
      "Training_R_Squared:-0.21460124021400007\n",
      "Epoch number: 493/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.904010534286499\n",
      "training loss:1.363227846170048\n",
      "training rmse:0.11675734864110472\n",
      "Training_R_Squared:-0.35967577270522777\n",
      "Epoch number: 494/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4182658195495605\n",
      "training loss:1.3646055880890344\n",
      "training rmse:0.11681633396443472\n",
      "Training_R_Squared:-0.3610499241400924\n",
      "Epoch number: 495/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.234722375869751\n",
      "training loss:1.159378596690786\n",
      "training rmse:0.10767444435383848\n",
      "Training_R_Squared:-0.1563576873423167\n",
      "Epoch number: 496/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.888901710510254\n",
      "training loss:1.483078321445646\n",
      "training rmse:0.12178170311855743\n",
      "Training_R_Squared:-0.4792139751786322\n",
      "Epoch number: 497/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9237427711486816\n",
      "training loss:1.313638586392699\n",
      "training rmse:0.11461407358578173\n",
      "Training_R_Squared:-0.3102157332161355\n",
      "Epoch number: 498/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8602724075317383\n",
      "training loss:1.338029261207371\n",
      "training rmse:0.1156732147563718\n",
      "Training_R_Squared:-0.3345428546220477\n",
      "Epoch number: 499/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9355175495147705\n",
      "training loss:1.4475612705570882\n",
      "training rmse:0.12031464044567013\n",
      "Training_R_Squared:-0.4437894644466496\n",
      "Epoch number: 500/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7236974239349365\n",
      "training loss:1.330452966336921\n",
      "training rmse:0.11534526285621446\n",
      "Training_R_Squared:-0.3269862897272444\n",
      "Epoch number: 501/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7295520305633545\n",
      "training loss:1.5504492242531\n",
      "training rmse:0.12451703595304138\n",
      "Training_R_Squared:-0.5464093182308392\n",
      "Epoch number: 502/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.86872935295105\n",
      "training loss:1.6119530545255714\n",
      "training rmse:0.12696271320846808\n",
      "Training_R_Squared:-0.6077529006815379\n",
      "Epoch number: 503/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.819898843765259\n",
      "training loss:1.301050355773441\n",
      "training rmse:0.11406359435742155\n",
      "Training_R_Squared:-0.29766029018545215\n",
      "Epoch number: 504/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.907539129257202\n",
      "training loss:1.0452531767793118\n",
      "training rmse:0.10223762403241342\n",
      "Training_R_Squared:-0.04252963441965352\n",
      "Epoch number: 505/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8769187927246094\n",
      "training loss:1.477328828123973\n",
      "training rmse:0.12154541653735745\n",
      "Training_R_Squared:-0.47347945021632865\n",
      "Epoch number: 506/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8718414306640625\n",
      "training loss:1.350290698225308\n",
      "training rmse:0.11620200937270009\n",
      "Training_R_Squared:-0.3467723464962891\n",
      "Epoch number: 507/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.860368490219116\n",
      "training loss:1.5259748671060152\n",
      "training rmse:0.12353035526161232\n",
      "Training_R_Squared:-0.5219987313383359\n",
      "Epoch number: 508/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9787893295288086\n",
      "training loss:1.4762858649728514\n",
      "training rmse:0.12150250470557598\n",
      "Training_R_Squared:-0.47243921517435417\n",
      "Epoch number: 509/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7768733501434326\n",
      "training loss:1.1472022164445832\n",
      "training rmse:0.10710752618021682\n",
      "Training_R_Squared:-0.14421303169271216\n",
      "Epoch number: 510/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.840819835662842\n",
      "training loss:1.359113480221879\n",
      "training rmse:0.11658102247886999\n",
      "Training_R_Squared:-0.3555721310343105\n",
      "Epoch number: 511/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9154579639434814\n",
      "training loss:1.2535987920794582\n",
      "training rmse:0.11196422607598636\n",
      "Training_R_Squared:-0.25033237751459336\n",
      "Epoch number: 512/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7825756072998047\n",
      "training loss:1.535670393718192\n",
      "training rmse:0.12392216886893935\n",
      "Training_R_Squared:-0.5316689921714521\n",
      "Epoch number: 513/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8926479816436768\n",
      "training loss:1.4673467250777321\n",
      "training rmse:0.12113408789757457\n",
      "Training_R_Squared:-0.46352335391400445\n",
      "Epoch number: 514/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1914374828338623\n",
      "training loss:1.2517040124500909\n",
      "training rmse:0.11187957867502411\n",
      "Training_R_Squared:-0.2484425182276857\n",
      "Epoch number: 515/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4227194786071777\n",
      "training loss:1.4685297747507207\n",
      "training rmse:0.12118291029475735\n",
      "Training_R_Squared:-0.46470332278911175\n",
      "Epoch number: 516/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.215135335922241\n",
      "training loss:1.2445611975588236\n",
      "training rmse:0.11155990308165491\n",
      "Training_R_Squared:-0.2413183299355528\n",
      "Epoch number: 517/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.916975259780884\n",
      "training loss:1.0808290631218966\n",
      "training rmse:0.10396292912004243\n",
      "Training_R_Squared:-0.07801282678060262\n",
      "Epoch number: 518/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9314818382263184\n",
      "training loss:1.3117565478466986\n",
      "training rmse:0.11453194086571215\n",
      "Training_R_Squared:-0.30833858930325353\n",
      "Epoch number: 519/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.860628366470337\n",
      "training loss:1.5184333526885894\n",
      "training rmse:0.12322472774117171\n",
      "Training_R_Squared:-0.5144768841660994\n",
      "Epoch number: 520/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.906644582748413\n",
      "training loss:1.238240801009411\n",
      "training rmse:0.11127626885411872\n",
      "Training_R_Squared:-0.23501440018164987\n",
      "Epoch number: 521/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7371299266815186\n",
      "training loss:1.6241623128187257\n",
      "training rmse:0.1274426268098208\n",
      "Training_R_Squared:-0.619930337258958\n",
      "Epoch number: 522/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9318885803222656\n",
      "training loss:1.1983806185026111\n",
      "training rmse:0.10947057223302577\n",
      "Training_R_Squared:-0.1952580896156515\n",
      "Epoch number: 523/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7641074657440186\n",
      "training loss:1.650043698023186\n",
      "training rmse:0.12845402671863526\n",
      "Training_R_Squared:-0.6457442877645976\n",
      "Epoch number: 524/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8915858268737793\n",
      "training loss:1.3541324389859852\n",
      "training rmse:0.11636719636504032\n",
      "Training_R_Squared:-0.3506040651217841\n",
      "Epoch number: 525/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9295198917388916\n",
      "training loss:1.1881563144465872\n",
      "training rmse:0.10900258320088507\n",
      "Training_R_Squared:-0.18506042017341473\n",
      "Epoch number: 526/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8438074588775635\n",
      "training loss:1.3071028005801715\n",
      "training rmse:0.11432859662307465\n",
      "Training_R_Squared:-0.30369697552797215\n",
      "Epoch number: 527/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.924544334411621\n",
      "training loss:1.0588644944100736\n",
      "training rmse:0.10290114160737351\n",
      "Training_R_Squared:-0.05610548707916596\n",
      "Epoch number: 528/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.418070077896118\n",
      "training loss:1.1856131164527142\n",
      "training rmse:0.10888586301502662\n",
      "Training_R_Squared:-0.18252384969837587\n",
      "Epoch number: 529/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.344425678253174\n",
      "training loss:1.4021513299513586\n",
      "training rmse:0.11841247104724058\n",
      "Training_R_Squared:-0.3984978409709683\n",
      "Epoch number: 530/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9654672145843506\n",
      "training loss:1.5406645075236156\n",
      "training rmse:0.12412350734343658\n",
      "Training_R_Squared:-0.5366501063093381\n",
      "Epoch number: 531/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.798021078109741\n",
      "training loss:1.4421246695810979\n",
      "training rmse:0.12008849526832693\n",
      "Training_R_Squared:-0.43836702599977295\n",
      "Epoch number: 532/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.768463134765625\n",
      "training loss:1.3340732663441486\n",
      "training rmse:0.11550208943322837\n",
      "Training_R_Squared:-0.33059715831387715\n",
      "Epoch number: 533/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9757087230682373\n",
      "training loss:1.551923125287658\n",
      "training rmse:0.12457620660815041\n",
      "Training_R_Squared:-0.547879387180821\n",
      "Epoch number: 534/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7840638160705566\n",
      "training loss:1.2169935166797927\n",
      "training rmse:0.1103174291161552\n",
      "Training_R_Squared:-0.2138224769132402\n",
      "Epoch number: 535/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7137691974639893\n",
      "training loss:1.2319737093630465\n",
      "training rmse:0.11099431108678708\n",
      "Training_R_Squared:-0.22876363494920193\n",
      "Epoch number: 536/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9036240577697754\n",
      "training loss:1.4188487775309113\n",
      "training rmse:0.11911543886209341\n",
      "Training_R_Squared:-0.41515177842272233\n",
      "Epoch number: 537/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.945613384246826\n",
      "training loss:1.6773563879387439\n",
      "training rmse:0.12951279426908927\n",
      "Training_R_Squared:-0.6729858280229564\n",
      "Epoch number: 538/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.786027193069458\n",
      "training loss:1.364262714634151\n",
      "training rmse:0.11680165729278635\n",
      "Training_R_Squared:-0.36070793875922647\n",
      "Epoch number: 539/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8001232147216797\n",
      "training loss:1.3921112799578168\n",
      "training rmse:0.11798776546565397\n",
      "Training_R_Squared:-0.3884839513756939\n",
      "Epoch number: 540/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9729044437408447\n",
      "training loss:1.1507607343774566\n",
      "training rmse:0.10727351650698586\n",
      "Training_R_Squared:-0.14776227696337751\n",
      "Epoch number: 541/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7488479614257812\n",
      "training loss:1.3960092324139879\n",
      "training rmse:0.11815283460052864\n",
      "Training_R_Squared:-0.39237174605512526\n",
      "Epoch number: 542/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.967615842819214\n",
      "training loss:1.307605337291534\n",
      "training rmse:0.11435057224568375\n",
      "Training_R_Squared:-0.30419820626868277\n",
      "Epoch number: 543/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6665210723876953\n",
      "training loss:1.38016562644043\n",
      "training rmse:0.11748045056265446\n",
      "Training_R_Squared:-0.3765694203648924\n",
      "Epoch number: 544/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8858931064605713\n",
      "training loss:1.6707234982632144\n",
      "training rmse:0.1292564697902281\n",
      "Training_R_Squared:-0.666370203537308\n",
      "Epoch number: 545/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0734429359436035\n",
      "training loss:1.3528924494285093\n",
      "training rmse:0.11631390499112775\n",
      "Training_R_Squared:-0.34936729939450784\n",
      "Epoch number: 546/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8911893367767334\n",
      "training loss:1.42580805029138\n",
      "training rmse:0.11940720456871017\n",
      "Training_R_Squared:-0.4220929259919557\n",
      "Epoch number: 547/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0057260990142822\n",
      "training loss:1.2797107267417402\n",
      "training rmse:0.11312430007481772\n",
      "Training_R_Squared:-0.2763762604148885\n",
      "Epoch number: 548/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.766162395477295\n",
      "training loss:1.3871632719041145\n",
      "training rmse:0.1177778957149479\n",
      "Training_R_Squared:-0.3835488328323644\n",
      "Epoch number: 549/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9379682540893555\n",
      "training loss:1.7524992359042386\n",
      "training rmse:0.1323819940892355\n",
      "Training_R_Squared:-0.747932869943597\n",
      "Epoch number: 550/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8264026641845703\n",
      "training loss:1.7394589718782676\n",
      "training rmse:0.13188855037031333\n",
      "Training_R_Squared:-0.73492658425293\n",
      "Epoch number: 551/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.826073169708252\n",
      "training loss:1.512140826059749\n",
      "training rmse:0.12296913539826769\n",
      "Training_R_Squared:-0.5082007516838305\n",
      "Epoch number: 552/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.929347038269043\n",
      "training loss:1.3801389685767163\n",
      "training rmse:0.11747931599122954\n",
      "Training_R_Squared:-0.3765428332198901\n",
      "Epoch number: 553/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8620240688323975\n",
      "training loss:1.1680549206073465\n",
      "training rmse:0.108076589537575\n",
      "Training_R_Squared:-0.16501140090916322\n",
      "Epoch number: 554/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.724848508834839\n",
      "training loss:1.3515869718312992\n",
      "training rmse:0.116257772722141\n",
      "Training_R_Squared:-0.34806523228742425\n",
      "Epoch number: 555/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.907377243041992\n",
      "training loss:1.731571259580278\n",
      "training rmse:0.13158918115028598\n",
      "Training_R_Squared:-0.7270594246415045\n",
      "Epoch number: 556/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8848912715911865\n",
      "training loss:1.5148011297357016\n",
      "training rmse:0.12307725743352024\n",
      "Training_R_Squared:-0.510854118399579\n",
      "Epoch number: 557/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9405980110168457\n",
      "training loss:1.2919866226748127\n",
      "training rmse:0.11366558945761962\n",
      "Training_R_Squared:-0.288620176560727\n",
      "Epoch number: 558/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8051540851593018\n",
      "training loss:1.2150407898077447\n",
      "training rmse:0.11022888867296743\n",
      "Training_R_Squared:-0.2118748454744741\n",
      "Epoch number: 559/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7160768508911133\n",
      "training loss:1.5354534192342726\n",
      "training rmse:0.12391341409364333\n",
      "Training_R_Squared:-0.5314525990123617\n",
      "Epoch number: 560/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.915574550628662\n",
      "training loss:1.4891530722590147\n",
      "training rmse:0.12203085971421387\n",
      "Training_R_Squared:-0.4852728900349168\n",
      "Epoch number: 561/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.709193229675293\n",
      "training loss:1.294405427154178\n",
      "training rmse:0.11377193973709765\n",
      "Training_R_Squared:-0.2910326841808728\n",
      "Epoch number: 562/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.093583583831787\n",
      "training loss:1.30895221918162\n",
      "training rmse:0.11440944974876945\n",
      "Training_R_Squared:-0.3055415569067428\n",
      "Epoch number: 563/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3124349117279053\n",
      "training loss:1.140857942748994\n",
      "training rmse:0.1068109518143619\n",
      "Training_R_Squared:-0.13788528565667502\n",
      "Epoch number: 564/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.259469509124756\n",
      "training loss:1.4827942831169736\n",
      "training rmse:0.12177004077838578\n",
      "Training_R_Squared:-0.478930679185386\n",
      "Epoch number: 565/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9690051078796387\n",
      "training loss:1.3518791835858792\n",
      "training rmse:0.11627033945017445\n",
      "Training_R_Squared:-0.34835667400905534\n",
      "Epoch number: 566/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6925108432769775\n",
      "training loss:0.9971271199433431\n",
      "training rmse:0.09985625268070814\n",
      "Training_R_Squared:0.005471022184005858\n",
      "Epoch number: 567/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.910977840423584\n",
      "training loss:1.6935794309283665\n",
      "training rmse:0.13013759760070748\n",
      "Training_R_Squared:-0.6891665875028761\n",
      "Epoch number: 568/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7265806198120117\n",
      "training loss:1.3306531697744504\n",
      "training rmse:0.1153539409718823\n",
      "Training_R_Squared:-0.3271859715391505\n",
      "Epoch number: 569/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9260878562927246\n",
      "training loss:1.2105496495804893\n",
      "training rmse:0.11002498123519446\n",
      "Training_R_Squared:-0.20739539709552846\n",
      "Epoch number: 570/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.984133243560791\n",
      "training loss:1.4854744345448125\n",
      "training rmse:0.12188004080015778\n",
      "Training_R_Squared:-0.48160383565753295\n",
      "Epoch number: 571/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6980910301208496\n",
      "training loss:1.2389852089416422\n",
      "training rmse:0.11130971246668649\n",
      "Training_R_Squared:-0.23575687044704874\n",
      "Epoch number: 572/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.841492176055908\n",
      "training loss:1.556575204636033\n",
      "training rmse:0.12476278309800697\n",
      "Training_R_Squared:-0.5525193451231569\n",
      "Epoch number: 573/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9015696048736572\n",
      "training loss:1.2346442868224585\n",
      "training rmse:0.1111145484093986\n",
      "Training_R_Squared:-0.2314272561274231\n",
      "Epoch number: 574/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.915008068084717\n",
      "training loss:1.4393150076398342\n",
      "training rmse:0.11997145525664987\n",
      "Training_R_Squared:-0.43556468335807574\n",
      "Epoch number: 575/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.744382858276367\n",
      "training loss:1.1390256849245475\n",
      "training rmse:0.10672514628355154\n",
      "Training_R_Squared:-0.13605780479036111\n",
      "Epoch number: 576/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9595577716827393\n",
      "training loss:1.4511588984289006\n",
      "training rmse:0.12046405681484003\n",
      "Training_R_Squared:-0.4473777122096818\n",
      "Epoch number: 577/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.768714189529419\n",
      "training loss:1.3056929109728799\n",
      "training rmse:0.11426692045263494\n",
      "Training_R_Squared:-0.302290756827714\n",
      "Epoch number: 578/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.844541311264038\n",
      "training loss:1.571291434233899\n",
      "training rmse:0.12535116410444297\n",
      "Training_R_Squared:-0.56719721720072\n",
      "Epoch number: 579/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2046375274658203\n",
      "training loss:1.5737385835845998\n",
      "training rmse:0.1254487378806419\n",
      "Training_R_Squared:-0.5696379923064012\n",
      "Epoch number: 580/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.779208183288574\n",
      "training loss:1.362683616285981\n",
      "training rmse:0.11673404029185236\n",
      "Training_R_Squared:-0.35913296628869307\n",
      "Epoch number: 581/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0243334770202637\n",
      "training loss:1.1403687602310129\n",
      "training rmse:0.10678804990405119\n",
      "Training_R_Squared:-0.1373973754817741\n",
      "Epoch number: 582/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.379493474960327\n",
      "training loss:1.4687096114485598\n",
      "training rmse:0.12119033011955037\n",
      "Training_R_Squared:-0.4648826955565044\n",
      "Epoch number: 583/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.3089776039123535\n",
      "training loss:1.397011183684981\n",
      "training rmse:0.11819522763990858\n",
      "Training_R_Squared:-0.39337108228619866\n",
      "Epoch number: 584/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8333027362823486\n",
      "training loss:1.0787893486407114\n",
      "training rmse:0.10386478463082237\n",
      "Training_R_Squared:-0.07597841765724267\n",
      "Epoch number: 585/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8949203491210938\n",
      "training loss:1.3024504956388228\n",
      "training rmse:0.11412495325908453\n",
      "Training_R_Squared:-0.29905679087444015\n",
      "Epoch number: 586/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7056591510772705\n",
      "training loss:1.3932565331078877\n",
      "training rmse:0.11803628819595641\n",
      "Training_R_Squared:-0.38962621499971184\n",
      "Epoch number: 587/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9961159229278564\n",
      "training loss:1.2664612956811254\n",
      "training rmse:0.11253716255891319\n",
      "Training_R_Squared:-0.26316136195429873\n",
      "Epoch number: 588/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6911466121673584\n",
      "training loss:1.4184473742836872\n",
      "training rmse:0.1190985883326787\n",
      "Training_R_Squared:-0.41475142043377056\n",
      "Epoch number: 589/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9241840839385986\n",
      "training loss:1.44563394525116\n",
      "training rmse:0.12023451855649275\n",
      "Training_R_Squared:-0.4418671499321529\n",
      "Epoch number: 590/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0959596633911133\n",
      "training loss:1.5162873791468883\n",
      "training rmse:0.12313762134891547\n",
      "Training_R_Squared:-0.5123364907156394\n",
      "Epoch number: 591/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7697465419769287\n",
      "training loss:1.4698200045139629\n",
      "training rmse:0.12123613341384502\n",
      "Training_R_Squared:-0.4659901863752651\n",
      "Epoch number: 592/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.969264268875122\n",
      "training loss:1.5747728974765778\n",
      "training rmse:0.12548995567281782\n",
      "Training_R_Squared:-0.5706696281673631\n",
      "Epoch number: 593/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7521204948425293\n",
      "training loss:1.2234560635110938\n",
      "training rmse:0.11060994817425301\n",
      "Training_R_Squared:-0.22026817536583887\n",
      "Epoch number: 594/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9393513202667236\n",
      "training loss:1.9307306541149956\n",
      "training rmse:0.13895073422314094\n",
      "Training_R_Squared:-0.9256998961659844\n",
      "Epoch number: 595/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9101009368896484\n",
      "training loss:1.3002573544403049\n",
      "training rmse:0.11402882769020757\n",
      "Training_R_Squared:-0.29686935778935064\n",
      "Epoch number: 596/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7558467388153076\n",
      "training loss:1.3108360891731223\n",
      "training rmse:0.11449175032172067\n",
      "Training_R_Squared:-0.3074205390223881\n",
      "Epoch number: 597/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9174370765686035\n",
      "training loss:1.3150817595207513\n",
      "training rmse:0.11467701424089971\n",
      "Training_R_Squared:-0.3116551412363142\n",
      "Epoch number: 598/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.723262310028076\n",
      "training loss:1.6520436793293811\n",
      "training rmse:0.1285318512793378\n",
      "Training_R_Squared:-0.6477390540236141\n",
      "Epoch number: 599/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1821036338806152\n",
      "training loss:1.283774117177927\n",
      "training rmse:0.11330375621213655\n",
      "Training_R_Squared:-0.28042906959084046\n",
      "Epoch number: 600/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8197367191314697\n",
      "training loss:1.2557005201511515\n",
      "training rmse:0.11205804389472232\n",
      "Training_R_Squared:-0.2524286220328422\n",
      "Epoch number: 601/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8117401599884033\n",
      "training loss:1.2671412482095548\n",
      "training rmse:0.11256736863805401\n",
      "Training_R_Squared:-0.26383955014270466\n",
      "Epoch number: 602/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9097392559051514\n",
      "training loss:1.7464934552880322\n",
      "training rmse:0.13215496416283545\n",
      "Training_R_Squared:-0.7419427381758199\n",
      "Epoch number: 603/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7175188064575195\n",
      "training loss:1.2660894225792276\n",
      "training rmse:0.11252063911030845\n",
      "Training_R_Squared:-0.2627904671759709\n",
      "Epoch number: 604/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.927213430404663\n",
      "training loss:1.1135718832473174\n",
      "training rmse:0.10552591545432417\n",
      "Training_R_Squared:-0.11067033046001673\n",
      "Epoch number: 605/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0534374713897705\n",
      "training loss:1.169073321444003\n",
      "training rmse:0.10812369404732725\n",
      "Training_R_Squared:-0.16602714352816283\n",
      "Epoch number: 606/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.743100166320801\n",
      "training loss:1.5276766813569793\n",
      "training rmse:0.12359921849902529\n",
      "Training_R_Squared:-0.5236961103885955\n",
      "Epoch number: 607/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.944594621658325\n",
      "training loss:1.2806404940674838\n",
      "training rmse:0.11316538755589024\n",
      "Training_R_Squared:-0.2773036216976299\n",
      "Epoch number: 608/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.862870931625366\n",
      "training loss:1.5090134272518299\n",
      "training rmse:0.12284190763952788\n",
      "Training_R_Squared:-0.5050814911061237\n",
      "Epoch number: 609/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.742271900177002\n",
      "training loss:1.292409204107571\n",
      "training rmse:0.11368417674010622\n",
      "Training_R_Squared:-0.2890416670072007\n",
      "Epoch number: 610/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.893387794494629\n",
      "training loss:1.2203545371967266\n",
      "training rmse:0.11046965815085727\n",
      "Training_R_Squared:-0.21717474943219872\n",
      "Epoch number: 611/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8402247428894043\n",
      "training loss:1.0617253505019448\n",
      "training rmse:0.10304005776890582\n",
      "Training_R_Squared:-0.05895888599454291\n",
      "Epoch number: 612/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9087133407592773\n",
      "training loss:1.5585979061400792\n",
      "training rmse:0.12484381867517828\n",
      "Training_R_Squared:-0.5545367705616486\n",
      "Epoch number: 613/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.770599126815796\n",
      "training loss:1.30699494902251\n",
      "training rmse:0.11432387978994196\n",
      "Training_R_Squared:-0.3035894009286051\n",
      "Epoch number: 614/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9117019176483154\n",
      "training loss:1.36104448819151\n",
      "training rmse:0.11666381136374339\n",
      "Training_R_Squared:-0.3574981050528181\n",
      "Epoch number: 615/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.833643674850464\n",
      "training loss:1.5525834952641162\n",
      "training rmse:0.1246027084482563\n",
      "Training_R_Squared:-0.5485380333127181\n",
      "Epoch number: 616/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.067803144454956\n",
      "training loss:1.416171613730711\n",
      "training rmse:0.11900300894224107\n",
      "Training_R_Squared:-0.412481597819329\n",
      "Epoch number: 617/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4093387126922607\n",
      "training loss:1.2048347775898947\n",
      "training rmse:0.10976496606795333\n",
      "Training_R_Squared:-0.20169543207532614\n",
      "Epoch number: 618/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2317283153533936\n",
      "training loss:1.4721426137723483\n",
      "training rmse:0.12133188425852243\n",
      "Training_R_Squared:-0.4683067507542653\n",
      "Epoch number: 619/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.919900894165039\n",
      "training loss:1.4636896430406523\n",
      "training rmse:0.1209830419125198\n",
      "Training_R_Squared:-0.4598758007946675\n",
      "Epoch number: 620/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8712778091430664\n",
      "training loss:1.4506436520291572\n",
      "training rmse:0.12044266901846527\n",
      "Training_R_Squared:-0.4468637993781204\n",
      "Epoch number: 621/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.832939863204956\n",
      "training loss:1.184619873422264\n",
      "training rmse:0.10884024409299457\n",
      "Training_R_Squared:-0.18153319102316656\n",
      "Epoch number: 622/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8646059036254883\n",
      "training loss:1.8082314621406113\n",
      "training rmse:0.134470497215583\n",
      "Training_R_Squared:-0.803519887467264\n",
      "Epoch number: 623/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8660976886749268\n",
      "training loss:1.2117006133839823\n",
      "training rmse:0.11007727346659628\n",
      "Training_R_Squared:-0.20854337773384302\n",
      "Epoch number: 624/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.864421844482422\n",
      "training loss:1.2023626897132687\n",
      "training rmse:0.10965230000840241\n",
      "Training_R_Squared:-0.1992297610013647\n",
      "Epoch number: 625/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8432774543762207\n",
      "training loss:1.5789834423583216\n",
      "training rmse:0.12565760790172323\n",
      "Training_R_Squared:-0.5748691815495666\n",
      "Epoch number: 626/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8792948722839355\n",
      "training loss:1.1084247103954112\n",
      "training rmse:0.10528175104904987\n",
      "Training_R_Squared:-0.10553655691332997\n",
      "Epoch number: 627/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.835726499557495\n",
      "training loss:1.7050485280313978\n",
      "training rmse:0.13057750679314556\n",
      "Training_R_Squared:-0.700605809846631\n",
      "Epoch number: 628/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8529257774353027\n",
      "training loss:1.5937588683578383\n",
      "training rmse:0.1262441629683463\n",
      "Training_R_Squared:-0.5896061182926049\n",
      "Epoch number: 629/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9204671382904053\n",
      "training loss:1.16501786016579\n",
      "training rmse:0.10793599307764717\n",
      "Training_R_Squared:-0.16198225106674502\n",
      "Epoch number: 630/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7488346099853516\n",
      "training loss:1.1691376464850691\n",
      "training rmse:0.10812666861071181\n",
      "Training_R_Squared:-0.16609130379111448\n",
      "Epoch number: 631/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.870800495147705\n",
      "training loss:1.3582078118245455\n",
      "training rmse:0.11654217313164131\n",
      "Training_R_Squared:-0.3546688292332769\n",
      "Epoch number: 632/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.759657621383667\n",
      "training loss:1.2359002268395045\n",
      "training rmse:0.11117104959653411\n",
      "Training_R_Squared:-0.23267992059474962\n",
      "Epoch number: 633/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1101226806640625\n",
      "training loss:1.7438781240696244\n",
      "training rmse:0.13205597767877167\n",
      "Training_R_Squared:-0.7393342085634804\n",
      "Epoch number: 634/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8429317474365234\n",
      "training loss:1.4198137846397003\n",
      "training rmse:0.1191559391990051\n",
      "Training_R_Squared:-0.4161142742992947\n",
      "Epoch number: 635/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.902926445007324\n",
      "training loss:1.5392971008591871\n",
      "training rmse:0.1240684126141375\n",
      "Training_R_Squared:-0.5352862646977767\n",
      "Epoch number: 636/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3039493560791016\n",
      "training loss:1.2217668128953392\n",
      "training rmse:0.1105335610977652\n",
      "Training_R_Squared:-0.2185833399753585\n",
      "Epoch number: 637/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2924110889434814\n",
      "training loss:0.9907736405720726\n",
      "training rmse:0.0995376130200073\n",
      "Training_R_Squared:0.011807953036614638\n",
      "Epoch number: 638/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.81170916557312\n",
      "training loss:1.4520452828760426\n",
      "training rmse:0.12050084161017477\n",
      "Training_R_Squared:-0.4482617883319895\n",
      "Epoch number: 639/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8256518840789795\n",
      "training loss:1.341577881556077\n",
      "training rmse:0.1158265030792209\n",
      "Training_R_Squared:-0.33808222851180236\n",
      "Epoch number: 640/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8582205772399902\n",
      "training loss:1.3970995473155199\n",
      "training rmse:0.11819896561795792\n",
      "Training_R_Squared:-0.3934592257814229\n",
      "Epoch number: 641/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8170108795166016\n",
      "training loss:1.3376583164820204\n",
      "training rmse:0.11565717947806009\n",
      "Training_R_Squared:-0.3341728745148411\n",
      "Epoch number: 642/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8805229663848877\n",
      "training loss:1.3546612736432735\n",
      "training rmse:0.11638991681598855\n",
      "Training_R_Squared:-0.3511315273975657\n",
      "Epoch number: 643/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8033525943756104\n",
      "training loss:1.441492765502744\n",
      "training rmse:0.12006218245154233\n",
      "Training_R_Squared:-0.4377367858607024\n",
      "Epoch number: 644/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8445887565612793\n",
      "training loss:1.37304200982544\n",
      "training rmse:0.11717687527091\n",
      "Training_R_Squared:-0.36946437190235826\n",
      "Epoch number: 645/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8230037689208984\n",
      "training loss:1.2130568894508542\n",
      "training rmse:0.11013886187222266\n",
      "Training_R_Squared:-0.20989610652388113\n",
      "Epoch number: 646/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.855286121368408\n",
      "training loss:1.4158234506500094\n",
      "training rmse:0.11898837971205463\n",
      "Training_R_Squared:-0.4121343415636438\n",
      "Epoch number: 647/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.917022705078125\n",
      "training loss:1.3295232404516355\n",
      "training rmse:0.11530495394611784\n",
      "Training_R_Squared:-0.3260590035773898\n",
      "Epoch number: 648/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8207051753997803\n",
      "training loss:1.1673777617133965\n",
      "training rmse:0.10804525726349105\n",
      "Training_R_Squared:-0.16433600745406385\n",
      "Epoch number: 649/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.821098804473877\n",
      "training loss:0.9644932197292064\n",
      "training rmse:0.09820861569787075\n",
      "Training_R_Squared:0.03801989505943548\n",
      "Epoch number: 650/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.820206880569458\n",
      "training loss:1.2422116108841692\n",
      "training rmse:0.11145454727754131\n",
      "Training_R_Squared:-0.2389748585726481\n",
      "Epoch number: 651/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8653790950775146\n",
      "training loss:1.1135884820841682\n",
      "training rmse:0.10552670193293108\n",
      "Training_R_Squared:-0.11068687885542072\n",
      "Epoch number: 652/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8649661540985107\n",
      "training loss:1.0680432778516433\n",
      "training rmse:0.1033461793126211\n",
      "Training_R_Squared:-0.0652603476279705\n",
      "Epoch number: 653/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8364992141723633\n",
      "training loss:1.535002381697268\n",
      "training rmse:0.12389521305108071\n",
      "Training_R_Squared:-0.5310027286772268\n",
      "Epoch number: 654/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8406145572662354\n",
      "training loss:1.5887980998218438\n",
      "training rmse:0.1260475346772734\n",
      "Training_R_Squared:-0.5846582868441743\n",
      "Epoch number: 655/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.040484666824341\n",
      "training loss:1.5130211714365487\n",
      "training rmse:0.12300492556952948\n",
      "Training_R_Squared:-0.5090787959615839\n",
      "Epoch number: 656/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.859692096710205\n",
      "training loss:1.317932813131847\n",
      "training rmse:0.11480125492048626\n",
      "Training_R_Squared:-0.31449876322194137\n",
      "Epoch number: 657/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8944003582000732\n",
      "training loss:1.612266869195981\n",
      "training rmse:0.1269750711437478\n",
      "Training_R_Squared:-0.6080659001933681\n",
      "Epoch number: 658/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9284374713897705\n",
      "training loss:1.036858621348589\n",
      "training rmse:0.10182625503025186\n",
      "Training_R_Squared:-0.03415695204023672\n",
      "Epoch number: 659/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.302722454071045\n",
      "training loss:1.4678401682022866\n",
      "training rmse:0.1211544538266046\n",
      "Training_R_Squared:-0.4640155232083696\n",
      "Epoch number: 660/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.156144857406616\n",
      "training loss:1.3268447033442499\n",
      "training rmse:0.11518874525509208\n",
      "Training_R_Squared:-0.3233874358745037\n",
      "Epoch number: 661/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9020676612854004\n",
      "training loss:1.2535909723634404\n",
      "training rmse:0.11196387686943679\n",
      "Training_R_Squared:-0.2503245883206451\n",
      "Epoch number: 662/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.771653413772583\n",
      "training loss:1.2267339270706543\n",
      "training rmse:0.1107580212477026\n",
      "Training_R_Squared:-0.22353751809284916\n",
      "Epoch number: 663/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9191436767578125\n",
      "training loss:1.3328993477425684\n",
      "training rmse:0.1154512601811937\n",
      "Training_R_Squared:-0.32942630575523557\n",
      "Epoch number: 664/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8499417304992676\n",
      "training loss:1.519430011490158\n",
      "training rmse:0.12326516180536\n",
      "Training_R_Squared:-0.5154709404587037\n",
      "Epoch number: 665/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.914005994796753\n",
      "training loss:1.474662108472458\n",
      "training rmse:0.12143566644410768\n",
      "Training_R_Squared:-0.47081967545850634\n",
      "Epoch number: 666/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8615903854370117\n",
      "training loss:0.9794570197366284\n",
      "training rmse:0.09896752092159469\n",
      "Training_R_Squared:0.023095080868856255\n",
      "Epoch number: 667/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.897840738296509\n",
      "training loss:0.9797158471377863\n",
      "training rmse:0.09898059643878623\n",
      "Training_R_Squared:0.022836930943576195\n",
      "Epoch number: 668/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3568835258483887\n",
      "training loss:1.3458188699370375\n",
      "training rmse:0.11600943366541522\n",
      "Training_R_Squared:-0.3423121646074818\n",
      "Epoch number: 669/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.153196334838867\n",
      "training loss:1.274193650311659\n",
      "training rmse:0.11288018649486982\n",
      "Training_R_Squared:-0.2708735607417747\n",
      "Epoch number: 670/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.826307773590088\n",
      "training loss:1.461473084611299\n",
      "training rmse:0.12089140104289052\n",
      "Training_R_Squared:-0.4576650275146148\n",
      "Epoch number: 671/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8167850971221924\n",
      "training loss:1.2921564689742695\n",
      "training rmse:0.1136730605277376\n",
      "Training_R_Squared:-0.2887895878907867\n",
      "Epoch number: 672/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9091992378234863\n",
      "training loss:1.182612844741243\n",
      "training rmse:0.1087480043376081\n",
      "Training_R_Squared:-0.17953137939351316\n",
      "Epoch number: 673/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7693240642547607\n",
      "training loss:1.5958296554969706\n",
      "training rmse:0.12632615150858395\n",
      "Training_R_Squared:-0.5916715144980349\n",
      "Epoch number: 674/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8735222816467285\n",
      "training loss:1.4558801086109838\n",
      "training rmse:0.12065985697865649\n",
      "Training_R_Squared:-0.45208663366099433\n",
      "Epoch number: 675/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8519887924194336\n",
      "training loss:1.3020053519339854\n",
      "training rmse:0.1141054491220286\n",
      "Training_R_Squared:-0.2986127999458199\n",
      "Epoch number: 676/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9176933765411377\n",
      "training loss:0.9854863906334685\n",
      "training rmse:0.09927166718825008\n",
      "Training_R_Squared:0.01708141940316943\n",
      "Epoch number: 677/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8021693229675293\n",
      "training loss:1.3188312368642983\n",
      "training rmse:0.11484037777995587\n",
      "Training_R_Squared:-0.31539483828544723\n",
      "Epoch number: 678/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8506531715393066\n",
      "training loss:1.2746543213098676\n",
      "training rmse:0.1129005899590373\n",
      "Training_R_Squared:-0.27133303931174524\n",
      "Epoch number: 679/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.875720739364624\n",
      "training loss:1.3127180444805617\n",
      "training rmse:0.11457390822000277\n",
      "Training_R_Squared:-0.3092975860516536\n",
      "Epoch number: 680/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9189298152923584\n",
      "training loss:1.1856736995578103\n",
      "training rmse:0.10888864493407062\n",
      "Training_R_Squared:-0.18258427257478327\n",
      "Epoch number: 681/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.808518886566162\n",
      "training loss:1.2296555093433597\n",
      "training rmse:0.11088983313827105\n",
      "Training_R_Squared:-0.22645147582296965\n",
      "Epoch number: 682/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.823173999786377\n",
      "training loss:1.1198099584224366\n",
      "training rmse:0.10582107344108907\n",
      "Training_R_Squared:-0.1168921420150344\n",
      "Epoch number: 683/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8573172092437744\n",
      "training loss:1.2946425539455646\n",
      "training rmse:0.11378236040553757\n",
      "Training_R_Squared:-0.2912691999838213\n",
      "Epoch number: 684/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8960554599761963\n",
      "training loss:1.2363026437728877\n",
      "training rmse:0.11118914712204998\n",
      "Training_R_Squared:-0.2330812920780525\n",
      "Epoch number: 685/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7747488021850586\n",
      "training loss:1.4921893202886167\n",
      "training rmse:0.12215520129280688\n",
      "Training_R_Squared:-0.4883012319953046\n",
      "Epoch number: 686/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7973248958587646\n",
      "training loss:1.465983906565441\n",
      "training rmse:0.12107782235262744\n",
      "Training_R_Squared:-0.46216409949794945\n",
      "Epoch number: 687/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.824298620223999\n",
      "training loss:1.246842839628016\n",
      "training rmse:0.11166211710459444\n",
      "Training_R_Squared:-0.24359402547347342\n",
      "Epoch number: 688/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.01482892036438\n",
      "training loss:1.172758788395413\n",
      "training rmse:0.108293988217048\n",
      "Training_R_Squared:-0.1697030135805624\n",
      "Epoch number: 689/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3364810943603516\n",
      "training loss:1.133379162101619\n",
      "training rmse:0.10646028189431113\n",
      "Training_R_Squared:-0.13042599888177864\n",
      "Epoch number: 690/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.229085922241211\n",
      "training loss:1.3060035553193643\n",
      "training rmse:0.11428051256970125\n",
      "Training_R_Squared:-0.30260058898532227\n",
      "Epoch number: 691/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8703317642211914\n",
      "training loss:1.4128136899853416\n",
      "training rmse:0.11886183954429368\n",
      "Training_R_Squared:-0.4091324228623592\n",
      "Epoch number: 692/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849989414215088\n",
      "training loss:1.6756125411686753\n",
      "training rmse:0.12944545342223016\n",
      "Training_R_Squared:-0.6712465117420578\n",
      "Epoch number: 693/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.854749917984009\n",
      "training loss:1.3929759205084338\n",
      "training rmse:0.11802440088847872\n",
      "Training_R_Squared:-0.3893463534993986\n",
      "Epoch number: 694/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9942917823791504\n",
      "training loss:0.9016325267457432\n",
      "training rmse:0.09495433253652744\n",
      "Training_R_Squared:0.10071679090596497\n",
      "Epoch number: 695/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8486430644989014\n",
      "training loss:1.1961857723249523\n",
      "training rmse:0.1093702780614986\n",
      "Training_R_Squared:-0.19306894874339986\n",
      "Epoch number: 696/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8170337677001953\n",
      "training loss:1.264292222423876\n",
      "training rmse:0.11244074983847609\n",
      "Training_R_Squared:-0.26099794180341074\n",
      "Epoch number: 697/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.979248046875\n",
      "training loss:1.150605676139321\n",
      "training rmse:0.1072662890259247\n",
      "Training_R_Squared:-0.1476076301575091\n",
      "Epoch number: 698/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7897047996520996\n",
      "training loss:1.128025037138741\n",
      "training rmse:0.10620852306377022\n",
      "Training_R_Squared:-0.12508581333108149\n",
      "Epoch number: 699/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.886519432067871\n",
      "training loss:1.5454194366506222\n",
      "training rmse:0.124314900018084\n",
      "Training_R_Squared:-0.5413926399526134\n",
      "Epoch number: 700/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8481574058532715\n",
      "training loss:1.613131196721156\n",
      "training rmse:0.1270091019069561\n",
      "Training_R_Squared:-0.6089279725818026\n",
      "Epoch number: 701/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849818229675293\n",
      "training loss:1.4555741938149822\n",
      "training rmse:0.12064717956980935\n",
      "Training_R_Squared:-0.4517815101496434\n",
      "Epoch number: 702/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8324782848358154\n",
      "training loss:1.010834832372808\n",
      "training rmse:0.10054028209492989\n",
      "Training_R_Squared:-0.008200969507247136\n",
      "Epoch number: 703/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3805322647094727\n",
      "training loss:1.3875749293656554\n",
      "training rmse:0.11779537042539726\n",
      "Training_R_Squared:-0.3839594179939567\n",
      "Epoch number: 704/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2422730922698975\n",
      "training loss:1.6935889649380442\n",
      "training rmse:0.1301379639051589\n",
      "Training_R_Squared:-0.6891761160548728\n",
      "Epoch number: 705/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9110071659088135\n",
      "training loss:1.6258596348325796\n",
      "training rmse:0.12750920103398733\n",
      "Training_R_Squared:-0.6216232431110031\n",
      "Epoch number: 706/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8758766651153564\n",
      "training loss:1.6461973092918925\n",
      "training rmse:0.12830422086945903\n",
      "Training_R_Squared:-0.6419079266803394\n",
      "Epoch number: 707/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.849458694458008\n",
      "training loss:1.1290997653431987\n",
      "training rmse:0.1062591062141593\n",
      "Training_R_Squared:-0.12615775464043688\n",
      "Epoch number: 708/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.902724027633667\n",
      "training loss:1.5435758211360167\n",
      "training rmse:0.12424072686265228\n",
      "Training_R_Squared:-0.5395538278893528\n",
      "Epoch number: 709/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.917769432067871\n",
      "training loss:1.368405934115799\n",
      "training rmse:0.11697888416786165\n",
      "Training_R_Squared:-0.3648403681519077\n",
      "Epoch number: 710/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.70701003074646\n",
      "training loss:1.5620963065314335\n",
      "training rmse:0.12498385121812472\n",
      "Training_R_Squared:-0.5580260598120244\n",
      "Epoch number: 711/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8815181255340576\n",
      "training loss:1.3035586160876846\n",
      "training rmse:0.11417349149814437\n",
      "Training_R_Squared:-0.30016201245684737\n",
      "Epoch number: 712/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.812889337539673\n",
      "training loss:1.7106100826806596\n",
      "training rmse:0.13079029332028655\n",
      "Training_R_Squared:-0.7061528649608368\n",
      "Epoch number: 713/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.867894411087036\n",
      "training loss:1.0919961521635742\n",
      "training rmse:0.10449861971162941\n",
      "Training_R_Squared:-0.08915081581949558\n",
      "Epoch number: 714/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9799439907073975\n",
      "training loss:1.2975132474174558\n",
      "training rmse:0.11390843899454754\n",
      "Training_R_Squared:-0.2941324149864384\n",
      "Epoch number: 715/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.826841354370117\n",
      "training loss:1.3403648648670128\n",
      "training rmse:0.11577412771716368\n",
      "Training_R_Squared:-0.336872377898793\n",
      "Epoch number: 716/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.728086233139038\n",
      "training loss:1.4732006125591397\n",
      "training rmse:0.1213754757996499\n",
      "Training_R_Squared:-0.469361999481001\n",
      "Epoch number: 717/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.843130588531494\n",
      "training loss:1.3503036589472686\n",
      "training rmse:0.11620256705199195\n",
      "Training_R_Squared:-0.3467852713351598\n",
      "Epoch number: 718/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9517598152160645\n",
      "training loss:1.3969807382241584\n",
      "training rmse:0.11819393970183743\n",
      "Training_R_Squared:-0.3933407126146009\n",
      "Epoch number: 719/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8358042240142822\n",
      "training loss:1.5291415904612222\n",
      "training rmse:0.1236584647511533\n",
      "Training_R_Squared:-0.5251572051782503\n",
      "Epoch number: 720/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7120776176452637\n",
      "training loss:1.549824012312456\n",
      "training rmse:0.12449192794364043\n",
      "Training_R_Squared:-0.5457857363825547\n",
      "Epoch number: 721/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1141417026519775\n",
      "training loss:1.6107966770297537\n",
      "training rmse:0.1269171649947222\n",
      "Training_R_Squared:-0.6065995334702627\n",
      "Epoch number: 722/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3909292221069336\n",
      "training loss:1.3314483098906749\n",
      "training rmse:0.11538840105880117\n",
      "Training_R_Squared:-0.32797904489806995\n",
      "Epoch number: 723/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.3365018367767334\n",
      "training loss:1.4459225292580413\n",
      "training rmse:0.12024651883767952\n",
      "Training_R_Squared:-0.44215498920166696\n",
      "Epoch number: 724/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.871020793914795\n",
      "training loss:1.249031411127362\n",
      "training rmse:0.11176007386930996\n",
      "Training_R_Squared:-0.24577689018133553\n",
      "Epoch number: 725/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9393680095672607\n",
      "training loss:1.0056976614934143\n",
      "training rmse:0.10028447843477147\n",
      "Training_R_Squared:-0.00307718316501826\n",
      "Epoch number: 726/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.810001850128174\n",
      "training loss:1.2278377189247465\n",
      "training rmse:0.11080783902435543\n",
      "Training_R_Squared:-0.22463841928670214\n",
      "Epoch number: 727/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.854095458984375\n",
      "training loss:1.1352066051616276\n",
      "training rmse:0.10654607478277309\n",
      "Training_R_Squared:-0.13224866865780038\n",
      "Epoch number: 728/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.881171941757202\n",
      "training loss:1.4715393382077764\n",
      "training rmse:0.12130702115738298\n",
      "Training_R_Squared:-0.46770504075767705\n",
      "Epoch number: 729/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9846930503845215\n",
      "training loss:1.2754379393581985\n",
      "training rmse:0.11293528852215318\n",
      "Training_R_Squared:-0.2721146136539139\n",
      "Epoch number: 730/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.720221757888794\n",
      "training loss:1.2159706208283865\n",
      "training rmse:0.11027105789047217\n",
      "Training_R_Squared:-0.21280224248757462\n",
      "Epoch number: 731/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.887251377105713\n",
      "training loss:1.206467065232573\n",
      "training rmse:0.10983929466418532\n",
      "Training_R_Squared:-0.20332346383219035\n",
      "Epoch number: 732/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.90669322013855\n",
      "training loss:1.221784269244381\n",
      "training rmse:0.110534350735162\n",
      "Training_R_Squared:-0.2186007436323265\n",
      "Epoch number: 733/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8596997261047363\n",
      "training loss:1.2840879346409224\n",
      "training rmse:0.11331760386810702\n",
      "Training_R_Squared:-0.28074207329197143\n",
      "Epoch number: 734/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9424989223480225\n",
      "training loss:1.1396684225865101\n",
      "training rmse:0.10675525385602856\n",
      "Training_R_Squared:-0.1366988563023679\n",
      "Epoch number: 735/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7722904682159424\n",
      "training loss:1.2794597246102057\n",
      "training rmse:0.11311320544526203\n",
      "Training_R_Squared:-0.276125920679142\n",
      "Epoch number: 736/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9473185539245605\n",
      "training loss:1.1196405201535526\n",
      "training rmse:0.10581306725322503\n",
      "Training_R_Squared:-0.11672315214020124\n",
      "Epoch number: 737/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6918156147003174\n",
      "training loss:1.3146521623797298\n",
      "training rmse:0.11465828196775538\n",
      "Training_R_Squared:-0.31122666698411616\n",
      "Epoch number: 738/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0655972957611084\n",
      "training loss:1.3541142087777445\n",
      "training rmse:0.11636641305710786\n",
      "Training_R_Squared:-0.35058589122349404\n",
      "Epoch number: 739/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.02746319770813\n",
      "training loss:1.7884966906767659\n",
      "training rmse:0.13373468849467463\n",
      "Training_R_Squared:-0.7838365322982905\n",
      "Epoch number: 740/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.785257577896118\n",
      "training loss:1.3369884928101783\n",
      "training rmse:0.11562821856321139\n",
      "Training_R_Squared:-0.33350478518038984\n",
      "Epoch number: 741/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4528095722198486\n",
      "training loss:1.1426296328872922\n",
      "training rmse:0.10689385543085685\n",
      "Training_R_Squared:-0.13965237088340166\n",
      "Epoch number: 742/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.252671957015991\n",
      "training loss:1.252261515454407\n",
      "training rmse:0.1119044912170377\n",
      "Training_R_Squared:-0.2489985889799453\n",
      "Epoch number: 743/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.876344919204712\n",
      "training loss:1.1434135768791123\n",
      "training rmse:0.10693051841635821\n",
      "Training_R_Squared:-0.14043426688593463\n",
      "Epoch number: 744/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9538416862487793\n",
      "training loss:1.4681100581019564\n",
      "training rmse:0.12116559157211078\n",
      "Training_R_Squared:-0.4642846996476293\n",
      "Epoch number: 745/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7465310096740723\n",
      "training loss:1.5879998325129066\n",
      "training rmse:0.12601586537071063\n",
      "Training_R_Squared:-0.5838620932345102\n",
      "Epoch number: 746/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8928561210632324\n",
      "training loss:1.2364218880325097\n",
      "training rmse:0.11119450921841913\n",
      "Training_R_Squared:-0.2332002279225267\n",
      "Epoch number: 747/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.878281831741333\n",
      "training loss:1.3097344114001999\n",
      "training rmse:0.11444362854262355\n",
      "Training_R_Squared:-0.3063217230279376\n",
      "Epoch number: 748/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.889983654022217\n",
      "training loss:1.126923327623345\n",
      "training rmse:0.10615664499329965\n",
      "Training_R_Squared:-0.123986984862132\n",
      "Epoch number: 749/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8549909591674805\n",
      "training loss:1.3402686185036146\n",
      "training rmse:0.11576997099868404\n",
      "Training_R_Squared:-0.3367763732456688\n",
      "Epoch number: 750/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.866015672683716\n",
      "training loss:1.310328633275276\n",
      "training rmse:0.11446958693361639\n",
      "Training_R_Squared:-0.3069144020230592\n",
      "Epoch number: 751/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8685476779937744\n",
      "training loss:1.100198316491685\n",
      "training rmse:0.10489033875870957\n",
      "Training_R_Squared:-0.09733159801441893\n",
      "Epoch number: 752/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8866689205169678\n",
      "training loss:1.3644576946675926\n",
      "training rmse:0.11681000362415851\n",
      "Training_R_Squared:-0.3609024140658579\n",
      "Epoch number: 753/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.799121141433716\n",
      "training loss:1.3508427064900843\n",
      "training rmse:0.11622575904205076\n",
      "Training_R_Squared:-0.34732290487693995\n",
      "Epoch number: 754/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9085943698883057\n",
      "training loss:1.2236807588985812\n",
      "training rmse:0.1106201048136631\n",
      "Training_R_Squared:-0.2204922912487306\n",
      "Epoch number: 755/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8282713890075684\n",
      "training loss:1.1032383121328166\n",
      "training rmse:0.10503515183655501\n",
      "Training_R_Squared:-0.10036368237121485\n",
      "Epoch number: 756/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.831240653991699\n",
      "training loss:1.5715330964781202\n",
      "training rmse:0.1253608031434914\n",
      "Training_R_Squared:-0.5674382695267626\n",
      "Epoch number: 757/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.837003469467163\n",
      "training loss:1.37927194273243\n",
      "training rmse:0.11744240898127176\n",
      "Training_R_Squared:-0.3756780674831415\n",
      "Epoch number: 758/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8808441162109375\n",
      "training loss:1.1296105843487112\n",
      "training rmse:0.10628313997754824\n",
      "Training_R_Squared:-0.12666723875712949\n",
      "Epoch number: 759/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8188982009887695\n",
      "training loss:1.2282546492114221\n",
      "training rmse:0.11082665064015162\n",
      "Training_R_Squared:-0.22505427022557822\n",
      "Epoch number: 760/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.081146478652954\n",
      "training loss:1.3802021702003913\n",
      "training rmse:0.11748200586474472\n",
      "Training_R_Squared:-0.37660587609348783\n",
      "Epoch number: 761/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8302597999572754\n",
      "training loss:1.2072573230941543\n",
      "training rmse:0.10987526214276598\n",
      "Training_R_Squared:-0.20411164314500718\n",
      "Epoch number: 762/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9456305503845215\n",
      "training loss:1.431812710180111\n",
      "training rmse:0.11965837664702422\n",
      "Training_R_Squared:-0.4280819260550035\n",
      "Epoch number: 763/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.811750888824463\n",
      "training loss:1.1014093465528276\n",
      "training rmse:0.10494805127075146\n",
      "Training_R_Squared:-0.09853947858009793\n",
      "Epoch number: 764/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.442824363708496\n",
      "training loss:1.4262581452923087\n",
      "training rmse:0.11942605014368969\n",
      "Training_R_Squared:-0.4225418391820437\n",
      "Epoch number: 765/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2345097064971924\n",
      "training loss:1.4484964534527762\n",
      "training rmse:0.12035349822305856\n",
      "Training_R_Squared:-0.44472220505257765\n",
      "Epoch number: 766/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8659942150115967\n",
      "training loss:0.9621723952895991\n",
      "training rmse:0.09809038664872308\n",
      "Training_R_Squared:0.040334673673673604\n",
      "Epoch number: 767/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.942748785018921\n",
      "training loss:1.2284896762869266\n",
      "training rmse:0.11083725349750086\n",
      "Training_R_Squared:-0.22528868782163447\n",
      "Epoch number: 768/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8358840942382812\n",
      "training loss:1.366969146569886\n",
      "training rmse:0.11691745577842028\n",
      "Training_R_Squared:-0.363407327671031\n",
      "Epoch number: 769/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9040236473083496\n",
      "training loss:1.4405885521216533\n",
      "training rmse:0.12002452049984008\n",
      "Training_R_Squared:-0.43683491863410184\n",
      "Epoch number: 770/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8516454696655273\n",
      "training loss:1.29695035720124\n",
      "training rmse:0.11388372830221356\n",
      "Training_R_Squared:-0.2935709730304805\n",
      "Epoch number: 771/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.885267734527588\n",
      "training loss:1.3882187630820226\n",
      "training rmse:0.11782269573736728\n",
      "Training_R_Squared:-0.3846015875306221\n",
      "Epoch number: 772/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9553658962249756\n",
      "training loss:1.1885637199027315\n",
      "training rmse:0.10902126947998411\n",
      "Training_R_Squared:-0.18546676799325224\n",
      "Epoch number: 773/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.349069118499756\n",
      "training loss:1.452218223787213\n",
      "training rmse:0.12050801731782051\n",
      "Training_R_Squared:-0.4484342714104401\n",
      "Epoch number: 774/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.320451498031616\n",
      "training loss:1.218432491825979\n",
      "training rmse:0.11038262960384569\n",
      "Training_R_Squared:-0.21525769718716115\n",
      "Epoch number: 775/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.880556106567383\n",
      "training loss:1.2041114458738775\n",
      "training rmse:0.10973201200533404\n",
      "Training_R_Squared:-0.2009739804454913\n",
      "Epoch number: 776/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.904865264892578\n",
      "training loss:0.9941918735116815\n",
      "training rmse:0.09970917076737132\n",
      "Training_R_Squared:0.008398624224520645\n",
      "Epoch number: 777/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.891512870788574\n",
      "training loss:1.4426990258084516\n",
      "training rmse:0.12011240676168518\n",
      "Training_R_Squared:-0.43893989029682334\n",
      "Epoch number: 778/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6320722103118896\n",
      "training loss:1.0483528392594508\n",
      "training rmse:0.10238910289964703\n",
      "Training_R_Squared:-0.045621216400465237\n",
      "Epoch number: 779/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.624584674835205\n",
      "training loss:1.245119220681545\n",
      "training rmse:0.11158491030070082\n",
      "Training_R_Squared:-0.24187489613123625\n",
      "Epoch number: 780/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.861205816268921\n",
      "training loss:1.170088334805996\n",
      "training rmse:0.10817062146470251\n",
      "Training_R_Squared:-0.16703952513761444\n",
      "Epoch number: 781/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.862339496612549\n",
      "training loss:1.0822995359767447\n",
      "training rmse:0.1040336261012152\n",
      "Training_R_Squared:-0.07947946397480199\n",
      "Epoch number: 782/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.916843891143799\n",
      "training loss:1.1093312625921499\n",
      "training rmse:0.10532479587410316\n",
      "Training_R_Squared:-0.10644075726969504\n",
      "Epoch number: 783/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8739826679229736\n",
      "training loss:1.2947477026845036\n",
      "training rmse:0.11378698091980925\n",
      "Training_R_Squared:-0.29137406868970883\n",
      "Epoch number: 784/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.879518747329712\n",
      "training loss:1.4529875224267244\n",
      "training rmse:0.12053993207343051\n",
      "Training_R_Squared:-0.44920157533341487\n",
      "Epoch number: 785/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9007205963134766\n",
      "training loss:0.9902757032725731\n",
      "training rmse:0.09951259735694637\n",
      "Training_R_Squared:0.012304595127650475\n",
      "Epoch number: 786/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9162304401397705\n",
      "training loss:1.1767172272906237\n",
      "training rmse:0.1084765978121836\n",
      "Training_R_Squared:-0.1736511387500883\n",
      "Epoch number: 787/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8699395656585693\n",
      "training loss:1.2907255396787605\n",
      "training rmse:0.11361010252960607\n",
      "Training_R_Squared:-0.2873623760830668\n",
      "Epoch number: 788/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8491859436035156\n",
      "training loss:1.66469559569126\n",
      "training rmse:0.12902308303909266\n",
      "Training_R_Squared:-0.6603580186656968\n",
      "Epoch number: 789/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.850956439971924\n",
      "training loss:1.2188462712854502\n",
      "training rmse:0.11040137097361835\n",
      "Training_R_Squared:-0.21567041510496376\n",
      "Epoch number: 790/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8868467807769775\n",
      "training loss:1.0819344677947527\n",
      "training rmse:0.1040160789394963\n",
      "Training_R_Squared:-0.07911534616320592\n",
      "Epoch number: 791/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.869485378265381\n",
      "training loss:1.2147481561752613\n",
      "training rmse:0.11021561396532079\n",
      "Training_R_Squared:-0.211582969815169\n",
      "Epoch number: 792/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.776848077774048\n",
      "training loss:1.3497982706130784\n",
      "training rmse:0.11618081901127562\n",
      "Training_R_Squared:-0.34628119149849446\n",
      "Epoch number: 793/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.065234899520874\n",
      "training loss:1.5527736507149825\n",
      "training rmse:0.12461033868483716\n",
      "Training_R_Squared:-0.548727695967514\n",
      "Epoch number: 794/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.422822952270508\n",
      "training loss:0.9972959988370889\n",
      "training rmse:0.09986470842280014\n",
      "Training_R_Squared:0.005302582440043668\n",
      "Epoch number: 795/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1898553371429443\n",
      "training loss:1.3795852495377403\n",
      "training rmse:0.11745574696615489\n",
      "Training_R_Squared:-0.3759905620596955\n",
      "Epoch number: 796/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9067282676696777\n",
      "training loss:1.5143601010443604\n",
      "training rmse:0.12305933938731999\n",
      "Training_R_Squared:-0.5104142383237147\n",
      "Epoch number: 797/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8742172718048096\n",
      "training loss:1.3139044341805857\n",
      "training rmse:0.11462567051845697\n",
      "Training_R_Squared:-0.31048087440633365\n",
      "Epoch number: 798/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.937756061553955\n",
      "training loss:1.172342882571229\n",
      "training rmse:0.10827478388670322\n",
      "Training_R_Squared:-0.16928818026068093\n",
      "Epoch number: 799/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.867680549621582\n",
      "training loss:1.499439241084474\n",
      "training rmse:0.12245159211233123\n",
      "Training_R_Squared:-0.49553225927342526\n",
      "Epoch number: 800/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8587324619293213\n",
      "training loss:0.9311700110165475\n",
      "training rmse:0.0964971507878107\n",
      "Training_R_Squared:0.07125627276783886\n",
      "Epoch number: 801/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9576640129089355\n",
      "training loss:1.3819801259887754\n",
      "training rmse:0.11755765079265472\n",
      "Training_R_Squared:-0.3783791864855883\n",
      "Epoch number: 802/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.87044095993042\n",
      "training loss:1.1445714049489197\n",
      "training rmse:0.10698464398916882\n",
      "Training_R_Squared:-0.14158907833033996\n",
      "Epoch number: 803/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8558003902435303\n",
      "training loss:1.1377991058016086\n",
      "training rmse:0.1066676664130986\n",
      "Training_R_Squared:-0.13483442419683467\n",
      "Epoch number: 804/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9647023677825928\n",
      "training loss:1.0045347038109185\n",
      "training rmse:0.10022647872747593\n",
      "Training_R_Squared:-0.0019172571036143449\n",
      "Epoch number: 805/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8452415466308594\n",
      "training loss:1.2692379128966422\n",
      "training rmse:0.11266045947432676\n",
      "Training_R_Squared:-0.2659307463174949\n",
      "Epoch number: 806/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.884761095046997\n",
      "training loss:1.3920032400347964\n",
      "training rmse:0.11798318693927522\n",
      "Training_R_Squared:-0.3883761968969899\n",
      "Epoch number: 807/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8798205852508545\n",
      "training loss:1.241513329570271\n",
      "training rmse:0.11142321704071692\n",
      "Training_R_Squared:-0.2382784086643972\n",
      "Epoch number: 808/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4127469062805176\n",
      "training loss:0.8636817146198155\n",
      "training rmse:0.0929344777044459\n",
      "Training_R_Squared:0.13856871901169066\n",
      "Epoch number: 809/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.22231125831604\n",
      "training loss:1.1648220929338322\n",
      "training rmse:0.10792692402425969\n",
      "Training_R_Squared:-0.16178698534369151\n",
      "Epoch number: 810/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.863349437713623\n",
      "training loss:1.611644041024963\n",
      "training rmse:0.12695054316642224\n",
      "Training_R_Squared:-0.6074446864433902\n",
      "Epoch number: 811/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8558177947998047\n",
      "training loss:1.3080628416088302\n",
      "training rmse:0.11437057495740896\n",
      "Training_R_Squared:-0.3046545069876505\n",
      "Epoch number: 812/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.907827854156494\n",
      "training loss:1.4150632937935121\n",
      "training rmse:0.1189564329405313\n",
      "Training_R_Squared:-0.4113761520428234\n",
      "Epoch number: 813/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.867478847503662\n",
      "training loss:1.2816460927363096\n",
      "training rmse:0.1132098093248244\n",
      "Training_R_Squared:-0.2783065922015697\n",
      "Epoch number: 814/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.871560573577881\n",
      "training loss:1.0269816572250647\n",
      "training rmse:0.10134010347463954\n",
      "Training_R_Squared:-0.024305716327086113\n",
      "Epoch number: 815/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.900413751602173\n",
      "training loss:1.1556938428740615\n",
      "training rmse:0.10750320194645653\n",
      "Training_R_Squared:-0.15268252617348588\n",
      "Epoch number: 816/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.832181453704834\n",
      "training loss:1.6472942506883328\n",
      "training rmse:0.1283469614244269\n",
      "Training_R_Squared:-0.6430020065077007\n",
      "Epoch number: 817/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8475639820098877\n",
      "training loss:0.9946589643977859\n",
      "training rmse:0.09973259068117031\n",
      "Training_R_Squared:0.007932753025527983\n",
      "Epoch number: 818/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.968977212905884\n",
      "training loss:1.219743235271995\n",
      "training rmse:0.11044198636714186\n",
      "Training_R_Squared:-0.2165650410655875\n",
      "Epoch number: 819/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8743913173675537\n",
      "training loss:1.5630264617290237\n",
      "training rmse:0.12502105669562322\n",
      "Training_R_Squared:-0.5589537966561937\n",
      "Epoch number: 820/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.917827606201172\n",
      "training loss:1.2310378740521628\n",
      "training rmse:0.11095214617357173\n",
      "Training_R_Squared:-0.22783024323092738\n",
      "Epoch number: 821/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.829855442047119\n",
      "training loss:1.2983262982131691\n",
      "training rmse:0.11394412219211525\n",
      "Training_R_Squared:-0.29494333549736407\n",
      "Epoch number: 822/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8116695880889893\n",
      "training loss:1.0084227057114958\n",
      "training rmse:0.10042025222590789\n",
      "Training_R_Squared:-0.005795134140391234\n",
      "Epoch number: 823/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.846003532409668\n",
      "training loss:1.2689719708612301\n",
      "training rmse:0.11264865604441227\n",
      "Training_R_Squared:-0.26566548525767564\n",
      "Epoch number: 824/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8515865802764893\n",
      "training loss:1.3120791090325703\n",
      "training rmse:0.11454602171322102\n",
      "Training_R_Squared:-0.3086603163504502\n",
      "Epoch number: 825/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.829402446746826\n",
      "training loss:1.3889970949720123\n",
      "training rmse:0.11785572090365458\n",
      "Training_R_Squared:-0.3853778799942891\n",
      "Epoch number: 826/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0429835319519043\n",
      "training loss:1.3891692514418175\n",
      "training rmse:0.11786302437328755\n",
      "Training_R_Squared:-0.38554959615437134\n",
      "Epoch number: 827/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3446524143218994\n",
      "training loss:1.4077215450588483\n",
      "training rmse:0.11864744182066667\n",
      "Training_R_Squared:-0.40405354123278303\n",
      "Epoch number: 828/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1971802711486816\n",
      "training loss:1.2208822478401231\n",
      "training rmse:0.11049354043744472\n",
      "Training_R_Squared:-0.2177010743713521\n",
      "Epoch number: 829/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8193018436431885\n",
      "training loss:1.2984290708651542\n",
      "training rmse:0.11394863188582627\n",
      "Training_R_Squared:-0.2950458427430658\n",
      "Epoch number: 830/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.902952194213867\n",
      "training loss:1.0377646443968302\n",
      "training rmse:0.10187073399150662\n",
      "Training_R_Squared:-0.035060609417395305\n",
      "Epoch number: 831/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.854442596435547\n",
      "training loss:1.4593560060620803\n",
      "training rmse:0.12080380813790931\n",
      "Training_R_Squared:-0.4555534634688321\n",
      "Epoch number: 832/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.855849027633667\n",
      "training loss:1.3065496955399318\n",
      "training rmse:0.11430440479438804\n",
      "Training_R_Squared:-0.3031453109570823\n",
      "Epoch number: 833/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.917351484298706\n",
      "training loss:1.1924706229965523\n",
      "training rmse:0.10920030325033682\n",
      "Training_R_Squared:-0.18936348871926745\n",
      "Epoch number: 834/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8534600734710693\n",
      "training loss:1.144525482574977\n",
      "training rmse:0.10698249775430452\n",
      "Training_R_Squared:-0.14154327739994121\n",
      "Epoch number: 835/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.859457015991211\n",
      "training loss:1.1977940242113618\n",
      "training rmse:0.10944377662577995\n",
      "Training_R_Squared:-0.19467301434698303\n",
      "Epoch number: 836/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8875932693481445\n",
      "training loss:1.3135474829177838\n",
      "training rmse:0.11461009915874708\n",
      "Training_R_Squared:-0.3101248566882504\n",
      "Epoch number: 837/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8263070583343506\n",
      "training loss:0.9750837603035052\n",
      "training rmse:0.09874632956740748\n",
      "Training_R_Squared:0.02745694533689902\n",
      "Epoch number: 838/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8410000801086426\n",
      "training loss:1.3987154198105145\n",
      "training rmse:0.11826729978360521\n",
      "Training_R_Squared:-0.3950708881971956\n",
      "Epoch number: 839/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.850680112838745\n",
      "training loss:1.5301752596127614\n",
      "training rmse:0.12370025301561681\n",
      "Training_R_Squared:-0.5261882261650754\n",
      "Epoch number: 840/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.911341667175293\n",
      "training loss:1.321602324135828\n",
      "training rmse:0.11496096398933979\n",
      "Training_R_Squared:-0.3181587114130011\n",
      "Epoch number: 841/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8774960041046143\n",
      "training loss:1.1874426700335619\n",
      "training rmse:0.10896984307750295\n",
      "Training_R_Squared:-0.18434863059910356\n",
      "Epoch number: 842/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8744540214538574\n",
      "training loss:1.071866289641239\n",
      "training rmse:0.10353097554071626\n",
      "Training_R_Squared:-0.06907339932698764\n",
      "Epoch number: 843/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.087578296661377\n",
      "training loss:1.2088539675828862\n",
      "training rmse:0.10994789527693953\n",
      "Training_R_Squared:-0.20570413708736912\n",
      "Epoch number: 844/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8873071670532227\n",
      "training loss:1.2468410559001768\n",
      "training rmse:0.111662037232901\n",
      "Training_R_Squared:-0.24359224906608512\n",
      "Epoch number: 845/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.909816265106201\n",
      "training loss:1.1101834378136317\n",
      "training rmse:0.10536524274226448\n",
      "Training_R_Squared:-0.10729070535796015\n",
      "Epoch number: 846/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.2869513034820557\n",
      "training loss:1.5183310399997936\n",
      "training rmse:0.1232205762038059\n",
      "Training_R_Squared:-0.5143748143794376\n",
      "Epoch number: 847/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1098380088806152\n",
      "training loss:1.4625173881913724\n",
      "training rmse:0.12093458513557535\n",
      "Training_R_Squared:-0.4587066086930107\n",
      "Epoch number: 848/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.884444236755371\n",
      "training loss:1.2777229924395215\n",
      "training rmse:0.1130364097288799\n",
      "Training_R_Squared:-0.2743937215669954\n",
      "Epoch number: 849/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.763709306716919\n",
      "training loss:1.111636844550219\n",
      "training rmse:0.10543419011640479\n",
      "Training_R_Squared:-0.10874032162046454\n",
      "Epoch number: 850/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7831168174743652\n",
      "training loss:1.2740641240736466\n",
      "training rmse:0.112874449016314\n",
      "Training_R_Squared:-0.2707443832588965\n",
      "Epoch number: 851/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.951744318008423\n",
      "training loss:1.1718036931606548\n",
      "training rmse:0.10824988190112056\n",
      "Training_R_Squared:-0.16875039872069175\n",
      "Epoch number: 852/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8946237564086914\n",
      "training loss:1.4857504546408933\n",
      "training rmse:0.1218913637072329\n",
      "Training_R_Squared:-0.48187913279415184\n",
      "Epoch number: 853/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8736629486083984\n",
      "training loss:1.1560989406930275\n",
      "training rmse:0.10752204149350157\n",
      "Training_R_Squared:-0.1530865746137977\n",
      "Epoch number: 854/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9524354934692383\n",
      "training loss:1.1474016672036669\n",
      "training rmse:0.10711683654793334\n",
      "Training_R_Squared:-0.14441196163323333\n",
      "Epoch number: 855/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.754290819168091\n",
      "training loss:1.2207130638185788\n",
      "training rmse:0.11048588433906743\n",
      "Training_R_Squared:-0.21753233914508052\n",
      "Epoch number: 856/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8827364444732666\n",
      "training loss:1.1488819212428325\n",
      "training rmse:0.10718590957970327\n",
      "Training_R_Squared:-0.14588835358740493\n",
      "Epoch number: 857/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.926565408706665\n",
      "training loss:1.0047984160346601\n",
      "training rmse:0.1002396336802295\n",
      "Training_R_Squared:-0.0021802810717688192\n",
      "Epoch number: 858/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7948920726776123\n",
      "training loss:1.25667226813205\n",
      "training rmse:0.11210139464485042\n",
      "Training_R_Squared:-0.25339784437206125\n",
      "Epoch number: 859/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8058407306671143\n",
      "training loss:1.2266164504724202\n",
      "training rmse:0.11075271782093747\n",
      "Training_R_Squared:-0.22342034335003347\n",
      "Epoch number: 860/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.877434730529785\n",
      "training loss:1.1068341186919497\n",
      "training rmse:0.10520618416670902\n",
      "Training_R_Squared:-0.1039501189235923\n",
      "Epoch number: 861/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7611072063446045\n",
      "training loss:1.3008176023845408\n",
      "training rmse:0.11405339111067854\n",
      "Training_R_Squared:-0.2974281504389833\n",
      "Epoch number: 862/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9104859828948975\n",
      "training loss:1.0638952502243626\n",
      "training rmse:0.10314529801325713\n",
      "Training_R_Squared:-0.06112313390539126\n",
      "Epoch number: 863/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8869335651397705\n",
      "training loss:1.264421130687822\n",
      "training rmse:0.11244648196754854\n",
      "Training_R_Squared:-0.2611265110581644\n",
      "Epoch number: 864/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.699565887451172\n",
      "training loss:1.2362597314265855\n",
      "training rmse:0.11118721740499604\n",
      "Training_R_Squared:-0.23303849240897812\n",
      "Epoch number: 865/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.0903778076171875\n",
      "training loss:1.1114975724267424\n",
      "training rmse:0.10542758521500635\n",
      "Training_R_Squared:-0.10860142019565577\n",
      "Epoch number: 866/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.904348373413086\n",
      "training loss:1.1274965284999467\n",
      "training rmse:0.1061836394412975\n",
      "Training_R_Squared:-0.1245586916854069\n",
      "Epoch number: 867/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.950120210647583\n",
      "training loss:1.34092469177358\n",
      "training rmse:0.1157983027411706\n",
      "Training_R_Squared:-0.3374307335943665\n",
      "Epoch number: 868/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8599371910095215\n",
      "training loss:1.028063220302073\n",
      "training rmse:0.1013934524662255\n",
      "Training_R_Squared:-0.025384461243364687\n",
      "Epoch number: 869/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3607563972473145\n",
      "training loss:0.983289885923682\n",
      "training rmse:0.09916097447704324\n",
      "Training_R_Squared:0.0192722027386083\n",
      "Epoch number: 870/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.251525640487671\n",
      "training loss:1.0649435293437364\n",
      "training rmse:0.10319610115424596\n",
      "Training_R_Squared:-0.06216867651483127\n",
      "Epoch number: 871/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8692264556884766\n",
      "training loss:1.2533742512561759\n",
      "training rmse:0.11195419828019743\n",
      "Training_R_Squared:-0.25010841917456306\n",
      "Epoch number: 872/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9312736988067627\n",
      "training loss:1.2997123221982747\n",
      "training rmse:0.1140049263057643\n",
      "Training_R_Squared:-0.2963257415732936\n",
      "Epoch number: 873/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7596967220306396\n",
      "training loss:0.9722960626913846\n",
      "training rmse:0.09860507404243377\n",
      "Training_R_Squared:0.03023737942148985\n",
      "Epoch number: 874/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.916414499282837\n",
      "training loss:1.5184112832184837\n",
      "training rmse:0.12322383224110844\n",
      "Training_R_Squared:-0.5144548778225382\n",
      "Epoch number: 875/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8580074310302734\n",
      "training loss:1.120241747487853\n",
      "training rmse:0.10584147332156016\n",
      "Training_R_Squared:-0.11732281804464129\n",
      "Epoch number: 876/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9451870918273926\n",
      "training loss:1.2035785353909887\n",
      "training rmse:0.10970772695626269\n",
      "Training_R_Squared:-0.2004424559776008\n",
      "Epoch number: 877/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8701891899108887\n",
      "training loss:1.109504437091914\n",
      "training rmse:0.1053330165281482\n",
      "Training_R_Squared:-0.10661347305790536\n",
      "Epoch number: 878/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.328648328781128\n",
      "training loss:1.3237793448705588\n",
      "training rmse:0.11505561024437526\n",
      "Training_R_Squared:-0.32033007550495673\n",
      "Epoch number: 879/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.304513692855835\n",
      "training loss:0.8608946634194581\n",
      "training rmse:0.09278440943496155\n",
      "Training_R_Squared:0.14134851203756638\n",
      "Epoch number: 880/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8002891540527344\n",
      "training loss:1.4328422882742053\n",
      "training rmse:0.11970139047956817\n",
      "Training_R_Squared:-0.4291088341050058\n",
      "Epoch number: 881/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.861988067626953\n",
      "training loss:1.0804174893332856\n",
      "training rmse:0.10394313297824372\n",
      "Training_R_Squared:-0.07760231552383301\n",
      "Epoch number: 882/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8634591102600098\n",
      "training loss:1.2439279913728036\n",
      "training rmse:0.11153151982165417\n",
      "Training_R_Squared:-0.24068677838416108\n",
      "Epoch number: 883/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.93544340133667\n",
      "training loss:1.3996097096869562\n",
      "training rmse:0.1183051017364406\n",
      "Training_R_Squared:-0.39596283000180943\n",
      "Epoch number: 884/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8823134899139404\n",
      "training loss:1.1903486121205447\n",
      "training rmse:0.10910309858663707\n",
      "Training_R_Squared:-0.18724700931323235\n",
      "Epoch number: 885/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.960559129714966\n",
      "training loss:1.5247167105835615\n",
      "training rmse:0.12347941976635465\n",
      "Training_R_Squared:-0.5207438601140606\n",
      "Epoch number: 886/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9988067150115967\n",
      "training loss:1.0545427262068188\n",
      "training rmse:0.10269093076834092\n",
      "Training_R_Squared:-0.05179497696893409\n",
      "Epoch number: 887/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.721951484680176\n",
      "training loss:1.3287356121727498\n",
      "training rmse:0.11527079474753134\n",
      "Training_R_Squared:-0.3252734185749586\n",
      "Epoch number: 888/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8356270790100098\n",
      "training loss:1.0179278157550016\n",
      "training rmse:0.1008924088202379\n",
      "Training_R_Squared:-0.015275473162257525\n",
      "Epoch number: 889/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8780300617218018\n",
      "training loss:1.0859120011100458\n",
      "training rmse:0.10420710153871693\n",
      "Training_R_Squared:-0.0830825129997892\n",
      "Epoch number: 890/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.912797212600708\n",
      "training loss:1.3832294478886524\n",
      "training rmse:0.11761077535194862\n",
      "Training_R_Squared:-0.37962526244410455\n",
      "Epoch number: 891/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8308048248291016\n",
      "training loss:1.5184184343427205\n",
      "training rmse:0.1232241224088336\n",
      "Training_R_Squared:-0.5144619938123631\n",
      "Epoch number: 892/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8282551765441895\n",
      "training loss:1.1246814070255056\n",
      "training rmse:0.10605099749769002\n",
      "Training_R_Squared:-0.12175089470607836\n",
      "Epoch number: 893/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.886996269226074\n",
      "training loss:1.0533545730413039\n",
      "training rmse:0.1026330635341898\n",
      "Training_R_Squared:-0.05060992413629939\n",
      "Epoch number: 894/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.890256881713867\n",
      "training loss:1.2090625439588791\n",
      "training rmse:0.10995738010515162\n",
      "Training_R_Squared:-0.2059121729158735\n",
      "Epoch number: 895/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8415985107421875\n",
      "training loss:0.998122451775604\n",
      "training rmse:0.09990607848252298\n",
      "Training_R_Squared:0.004478291969651105\n",
      "Epoch number: 896/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8661837577819824\n",
      "training loss:1.3124823014734517\n",
      "training rmse:0.11456361994426728\n",
      "Training_R_Squared:-0.3090624533680706\n",
      "Epoch number: 897/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8622329235076904\n",
      "training loss:1.3183925680527864\n",
      "training rmse:0.114821277124616\n",
      "Training_R_Squared:-0.3149573248366069\n",
      "Epoch number: 898/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.072202444076538\n",
      "training loss:1.2372912504727402\n",
      "training rmse:0.11123359431721787\n",
      "Training_R_Squared:-0.23406731948734327\n",
      "Epoch number: 899/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.3036069869995117\n",
      "training loss:1.0905240310175799\n",
      "training rmse:0.10442815860760832\n",
      "Training_R_Squared:-0.08768252329699178\n",
      "Epoch number: 900/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.2505693435668945\n",
      "training loss:1.0693843888907395\n",
      "training rmse:0.10341104336050089\n",
      "Training_R_Squared:-0.06659797523048194\n",
      "Epoch number: 901/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9481611251831055\n",
      "training loss:1.2418800756749988\n",
      "training rmse:0.11143967317230426\n",
      "Training_R_Squared:-0.23864419816637472\n",
      "Epoch number: 902/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.806299924850464\n",
      "training loss:1.5774104285546855\n",
      "training rmse:0.1255950010372501\n",
      "Training_R_Squared:-0.5733002716294704\n",
      "Epoch number: 903/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9263737201690674\n",
      "training loss:1.3387606456889216\n",
      "training rmse:0.11570482469149337\n",
      "Training_R_Squared:-0.3352723247720648\n",
      "Epoch number: 904/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9250264167785645\n",
      "training loss:1.1149179499045658\n",
      "training rmse:0.10558967515361366\n",
      "Training_R_Squared:-0.11201288969095224\n",
      "Epoch number: 905/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8186841011047363\n",
      "training loss:1.0970869919418647\n",
      "training rmse:0.10474192054482602\n",
      "Training_R_Squared:-0.09422838706253\n",
      "Epoch number: 906/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.913749933242798\n",
      "training loss:1.3122478846435435\n",
      "training rmse:0.11455338862921269\n",
      "Training_R_Squared:-0.30882865290399586\n",
      "Epoch number: 907/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9465625286102295\n",
      "training loss:1.5878863763600748\n",
      "training rmse:0.126011363628844\n",
      "Training_R_Squared:-0.5837489284813981\n",
      "Epoch number: 908/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9289824962615967\n",
      "training loss:1.3840278708095184\n",
      "training rmse:0.11764471389779986\n",
      "Training_R_Squared:-0.3804216102625053\n",
      "Epoch number: 909/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9172306060791016\n",
      "training loss:1.5134323883669651\n",
      "training rmse:0.12302163989993652\n",
      "Training_R_Squared:-0.5094889502182627\n",
      "Epoch number: 910/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9302327632904053\n",
      "training loss:1.252062957442181\n",
      "training rmse:0.11189561910290237\n",
      "Training_R_Squared:-0.2488005452381028\n",
      "Epoch number: 911/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9375221729278564\n",
      "training loss:1.0748133411832441\n",
      "training rmse:0.10367320488840133\n",
      "Training_R_Squared:-0.07201277303301179\n",
      "Epoch number: 912/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.910393238067627\n",
      "training loss:1.0949680353483018\n",
      "training rmse:0.1046407203409983\n",
      "Training_R_Squared:-0.09211495324475094\n",
      "Epoch number: 913/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.350458860397339\n",
      "training loss:0.9398108030674734\n",
      "training rmse:0.09694383957052008\n",
      "Training_R_Squared:0.06263800359858551\n",
      "Epoch number: 914/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1026856899261475\n",
      "training loss:1.4356554914770356\n",
      "training rmse:0.11981884206906006\n",
      "Training_R_Squared:-0.4319147113926305\n",
      "Epoch number: 915/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8914990425109863\n",
      "training loss:1.3691074696771466\n",
      "training rmse:0.11700886588960456\n",
      "Training_R_Squared:-0.3655400789925751\n",
      "Epoch number: 916/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7000327110290527\n",
      "training loss:1.2622166771720913\n",
      "training rmse:0.11234841686343833\n",
      "Training_R_Squared:-0.25892780605178944\n",
      "Epoch number: 917/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8816964626312256\n",
      "training loss:0.8627069237991236\n",
      "training rmse:0.0928820178397909\n",
      "Training_R_Squared:0.1395409697318034\n",
      "Epoch number: 918/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8461437225341797\n",
      "training loss:1.1606889717659783\n",
      "training rmse:0.10773527610611013\n",
      "Training_R_Squared:-0.1576646404151425\n",
      "Epoch number: 919/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.754734992980957\n",
      "training loss:1.5326005202978763\n",
      "training rmse:0.1237982439414177\n",
      "Training_R_Squared:-0.5286071320176287\n",
      "Epoch number: 920/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.872976779937744\n",
      "training loss:1.073693337384526\n",
      "training rmse:0.10361917474022489\n",
      "Training_R_Squared:-0.07089568159648718\n",
      "Epoch number: 921/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8761343955993652\n",
      "training loss:1.140549274056184\n",
      "training rmse:0.10679650153709082\n",
      "Training_R_Squared:-0.13757741943936463\n",
      "Epoch number: 922/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9873781204223633\n",
      "training loss:0.9593598984467917\n",
      "training rmse:0.09794691921887036\n",
      "Training_R_Squared:0.043139843246790766\n",
      "Epoch number: 923/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.796196460723877\n",
      "training loss:1.2692808241490638\n",
      "training rmse:0.11266236390867465\n",
      "Training_R_Squared:-0.2659735367745837\n",
      "Epoch number: 924/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.685779571533203\n",
      "training loss:1.1280002286805484\n",
      "training rmse:0.10620735514457313\n",
      "Training_R_Squared:-0.12506107541255096\n",
      "Epoch number: 925/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9464118480682373\n",
      "training loss:1.5515510208897467\n",
      "training rmse:0.12456127090270662\n",
      "Training_R_Squared:-0.5475082466790537\n",
      "Epoch number: 926/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9337780475616455\n",
      "training loss:1.4415117139780875\n",
      "training rmse:0.12006297155984802\n",
      "Training_R_Squared:-0.43775566799561605\n",
      "Epoch number: 927/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.771946668624878\n",
      "training loss:1.095988265321921\n",
      "training rmse:0.104689458176166\n",
      "Training_R_Squared:-0.09313252891936519\n",
      "Epoch number: 928/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9128944873809814\n",
      "training loss:1.2194208074115522\n",
      "training rmse:0.11042738824275218\n",
      "Training_R_Squared:-0.21624344908723958\n",
      "Epoch number: 929/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.785672664642334\n",
      "training loss:1.1316322109204329\n",
      "training rmse:0.10637820316777459\n",
      "Training_R_Squared:-0.12868359203516522\n",
      "Epoch number: 930/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.873561382293701\n",
      "training loss:1.4548598419914924\n",
      "training rmse:0.12061757094186122\n",
      "Training_R_Squared:-0.4510690225871301\n",
      "Epoch number: 931/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1867597103118896\n",
      "training loss:1.179772014651462\n",
      "training rmse:0.10861731052882234\n",
      "Training_R_Squared:-0.1766979683192733\n",
      "Epoch number: 932/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.366609573364258\n",
      "training loss:1.3827414877196134\n",
      "training rmse:0.11759002881705631\n",
      "Training_R_Squared:-0.3791385703576122\n",
      "Epoch number: 933/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.25942063331604\n",
      "training loss:1.2737681461999841\n",
      "training rmse:0.11286133732151078\n",
      "Training_R_Squared:-0.2704491729519629\n",
      "Epoch number: 934/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.891078233718872\n",
      "training loss:1.1004095244705923\n",
      "training rmse:0.10490040631335001\n",
      "Training_R_Squared:-0.0975422649072124\n",
      "Epoch number: 935/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.847074270248413\n",
      "training loss:0.9402162011201085\n",
      "training rmse:0.09696474622872524\n",
      "Training_R_Squared:0.06223365038810991\n",
      "Epoch number: 936/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8820900917053223\n",
      "training loss:0.9243127310969612\n",
      "training rmse:0.0961411842602826\n",
      "Training_R_Squared:0.07809568637713782\n",
      "Epoch number: 937/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7859365940093994\n",
      "training loss:0.7280371561548691\n",
      "training rmse:0.08532509338728374\n",
      "Training_R_Squared:0.2738598391799949\n",
      "Epoch number: 938/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9141430854797363\n",
      "training loss:1.3612526738110091\n",
      "training rmse:0.1166727334817784\n",
      "Training_R_Squared:-0.35770575375250657\n",
      "Epoch number: 939/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8138983249664307\n",
      "training loss:1.3433821517834446\n",
      "training rmse:0.11590436367037457\n",
      "Training_R_Squared:-0.3398817768013658\n",
      "Epoch number: 940/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.025000810623169\n",
      "training loss:1.3254632729118043\n",
      "training rmse:0.11512876586291561\n",
      "Training_R_Squared:-0.32200960879397744\n",
      "Epoch number: 941/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.6985747814178467\n",
      "training loss:1.5085506273099476\n",
      "training rmse:0.12282306897769439\n",
      "Training_R_Squared:-0.5046198962434016\n",
      "Epoch number: 942/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.969170570373535\n",
      "training loss:1.2430009855241906\n",
      "training rmse:0.11148995405525067\n",
      "Training_R_Squared:-0.23976218464841792\n",
      "Epoch number: 943/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8233416080474854\n",
      "training loss:1.1350245109233583\n",
      "training rmse:0.10653752911173406\n",
      "Training_R_Squared:-0.13206705060822976\n",
      "Epoch number: 944/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9587514400482178\n",
      "training loss:0.979273732831416\n",
      "training rmse:0.0989582605360167\n",
      "Training_R_Squared:0.023277894589065706\n",
      "Epoch number: 945/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7423958778381348\n",
      "training loss:1.2027738046035665\n",
      "training rmse:0.10967104470203456\n",
      "Training_R_Squared:-0.19963982541867642\n",
      "Epoch number: 946/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.896655559539795\n",
      "training loss:1.321802052244493\n",
      "training rmse:0.11496965044064861\n",
      "Training_R_Squared:-0.3183579209191465\n",
      "Epoch number: 947/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7983336448669434\n",
      "training loss:1.234944026296489\n",
      "training rmse:0.11112803544994797\n",
      "Training_R_Squared:-0.23172622017659128\n",
      "Epoch number: 948/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.1858394145965576\n",
      "training loss:1.0946316102883884\n",
      "training rmse:0.10462464386024874\n",
      "Training_R_Squared:-0.09177940347917302\n",
      "Epoch number: 949/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.745251417160034\n",
      "training loss:1.1732483370635691\n",
      "training rmse:0.10831658862166815\n",
      "Training_R_Squared:-0.17019129155705404\n",
      "Epoch number: 950/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9400665760040283\n",
      "training loss:1.2175522576044386\n",
      "training rmse:0.11034275044625445\n",
      "Training_R_Squared:-0.21437976761127198\n",
      "Epoch number: 951/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.345018148422241\n",
      "training loss:1.1055606575830552\n",
      "training rmse:0.10514564458802159\n",
      "Training_R_Squared:-0.10267997424995756\n",
      "Epoch number: 952/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.338557720184326\n",
      "training loss:1.3915326106248358\n",
      "training rmse:0.11796324048723127\n",
      "Training_R_Squared:-0.3879067856893208\n",
      "Epoch number: 953/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.801222562789917\n",
      "training loss:1.331089806659918\n",
      "training rmse:0.1153728653826331\n",
      "Training_R_Squared:-0.3276214782235134\n",
      "Epoch number: 954/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.956587076187134\n",
      "training loss:1.2255442954996312\n",
      "training rmse:0.11070430413943404\n",
      "Training_R_Squared:-0.2223509790238931\n",
      "Epoch number: 955/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.732792854309082\n",
      "training loss:1.3558011108758592\n",
      "training rmse:0.11643887284218528\n",
      "Training_R_Squared:-0.3522683910988449\n",
      "Epoch number: 956/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9307870864868164\n",
      "training loss:1.2377148561558897\n",
      "training rmse:0.11125263395335365\n",
      "Training_R_Squared:-0.23448983049940608\n",
      "Epoch number: 957/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7622334957122803\n",
      "training loss:1.5623321627381301\n",
      "training rmse:0.1249932863292317\n",
      "Training_R_Squared:-0.5582613037774147\n",
      "Epoch number: 958/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9409968852996826\n",
      "training loss:1.1107366269388592\n",
      "training rmse:0.1053914904979932\n",
      "Training_R_Squared:-0.10784246113596274\n",
      "Epoch number: 959/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7794649600982666\n",
      "training loss:1.3495677033772608\n",
      "training rmse:0.11617089581204325\n",
      "Training_R_Squared:-0.3460512292731277\n",
      "Epoch number: 960/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9155492782592773\n",
      "training loss:1.134668409294079\n",
      "training rmse:0.10652081530358652\n",
      "Training_R_Squared:-0.1317118885754036\n",
      "Epoch number: 961/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8110742568969727\n",
      "training loss:1.1511378940310806\n",
      "training rmse:0.1072910944128673\n",
      "Training_R_Squared:-0.14813845089502342\n",
      "Epoch number: 962/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9173946380615234\n",
      "training loss:0.9527531047469893\n",
      "training rmse:0.0976090725674099\n",
      "Training_R_Squared:0.04972940915062718\n",
      "Epoch number: 963/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.752340078353882\n",
      "training loss:1.4347967007563511\n",
      "training rmse:0.11978299966006659\n",
      "Training_R_Squared:-0.43105814068839665\n",
      "Epoch number: 964/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.931816577911377\n",
      "training loss:1.0600787222674946\n",
      "training rmse:0.10296012443016445\n",
      "Training_R_Squared:-0.057316547793990535\n",
      "Epoch number: 965/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.734246015548706\n",
      "training loss:1.1067635998663263\n",
      "training rmse:0.10520283265512988\n",
      "Training_R_Squared:-0.10387977934461357\n",
      "Epoch number: 966/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8559205532073975\n",
      "training loss:0.9501258492992726\n",
      "training rmse:0.09747439916712862\n",
      "Training_R_Squared:0.052349824903493936\n",
      "Epoch number: 967/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8421683311462402\n",
      "training loss:1.1503099740886036\n",
      "training rmse:0.10725250459027069\n",
      "Training_R_Squared:-0.14731269951484904\n",
      "Epoch number: 968/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.132798671722412\n",
      "training loss:1.2550829200396834\n",
      "training rmse:0.11203048335340178\n",
      "Training_R_Squared:-0.25181263327074355\n",
      "Epoch number: 969/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.767049789428711\n",
      "training loss:1.029853312996238\n",
      "training rmse:0.10148168864362861\n",
      "Training_R_Squared:-0.02716990139126696\n",
      "Epoch number: 970/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9487082958221436\n",
      "training loss:1.5309834804365892\n",
      "training rmse:0.12373291722240243\n",
      "Training_R_Squared:-0.5269943004143927\n",
      "Epoch number: 971/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.918976068496704\n",
      "training loss:1.3168219611980305\n",
      "training rmse:0.11475286319730896\n",
      "Training_R_Squared:-0.31339079979173423\n",
      "Epoch number: 972/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.792262315750122\n",
      "training loss:1.1933066400098085\n",
      "training rmse:0.10923857560449095\n",
      "Training_R_Squared:-0.1901973259565477\n",
      "Epoch number: 973/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8636083602905273\n",
      "training loss:1.209345492270586\n",
      "training rmse:0.10997024562446817\n",
      "Training_R_Squared:-0.20619438064234652\n",
      "Epoch number: 974/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9617362022399902\n",
      "training loss:1.2238680820645413\n",
      "training rmse:0.11062857144809117\n",
      "Training_R_Squared:-0.22067913507306725\n",
      "Epoch number: 975/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.701329469680786\n",
      "training loss:1.452458938098971\n",
      "training rmse:0.12051800438519429\n",
      "Training_R_Squared:-0.44867436460523313\n",
      "Epoch number: 976/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9646172523498535\n",
      "training loss:1.0770234606885651\n",
      "training rmse:0.10377974083069225\n",
      "Training_R_Squared:-0.07421713090741311\n",
      "Epoch number: 977/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7620491981506348\n",
      "training loss:0.8288919601013731\n",
      "training rmse:0.09104350389244545\n",
      "Training_R_Squared:0.17326782137131802\n",
      "Epoch number: 978/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9341063499450684\n",
      "training loss:1.3820699749305732\n",
      "training rmse:0.11756147221477677\n",
      "Training_R_Squared:-0.3784688035659056\n",
      "Epoch number: 979/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7714154720306396\n",
      "training loss:1.3189534065681983\n",
      "training rmse:0.11484569676606078\n",
      "Training_R_Squared:-0.31551669700859764\n",
      "Epoch number: 980/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.929743766784668\n",
      "training loss:1.1205015233237958\n",
      "training rmse:0.1058537445404647\n",
      "Training_R_Squared:-0.11758190769587018\n",
      "Epoch number: 981/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.779827833175659\n",
      "training loss:1.1598430115582232\n",
      "training rmse:0.10769600789064668\n",
      "Training_R_Squared:-0.15682088757858348\n",
      "Epoch number: 982/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8712122440338135\n",
      "training loss:1.348268876226939\n",
      "training rmse:0.1161149807831418\n",
      "Training_R_Squared:-0.3447558007203262\n",
      "Epoch number: 983/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.803009271621704\n",
      "training loss:1.0701765462034132\n",
      "training rmse:0.10344933765875029\n",
      "Training_R_Squared:-0.06738805577408336\n",
      "Epoch number: 984/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.893573522567749\n",
      "training loss:1.1956851086154074\n",
      "training rmse:0.10934738719399781\n",
      "Training_R_Squared:-0.19256959780234384\n",
      "Epoch number: 985/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.10494065284729\n",
      "training loss:1.0795574531575767\n",
      "training rmse:0.10390175422761526\n",
      "Training_R_Squared:-0.07674452673814569\n",
      "Epoch number: 986/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.4641025066375732\n",
      "training loss:1.0357701282758995\n",
      "training rmse:0.10177279244846825\n",
      "Training_R_Squared:-0.03307129225192562\n",
      "Epoch number: 987/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:3.254789113998413\n",
      "training loss:1.1461559008887434\n",
      "training rmse:0.10705867087203835\n",
      "Training_R_Squared:-0.14316945196310793\n",
      "Epoch number: 988/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9499025344848633\n",
      "training loss:1.0228783045058663\n",
      "training rmse:0.10113744630481167\n",
      "Training_R_Squared:-0.020213059659552668\n",
      "Epoch number: 989/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7790181636810303\n",
      "training loss:1.3123055425161851\n",
      "training rmse:0.11455590523915322\n",
      "Training_R_Squared:-0.3088861515782477\n",
      "Epoch number: 990/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.922720193862915\n",
      "training loss:1.1744874401339445\n",
      "training rmse:0.10837377174085733\n",
      "Training_R_Squared:-0.17142716743226782\n",
      "Epoch number: 991/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.844924211502075\n",
      "training loss:1.401335774369727\n",
      "training rmse:0.11837802897369626\n",
      "Training_R_Squared:-0.39768441615616346\n",
      "Epoch number: 992/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9832069873809814\n",
      "training loss:1.0846488875522233\n",
      "training rmse:0.10414647797944121\n",
      "Training_R_Squared:-0.08182269188352231\n",
      "Epoch number: 993/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9408950805664062\n",
      "training loss:1.307509617715482\n",
      "training rmse:0.11434638681285395\n",
      "Training_R_Squared:-0.30410272433396424\n",
      "Epoch number: 994/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9488444328308105\n",
      "training loss:1.3863578988462564\n",
      "training rmse:0.11774370041943885\n",
      "Training_R_Squared:-0.3827455561018511\n",
      "Epoch number: 995/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.799755573272705\n",
      "training loss:1.3704175088614647\n",
      "training rmse:0.1170648328432354\n",
      "Training_R_Squared:-0.3668466989347485\n",
      "Epoch number: 996/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.9958627223968506\n",
      "training loss:1.5697095976420314\n",
      "training rmse:0.12528805200983975\n",
      "Training_R_Squared:-0.5656195159869191\n",
      "Epoch number: 997/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.7848939895629883\n",
      "training loss:1.0032547292362324\n",
      "training rmse:0.100162604261083\n",
      "Training_R_Squared:-0.0006406167214483283\n",
      "Epoch number: 998/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.986013412475586\n",
      "training loss:1.4526661486733161\n",
      "training rmse:0.12052660074329302\n",
      "Training_R_Squared:-0.44888102874161273\n",
      "Epoch number: 999/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.691962242126465\n",
      "training loss:1.001230819335433\n",
      "training rmse:0.1000615220419634\n",
      "Training_R_Squared:0.001378024537060707\n",
      "Epoch number: 1000/1000\n",
      "LR: 1e-06\n",
      "Batch: 0\n",
      "training time this epoch:2.8524675369262695\n",
      "training loss:1.2414205717309557\n",
      "training rmse:0.11141905455221543\n",
      "Training_R_Squared:-0.23818589173053195\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1000\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "filefolder = glob.glob('./graph_data_train_multiple.pickle')\n",
    "print(\"Training\")\n",
    "print(f\"Total Number of epoches: {num_epoch}\")\n",
    "\n",
    "t = trange(num_epoch)\n",
    "for e in t:\n",
    "    print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "    print(\"LR: \"+str(learning_rate))\n",
    "  \n",
    "    \n",
    "    train_loss,train_mse,train_score = train(filefolder)\n",
    "    t.set_description(f'{train_score:.03e}')\n",
    "    t.refresh()\n",
    "    training_loss_epochs.append(train_loss)\n",
    "    training_mse_epochs.append(train_mse)\n",
    "    training_score_epochs.append(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x219b189ae20>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# min(list(range(len(training_score_epochs))), key=lambda i: training_score_epochs[i])\n",
    "plt.plot(training_score_epochs[150:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x219b93f8a00>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

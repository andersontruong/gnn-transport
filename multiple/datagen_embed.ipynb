{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import time\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from tqdm.auto import trange\n",
    "import grape\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import cupy\n",
    "from graphcuda import floyd_warshall_gpu\n",
    "from xx_AllFunctions import gnp_random_connected_graph, get_adjacency_feature_centrality, get_adjacency_centrality, get_node_embedding, sparse_mx_to_torch_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_sum(G: np.ndarray):\n",
    "    fw = floyd_warshall_gpu(G)\n",
    "    return cupy.sum(fw)\n",
    "\n",
    "def get_embedding(G: np.array, walk_len=100, num_walks=10, dimension_size=128, p=1, q=2):\n",
    "    folder_path = os.path.join(os.getcwd(), 'tmp')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    temp_file_path = os.path.join(folder_path, 'tmp.tsv')\n",
    "    with open(temp_file_path, 'w') as f:\n",
    "        # Write edgelist\n",
    "        for i, row in enumerate(G[:-1]):\n",
    "            for j, col in enumerate(row[i+1:]):\n",
    "                if col > 0:\n",
    "                    f.write(f'{i}\\t{j + i + 1}\\t{col}\\n')\n",
    "    \n",
    "    # GRAPE Model\n",
    "    grape_model = grape.Graph.from_csv(\n",
    "        # Edges related parameters\n",
    "\n",
    "        ## The path to the edges list tsv\n",
    "        edge_path=temp_file_path,\n",
    "        ## Set the tab as the separator between values\n",
    "        edge_list_separator=\"\\t\",\n",
    "        ## The first rows should NOT be used as the columns names\n",
    "        edge_list_header=False,\n",
    "        ## The source nodes are in the first nodes\n",
    "        sources_column_number=0,\n",
    "        ## The destination nodes are in the second column\n",
    "        destinations_column_number=1,\n",
    "        ## Both source and destinations columns use numeric node_ids instead of node names\n",
    "        edge_list_numeric_node_ids=True,\n",
    "        ## The weights are in the third column\n",
    "        weights_column_number=2,\n",
    "\n",
    "        # Graph related parameters\n",
    "        ## The graph is undirected\n",
    "        directed=False,\n",
    "        ## The name of the graph is HomoSapiens\n",
    "        name=\"Temp Grape Graph\",\n",
    "        ## Display a progress bar, (this might be in the terminal and not in the notebook)\n",
    "        verbose=True,\n",
    "    )\n",
    "    walks = grape_model.complete_walks(\n",
    "        walk_length=walk_len,\n",
    "        iterations=num_walks,\n",
    "        return_weight=p, # p\n",
    "        explore_weight=q # q\n",
    "    )\n",
    "\n",
    "    grape_word2vec = Word2Vec(\n",
    "        walks.tolist(),\n",
    "        vector_size=dimension_size,\n",
    "        window=5,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=16,\n",
    "        epochs=10,\n",
    "        seed=123\n",
    "    )\n",
    "\n",
    "    feature_matrix = sparse.csr_matrix(grape_word2vec.wv.vectors)\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damage_graph(base_graph):\n",
    "    G0=base_graph.copy()\n",
    "    while True:\n",
    "        tempg=base_graph.copy()\n",
    "        ndel=list(np.random.randint(0,len(base_graph.edges())-1,random.randint(0,20)))\n",
    "        ndel.sort(reverse=True)\n",
    "        # ndel=i\n",
    "        try:\n",
    "            for j in range(len(ndel)):\n",
    "                u,v=list(base_graph.edges())[ndel[j]][0:2]\n",
    "                \n",
    "                tempg.remove_edge(u, v)\n",
    "            \n",
    "            if nx.is_connected(tempg):\n",
    "                for j in range(len(ndel)):\n",
    "                    u,v=list(base_graph.edges())[ndel[j]][0:2]\n",
    "                    # G0.remove_edge(u, v)\n",
    "                    G0.edges[u,v]['weight'] = 1000000000\n",
    "                break\n",
    "            else:\n",
    "                print(\"try again\")\n",
    "                continue\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except  nx.NetworkXError:\n",
    "            continue\n",
    "    # list_node_weights_train.append(np.random.random(max_node).reshape(1,-1))\n",
    "    return G0.to_directed()\n",
    "\n",
    "def get_fm_centrality(graph: nx.Graph):\n",
    "    pfilename=\"test.edg\"\n",
    "\n",
    "    # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "    total_sum_new=distance_sum(nx.to_numpy_array(graph))\n",
    "    # eff=total_sum_orig/total_sum_new\n",
    "    \n",
    "    centrality = total_sum_new\n",
    "        \n",
    "    # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "    # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "    # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "    # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "    \n",
    "\n",
    "    \n",
    "    ndim = 256\n",
    "    feature_mat = np.zeros((graph.number_of_nodes(),ndim))\n",
    "\n",
    "    # USING GRAPE\n",
    "    feature_mat_excerpt = get_embedding(nx.to_numpy_array(graph), walk_len=50, num_walks=50, dimension_size=ndim, p=1, q=2).todense()\n",
    "    # feature_mat_excerpt = get_node_embedding(graph, dimensions = ndim, walk_length = 50, num_walks = 50, workers = 12,pfilename=pfilename).todense()\n",
    "    feature_mat[0:feature_mat_excerpt.shape[0]] = feature_mat_excerpt\n",
    "    feature_mat = sparse.csr_matrix(feature_mat)\n",
    "\n",
    "    # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "\n",
    "    feature_mat = sparse_mx_to_torch_sparse_tensor(feature_mat)\n",
    "\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    edge_weight = torch.tensor([graph[u][v]['weight'] for u, v in graph.edges])\n",
    "\n",
    "    return edge_index, edge_weight, centrality, feature_mat\n",
    "\n",
    "def get_centrality(graph):\n",
    "    # ebc=nx.betweenness_centrality(g, weight = 'weight')\n",
    "    total_sum_new=distance_sum(nx.to_numpy_array(graph))\n",
    "    # eff=total_sum_orig/total_sum_new\n",
    "    centrality = total_sum_new\n",
    "        \n",
    "    # adj_mat = np.zeros((num_nodes,num_nodes))\n",
    "    # adj_mat_excerpt = get_adj_matrix(g).todense()\n",
    "    # adj_mat[0:adj_mat_excerpt.shape[0], 0:adj_mat_excerpt.shape[1]] = adj_mat_excerpt\n",
    "    # adj_mat = sparse.csr_matrix(adj_mat)\n",
    "    \n",
    "    # adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)  \n",
    "    # list_adj.append(adj_mat)\n",
    "    \n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    edge_weight = torch.tensor([graph[u][v]['weight'] for u, v in graph.edges])\n",
    "\n",
    "    return edge_index, edge_weight, centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNSample:\n",
    "    def __init__(self, base_graph):\n",
    "        self.base_graph = base_graph\n",
    "        self.mod_graph = damage_graph(base_graph)\n",
    "\n",
    "        self.base_edge_index, self.base_edge_weight, self.base_centrality, self.base_fm = get_fm_centrality(self.base_graph)\n",
    "        self.mod_edge_index, self.mod_edge_weight, self.mod_centrality = get_centrality(self.mod_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a592407357214f8cb6fc45941c7f1a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "n_train_g = 100\n",
    "\n",
    "nnodes = 500\n",
    "\n",
    "prob_edge = 1.2/(100-1)\n",
    "\n",
    "list_train_graph_orig = []\n",
    "list_train_graph = list()\n",
    "\n",
    "list_test_graph_orig  = []\n",
    "list_test_graph= list()\n",
    "\n",
    "new_graph = gnp_random_connected_graph(nnodes, prob_edge)\n",
    "\n",
    "graphs = [GNNSample(new_graph) for _ in trange(n_train_g)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in graphs:\n",
    "    sample.ratio = 1 - (sample.mod_centrality - sample.base_centrality) / sample.base_centrality\n",
    "\n",
    "ratio_min = min(graphs, key=lambda sample: sample.ratio).ratio\n",
    "\n",
    "for sample in graphs:\n",
    "    sample.ratio_normalized = (sample.ratio - ratio_min) / (1 - ratio_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./graph_data_train_embed.pickle', 'wb') as handle:\n",
    "    pickle.dump(graphs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following device is used: cuda\n"
     ]
    }
   ],
   "source": [
    "from model_bet import Graph_Diff_Reg\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random, glob, time, os, pickle\n",
    "import numpy as np\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('The following device is used: '+str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 256\n",
    "num_epoch = 200\n",
    "\n",
    "n_features= 256#list_fm_train_shuffled[0].shape[1]\n",
    "dropout_val = 0.4\n",
    "learning_rate = 0.00005\n",
    "\n",
    "model = Graph_Diff_Reg(input_dim=n_features,hidden_dim=hidden,output_dim=1,dropout=dropout_val)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00005/10)\n",
    "\n",
    "training_loss_epochs = []\n",
    "training_mse_epochs = []\n",
    "\n",
    "testing_loss_epochs= []\n",
    "testing_mse_epochs= []\n",
    "\n",
    "training_score_epochs = []\n",
    "testing_score_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(filefolder):\n",
    "    model.train() \n",
    "    \n",
    "    loss_train = 0\n",
    "    num_files_train = len(filefolder)\n",
    "    \n",
    "    starttime3 = time.time()\n",
    "    y_trues=[]\n",
    "    y_preds=[]\n",
    "    \n",
    "    random.shuffle(filefolder)\n",
    "    n=0\n",
    "    \n",
    "    for i in range(num_files_train):\n",
    "        print(\"Batch: \"+str(i))\n",
    "\n",
    "        filepath=filefolder[i]\n",
    "        with open(filepath,\"rb\") as f:\n",
    "            # list_adj_deg_train_shuffled, list_adj_wt_train_shuffled,list_num_edge_train, bc_mat_train_shuffled, list_fm_train_shuffled, list_n_seq_train, list_n_unshuffle_train,list_adj_deg_test_shuffled, list_adj_wt_test_shuffled,list_num_edge_test, bc_mat_test_shuffled, list_fm_test_shuffled, list_n_seq_test, list_n_unshuffle_test=pickle.load(f)\n",
    "            list_train_graph, list_edge_index_train,list_edge_weight_train, eff_mat_train, list_fm_train_orig,list_edge_index_train_orig,list_edge_weight_train_orig=pickle.load(f)\n",
    "        \n",
    "        # for j in range(len(list_train_graph)):\n",
    "        #     print(len(list_train_graph[j].edges()))\n",
    "        \n",
    "        \n",
    "        for j in range(len(list_train_graph)):\n",
    "            \n",
    "            # g=list_train_graph\n",
    "            # adj2=torch.tensor(nx.adjacency_matrix(list_train_graph[j]).toarray()).numpy()\n",
    "            \n",
    "            # for \n",
    "            \n",
    "            \n",
    "            edge_index=list_edge_index_train[j]\n",
    "            edge_weigth=list_edge_weight_train[j]\n",
    "            \n",
    "            # adj=torch.tensor(edge_list_to_adjacency_matrix(edge_index.numpy().transpose(), edge_weigth.numpy(), 500))\n",
    "            # adj=adj.to(device)\n",
    "            \n",
    "            edge_index=edge_index.to(device)\n",
    "            edge_weigth=edge_weigth.to(device)\n",
    "            fm=list_fm_train_orig[0]\n",
    "            # fm=torch.tensor(np.eye(500,dtype=np.float32))\n",
    "            \n",
    "            batch_vector=np.zeros((fm.shape[0]),dtype=np.int64)\n",
    "            batch_vector=torch.tensor(batch_vector)\n",
    "            batch_vector=batch_vector.to(device)\n",
    "            \n",
    "            fm=fm.to(device).to_dense()\n",
    "            # adj_orig=torch.tensor(nx.adjacency_matrix(list_train_graph_orig[j]).toarray())\n",
    "            edge_index_orig=list_edge_index_train_orig[0]\n",
    "            edge_weigth_orig=list_edge_weight_train_orig[0]\n",
    "            \n",
    "            # adj_orig=torch.tensor(edge_list_to_adjacency_matrix(edge_index_orig.numpy().transpose(), edge_weigth_orig.numpy(), 500))\n",
    "            # adj_orig=adj_orig.to(device)\n",
    "            \n",
    "            # adjn=adj.cpu().numpy()\n",
    "            # adjon=adj_orig.cpu().numpy()\n",
    "            # adjr=adjon-adjn\n",
    "            # adjr[adjr!=0]=1\n",
    "            # adjr=torch.tensor(adjr.astype(np.float32)).to(device)\n",
    "            \n",
    "            # edge_index_n=edge_index.cpu().numpy()\n",
    "            # edge_index_orig_n=edge_index_orig.cpu().numpy()\n",
    "            edge_weight_n=edge_weigth.cpu().numpy()\n",
    "            edge_weight_n[edge_weight_n<10000]=1\n",
    "            edge_weight_n[edge_weight_n!=1]=0\n",
    "            edge_mask=torch.tensor(edge_weight_n.astype(np.int64)).to(device)\n",
    "            \n",
    "            edge_index_orig=edge_index_orig.to(device)\n",
    "            \n",
    "            edge_weigth_orig=edge_weigth_orig.to(device)\n",
    "            \n",
    "            y_true=torch.tensor(eff_mat_train[0,j].reshape([1,1]).astype(np.float32))\n",
    "            y_true=y_true.to(device)\n",
    "            \n",
    "            # adj = list_adj_deg_train_shuffled[j]\n",
    "            # num_nodes = list_num_edge_train[j]\n",
    "            # adj_t = list_adj_wt_train_shuffled[j]\n",
    "            # fm = list_fm_train_shuffled[j]\n",
    "            # adj = adj.to(device)\n",
    "            # adj_t = adj_t.to(device)\n",
    "            # fm=fm.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # y_out = model(adj,adj_t,fm)\n",
    "            y_out = model(edge_index,edge_weigth.float(),edge_index_orig,edge_weigth_orig.float(),fm.float(),batch_vector)#,adjr.float(),edge_mask)#,adj,adj_orig)\n",
    "                        \n",
    "            # true_arr = torch.from_numpy(bc_mat_train_shuffled[:,j]).float()\n",
    "            # true_val = true_arr.to(device)\n",
    "            \n",
    "            # true_val_final = true_val[list_n_unshuffle_train[j]]\n",
    "            # y_out_final = y_out[list_n_unshuffle_train[j]]\n",
    "            \n",
    "            # true_val_ultimate = true_val_final[:num_nodes]\n",
    "            # y_out_ultimate = y_out_final[:num_nodes]\n",
    "            \n",
    "            mse = loss_fn(y_out, y_true)\n",
    "            \n",
    "            mse.backward()\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            y_trues.append(float(y_true))\n",
    "            y_preds.append(float(y_out))\n",
    "            \n",
    "            \n",
    "            # loss_rank = new_loss_cal(y_out_ultimate, true_val_ultimate, num_nodes, device, model_size)\n",
    "            loss_train = loss_train + float(mse)\n",
    "            n+=1\n",
    "            \n",
    "    mse_final=loss_train/n\n",
    "    duration3 = time.time()-starttime3\n",
    "    print(\"training time this epoch:\"+str(duration3))\n",
    "    print(\"training loss:\"+str(loss_train))\n",
    "    print(\"training rmse:\"+str(np.sqrt(float(mse_final))))\n",
    "    \n",
    "    score=r2_score(y_trues, y_preds)\n",
    "    print(\"Training_R_Squared:\"+str(score))\n",
    "    \n",
    "    return loss_train,mse_final,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Total Number of epoches: 200\n",
      "Epoch number: 1/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:3.6668589115142822\n",
      "training loss:37.57090714174274\n",
      "training rmse:0.6129511166621914\n",
      "Training_R_Squared:-9.126560765832174\n",
      "Epoch number: 2/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.1442008018493652\n",
      "training loss:22.098883172905516\n",
      "training rmse:0.4700944923406944\n",
      "Training_R_Squared:-4.956355764515544\n",
      "Epoch number: 3/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9184436798095703\n",
      "training loss:9.376490368431405\n",
      "training rmse:0.3062105544952918\n",
      "Training_R_Squared:-1.5272640129401864\n",
      "Epoch number: 4/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.7919204235076904\n",
      "training loss:5.780198813694369\n",
      "training rmse:0.2404204403476204\n",
      "Training_R_Squared:-0.5579484119609235\n",
      "Epoch number: 5/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.5983502864837646\n",
      "training loss:5.375439678059365\n",
      "training rmse:0.23184994453437693\n",
      "Training_R_Squared:-0.4488528781266652\n",
      "Epoch number: 6/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9396634101867676\n",
      "training loss:5.762658641707759\n",
      "training rmse:0.2400553819789875\n",
      "Training_R_Squared:-0.5532207842632337\n",
      "Epoch number: 7/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9893529415130615\n",
      "training loss:6.038859082011186\n",
      "training rmse:0.24574090180536057\n",
      "Training_R_Squared:-0.6276656355271517\n",
      "Epoch number: 8/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.6761860847473145\n",
      "training loss:5.330053994682203\n",
      "training rmse:0.2308690969939936\n",
      "Training_R_Squared:-0.43661999227169246\n",
      "Epoch number: 9/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.3738787174224854\n",
      "training loss:5.030695450834628\n",
      "training rmse:0.22429211869422938\n",
      "Training_R_Squared:-0.3559332696205362\n",
      "Epoch number: 10/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9289662837982178\n",
      "training loss:4.397008088118355\n",
      "training rmse:0.20969044060515385\n",
      "Training_R_Squared:-0.18513427081298728\n",
      "Epoch number: 11/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.088982582092285\n",
      "training loss:4.983896588023981\n",
      "training rmse:0.22324642411523596\n",
      "Training_R_Squared:-0.3433194996323725\n",
      "Epoch number: 12/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.0402119159698486\n",
      "training loss:5.002836227563421\n",
      "training rmse:0.22367020873516932\n",
      "Training_R_Squared:-0.34842432230465015\n",
      "Epoch number: 13/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.682981014251709\n",
      "training loss:4.901798402223449\n",
      "training rmse:0.22140005425074874\n",
      "Training_R_Squared:-0.32119141590228795\n",
      "Epoch number: 14/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.143772602081299\n",
      "training loss:4.403747143622368\n",
      "training rmse:0.20985106965708725\n",
      "Training_R_Squared:-0.18695068305825546\n",
      "Epoch number: 15/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.0520594120025635\n",
      "training loss:5.2431100651519955\n",
      "training rmse:0.22897838468187331\n",
      "Training_R_Squared:-0.4131858121504246\n",
      "Epoch number: 16/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.136899471282959\n",
      "training loss:4.730266718699227\n",
      "training rmse:0.21749176349230392\n",
      "Training_R_Squared:-0.2749581264039844\n",
      "Epoch number: 17/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.755976676940918\n",
      "training loss:4.520954088288818\n",
      "training rmse:0.21262535333983146\n",
      "Training_R_Squared:-0.21854170028192765\n",
      "Epoch number: 18/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.3240256309509277\n",
      "training loss:3.346417757540422\n",
      "training rmse:0.1829321665957199\n",
      "Training_R_Squared:0.09803340813788564\n",
      "Epoch number: 19/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.115149974822998\n",
      "training loss:3.421635944130685\n",
      "training rmse:0.18497664566454558\n",
      "Training_R_Squared:0.07775970646116948\n",
      "Epoch number: 20/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.697385311126709\n",
      "training loss:3.3105983113939743\n",
      "training rmse:0.18195049632781918\n",
      "Training_R_Squared:0.1076878956660956\n",
      "Epoch number: 21/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9664247035980225\n",
      "training loss:2.5995869083317302\n",
      "training rmse:0.16123234502827682\n",
      "Training_R_Squared:0.2993282024427367\n",
      "Epoch number: 22/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.8428707122802734\n",
      "training loss:2.8086673177931516\n",
      "training rmse:0.16759079085060585\n",
      "Training_R_Squared:0.24297434724427736\n",
      "Epoch number: 23/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.8752775192260742\n",
      "training loss:2.5452947565318027\n",
      "training rmse:0.15953979931452222\n",
      "Training_R_Squared:0.31396167222665994\n",
      "Epoch number: 24/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.5836503505706787\n",
      "training loss:2.0783801944670586\n",
      "training rmse:0.14416588342832914\n",
      "Training_R_Squared:0.4398100856504672\n",
      "Epoch number: 25/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9543638229370117\n",
      "training loss:2.418117709364992\n",
      "training rmse:0.15550298097994752\n",
      "Training_R_Squared:0.34823995114627293\n",
      "Epoch number: 26/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.8466784954071045\n",
      "training loss:2.245140583573402\n",
      "training rmse:0.14983793189888206\n",
      "Training_R_Squared:0.39486281901658216\n",
      "Epoch number: 27/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.0647788047790527\n",
      "training loss:2.6106989886802694\n",
      "training rmse:0.1615765759223864\n",
      "Training_R_Squared:0.2963331473705061\n",
      "Epoch number: 28/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.0024843215942383\n",
      "training loss:2.0125899938341405\n",
      "training rmse:0.14186578142153028\n",
      "Training_R_Squared:0.4575426502000718\n",
      "Epoch number: 29/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.6291568279266357\n",
      "training loss:2.36884011720295\n",
      "training rmse:0.15391036733121488\n",
      "Training_R_Squared:0.36152184820193256\n",
      "Epoch number: 30/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9966199398040771\n",
      "training loss:2.6044135340492574\n",
      "training rmse:0.1613819548168028\n",
      "Training_R_Squared:0.29802726888517894\n",
      "Epoch number: 31/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.925811767578125\n",
      "training loss:2.0957106804753494\n",
      "training rmse:0.14476569622929839\n",
      "Training_R_Squared:0.435138962092266\n",
      "Epoch number: 32/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.038652181625366\n",
      "training loss:2.0635360433088863\n",
      "training rmse:0.14365013203296703\n",
      "Training_R_Squared:0.4438110547515113\n",
      "Epoch number: 33/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.6144893169403076\n",
      "training loss:2.2282276735310518\n",
      "training rmse:0.14927249155591435\n",
      "Training_R_Squared:0.3994213895030274\n",
      "Epoch number: 34/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.2816290855407715\n",
      "training loss:1.476116856061708\n",
      "training rmse:0.12149554955066082\n",
      "Training_R_Squared:0.6021393094232643\n",
      "Epoch number: 35/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:2.1904561519622803\n",
      "training loss:1.8622903401110307\n",
      "training rmse:0.13646575907937605\n",
      "Training_R_Squared:0.49805320460783764\n",
      "Epoch number: 36/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.8557415008544922\n",
      "training loss:1.6768471717363411\n",
      "training rmse:0.12949313386185157\n",
      "Training_R_Squared:0.5480360685657246\n",
      "Epoch number: 37/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.993119239807129\n",
      "training loss:2.39284415700844\n",
      "training rmse:0.1546882075986544\n",
      "Training_R_Squared:0.3550519827937424\n",
      "Epoch number: 38/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.876795768737793\n",
      "training loss:1.443870361767523\n",
      "training rmse:0.12016115685892521\n",
      "Training_R_Squared:0.6108307639534702\n",
      "Epoch number: 39/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002010A2F31C0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Anderson\\miniconda3\\envs\\gnn\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time this epoch:2.0539748668670654\n",
      "training loss:1.7366298251199623\n",
      "training rmse:0.13178125151628975\n",
      "Training_R_Squared:0.5319227355123227\n",
      "Epoch number: 40/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9896676540374756\n",
      "training loss:1.4798708561152267\n",
      "training rmse:0.12164994270920257\n",
      "Training_R_Squared:0.6011274848510166\n",
      "Epoch number: 41/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.722649335861206\n",
      "training loss:1.3940404590621256\n",
      "training rmse:0.11806949051563345\n",
      "Training_R_Squared:0.6242615189333358\n",
      "Epoch number: 42/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9715509414672852\n",
      "training loss:1.6165244993026135\n",
      "training rmse:0.12714261674602317\n",
      "Training_R_Squared:0.5642949571925684\n",
      "Epoch number: 43/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.8436689376831055\n",
      "training loss:1.5072858097902895\n",
      "training rmse:0.12277156876859925\n",
      "Training_R_Squared:0.5937382810897722\n",
      "Epoch number: 44/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n",
      "training time this epoch:1.9905986785888672\n",
      "training loss:1.5722611721073605\n",
      "training rmse:0.12538983898655268\n",
      "Training_R_Squared:0.5762253372487622\n",
      "Epoch number: 45/200\n",
      "LR: 5e-05\n",
      "Batch: 0\n"
     ]
    }
   ],
   "source": [
    "filefolder = glob.glob('./graph_data_train.pickle')\n",
    "print(\"Training\")\n",
    "print(f\"Total Number of epoches: {num_epoch}\")\n",
    "for e in range(num_epoch):\n",
    "    print(f\"Epoch number: {e+1}/{num_epoch}\")\n",
    "    print(\"LR: \"+str(learning_rate))\n",
    "  \n",
    "    \n",
    "    train_loss,train_mse,train_score = train(filefolder)\n",
    "    training_loss_epochs.append(train_loss)\n",
    "    training_mse_epochs.append(train_mse)\n",
    "    training_score_epochs.append(train_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
